<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 20]
- [cs.AI](#cs.AI) [Total: 16]
- [cs.CL](#cs.CL) [Total: 17]
- [stat.ME](#stat.ME) [Total: 3]
- [cs.LG](#cs.LG) [Total: 1]
- [math.ST](#math.ST) [Total: 5]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Dynamic Epsilon Scheduling: A Multi-Factor Adaptive Perturbation Budget for Adversarial Training](https://arxiv.org/abs/2506.04263)
*Alan Mitkiy,James Smith,Hana Satou,Hiroshi Tanaka,Emily Johnson,F Monkey*

Main category: cs.CV

TL;DR: This paper introduces Dynamic Epsilon Scheduling (DES), a novel method that adaptively adjusts adversarial perturbation budgets during training, improving adversarial robustness and standard accuracy.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limitation of fixed perturbation budgets in adversarial training, which fail to account for instance-specific robustness characteristics. Current adaptive methods often rely on heuristic or static approximations of data robustness.

Method: DES integrates three key factors to dynamically adjust the perturbation budget: (1) distance to the decision boundary, (2) prediction confidence, and (3) model uncertainty. These factors are combined into a unified scheduling strategy to guide more effective adversarial learning.

Result: Experiments on CIFAR-10 and CIFAR-100 show that DES consistently improves both adversarial robustness and standard accuracy compared to fixed-epsilon baselines and prior adaptive methods.

Conclusion: The paper provides theoretical insights into the stability and convergence of the scheduling policy, opening new avenues for instance-aware, data-driven adversarial training methods.

Abstract: Adversarial training is among the most effective strategies for defending
deep neural networks against adversarial examples. A key limitation of existing
adversarial training approaches lies in their reliance on a fixed perturbation
budget, which fails to account for instance-specific robustness
characteristics. While prior works such as IAAT and MMA introduce
instance-level adaptations, they often rely on heuristic or static
approximations of data robustness. In this paper, we propose Dynamic Epsilon
Scheduling (DES), a novel framework that adaptively adjusts the adversarial
perturbation budget per instance and per training iteration. DES integrates
three key factors: (1) the distance to the decision boundary approximated via
gradient-based proxies, (2) prediction confidence derived from softmax entropy,
and (3) model uncertainty estimated via Monte Carlo dropout. By combining these
cues into a unified scheduling strategy, DES tailors the perturbation budget
dynamically to guide more effective adversarial learning. Experimental results
on CIFAR-10 and CIFAR-100 show that our method consistently improves both
adversarial robustness and standard accuracy compared to fixed-epsilon
baselines and prior adaptive methods. Moreover, we provide theoretical insights
into the stability and convergence of our scheduling policy. This work opens a
new avenue for instance-aware, data-driven adversarial training methods.

</details>


### [2] [RSVP: Reasoning Segmentation via Visual Prompting and Multi-modal Chain-of-Thought](https://arxiv.org/abs/2506.04277)
*Yi Lu,Jiawang Cao,Yongliang Wu,Bozheng Li,Licheng Tang,Yangguang Ji,Chong Wu,Jay Wu,Wenbo Zhu*

Main category: cs.CV

TL;DR: A novel framework RSVP that integrates multi-step multimodal reasoning with grounded visual understanding using visual prompting and segmentation refinement, achieving state-of-the-art performance in reasoning segmentation tasks.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between cognitive reasoning and visual perception in multi-modal large language models (MLLMs) by introducing explicit mechanisms for visual grounding and segmentation.

Method: RSVP consists of a two-stage framework: 1) a reasoning stage that uses multimodal chain-of-thought visual prompts to generate interpretable region proposals, and 2) a segmentation stage that refines these proposals using a Vision-Language Segmentation Module (VLSM) to produce precise segmentation masks.

Result: RSVP achieves state-of-the-art performance on ReasonSeg and SegInW datasets, surpassing existing methods by up to +6.5 gIoU and +9.2 cIoU on ReasonSeg, and achieving 49.7 mAP on SegInW under zero-shot settings.

Conclusion: RSVP introduces a new paradigm for interpretable reasoning segmentation by explicitly modeling the interaction between multimodal reasoning and segmentation, leveraging MLLMs' localization capabilities to enhance visual understanding.

Abstract: Multi-modal Large Language Models (MLLMs) have demonstrated remarkable
reasoning capability while lack explicit mechanisms for visual grounding and
segmentation, creating a gap between cognitive reasoning and visual perception.
To bridge this gap, we introduce Reasoning Segmentation via Visual Prompting
(RSVP), a novel framework that unifies multi-step multimodal reasoning with
grounded visual understanding. RSVP is a two-stage structuralized framework
that integrates reasoning-driven localization with segmentation refinement. In
the reasoning stage, RSVP employs multimodal chain-of-thought visual prompts to
help MLLMs understand queries and infer targets, generating interpretable
region proposals that enhance visual grounding. In segmentation stage, RSVP
refines these proposals with a Vision-Language Segmentation Module (VLSM),
seamlessly integrates textual and visual cues to produce precise segmentation
masks. By explicitly modelling the interaction between multimodal reasoning and
segmentation, RSVP introduces a new paradigm for interpretable reasoning
segmentation. It exploits MLLMs' inherent localization capabilities, enabling
the models to not only reason about objects but also generate structured visual
representations. Our extensive experiments demonstrate that RSVP achieves
state-of-the-art performance, surpasses state-of-the-art methods by up to +6.5
gIoU and +9.2 cIoU on ReasonSeg, and achieves 49.7 mAP on SegInW under
zero-shot settings. These results validate RSVP as an effective and scalable
framework for integrating cognitive reasoning with structured visual
understanding.

</details>


### [3] [Evaluating MLLMs with Multimodal Multi-image Reasoning Benchmark](https://arxiv.org/abs/2506.04280)
*Ziming Cheng,Binrui Xu,Lisheng Gong,Zuhe Song,Tianshuo Zhou,Shiqi Zhong,Siyu Ren,Mingxiang Chen,Xiangchao Meng,Yuxin Zhang,Yanlin Li,Lei Ren,Wei Chen,Zhiyuan Huang,Mingjie Zhan,Xiaojie Wang,Fangxiang Feng*

Main category: cs.CV

TL;DR: The paper introduces MMRB, a benchmark for evaluating MLLMs' reasoning over multiple images, and shows that open-source MLLMs and reward models perform poorly in these tasks compared to commercial models.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of benchmarks that evaluate the reasoning capabilities of MLLMs over multiple images. Existing benchmarks focus on single-image visual reasoning or final-answer evaluation, which does not fully capture the reasoning abilities of MLLMs in multi-image scenarios.

Method: The authors introduce the Multimodal Multi-image Reasoning Benchmark (MMRB), a new benchmark for evaluating MLLMs' ability to reason over multiple images. MMRB consists of 92 sub-tasks that assess spatial, temporal, and semantic reasoning, with annotations generated by GPT-4 and refined by human experts. They also propose a sentence-level matching framework for scalable evaluation and conduct extensive experiments on 40 MLLMs, including reasoning-specific models and reward models.

Result: Extensive experiments reveal that open-source MLLMs perform poorly compared to commercial MLLMs in multi-image reasoning tasks. Multimodal reward models show minimal effectiveness in handling multi-image reward ranking tasks.

Conclusion: The results show that open-source MLLMs significantly lag behind commercial MLLMs in multi-image reasoning tasks. Additionally, current multimodal reward models struggle with multi-image reward ranking tasks.

Abstract: With enhanced capabilities and widespread applications, Multimodal Large
Language Models (MLLMs) are increasingly required to process and reason over
multiple images simultaneously. However, existing MLLM benchmarks focus either
on single-image visual reasoning or on multi-image understanding tasks with
only final-answer evaluation, leaving the reasoning capabilities of MLLMs over
multi-image inputs largely underexplored. To address this gap, we introduce the
$\textbf{Multimodal Multi-image Reasoning Benchmark (MMRB)}$, the first
benchmark designed to evaluate structured visual reasoning across multiple
images. MMRB comprises $\textbf{92 sub-tasks}$ covering spatial, temporal, and
semantic reasoning, with multi-solution, CoT-style annotations generated by
GPT-4o and refined by human experts. A derivative subset is designed to
evaluate multimodal reward models in multi-image scenarios. To support fast and
scalable evaluation, we propose a sentence-level matching framework using
open-source LLMs. Extensive baseline experiments on $\textbf{40 MLLMs}$,
including 9 reasoning-specific models and 8 reward models, demonstrate that
open-source MLLMs still lag significantly behind commercial MLLMs in
multi-image reasoning tasks. Furthermore, current multimodal reward models are
nearly incapable of handling multi-image reward ranking tasks.

</details>


### [4] [HuGeDiff: 3D Human Generation via Diffusion with Gaussian Splatting](https://arxiv.org/abs/2506.04351)
*Maksym Ivashechkin,Oscar Mendez,Richard Bowden*

Main category: cs.CV

TL;DR: A weakly supervised pipeline using image diffusion, feature mapping, and point-cloud diffusion is proposed to generate 3D humans from text prompts, achieving significant improvements in speed, realism, and text-prompt alignment.


<details>
  <summary>Details</summary>
Motivation: Current methods for generating 3D humans from text prompts struggle with fine detail, accurate rendering of hands and faces, human realism, and controllability over appearance. The lack of diversity, realism, and annotation in human image data further hinders the development of a foundational 3D human model.

Method: The paper introduces a weakly supervised pipeline that consists of three key steps to generate 3D humans from text prompts. First, a state-of-the-art image diffusion model is used to generate a dataset of photorealistic human images with controllable attributes. Second, a transformer-based architecture is proposed to efficiently map image features to 3D point clouds. Finally, a point-cloud diffusion model is trained, conditioned on the same text prompts, to generate 3D humans.

Result: The proposed method achieves orders-of-magnitude speed-ups in 3D human generation compared to existing approaches, while also improving text-prompt alignment, realism, and rendering quality.

Conclusion: The paper addresses the challenges of generating accurate and realistic 3D humans from text prompts by introducing a novel weakly supervised pipeline. The method significantly improves the speed, text-prompt alignment, realism, and rendering quality of 3D human generation. The authors will make the code and dataset publicly available.

Abstract: 3D human generation is an important problem with a wide range of applications
in computer vision and graphics. Despite recent progress in generative AI such
as diffusion models or rendering methods like Neural Radiance Fields or
Gaussian Splatting, controlling the generation of accurate 3D humans from text
prompts remains an open challenge. Current methods struggle with fine detail,
accurate rendering of hands and faces, human realism, and controlability over
appearance. The lack of diversity, realism, and annotation in human image data
also remains a challenge, hindering the development of a foundational 3D human
model. We present a weakly supervised pipeline that tries to address these
challenges. In the first step, we generate a photorealistic human image dataset
with controllable attributes such as appearance, race, gender, etc using a
state-of-the-art image diffusion model. Next, we propose an efficient mapping
approach from image features to 3D point clouds using a transformer-based
architecture. Finally, we close the loop by training a point-cloud diffusion
model that is conditioned on the same text prompts used to generate the
original samples. We demonstrate orders-of-magnitude speed-ups in 3D human
generation compared to the state-of-the-art approaches, along with
significantly improved text-prompt alignment, realism, and rendering quality.
We will make the code and dataset available.

</details>


### [5] [ReXVQA: A Large-scale Visual Question Answering Benchmark for Generalist Chest X-ray Understanding](https://arxiv.org/abs/2506.04353)
*Ankit Pal,Jung-Oh Lee,Xiaoman Zhang,Malaikannan Sankarasubbu,Seunghyeon Roh,Won Jung Kim,Meesun Lee,Pranav Rajpurkar*

Main category: cs.CV

TL;DR: ReXVQA is a new, large-scale benchmark for VQA in chest radiology, evaluating AI models' performance on clinically relevant tasks and demonstrating superior AI accuracy compared to human radiology residents.


<details>
  <summary>Details</summary>
Motivation: To create a comprehensive benchmark for evaluating AI performance in VQA for chest radiology, focusing on clinically relevant tasks and comparison with human experts.

Method: Developed ReXVQA, a dataset of 696,000 questions and 160,000 chest X-rays, and evaluated 8 state-of-the-art multimodal models on five core radiological reasoning skills.

Result: The best model (MedGemma) achieved 83.24% overall accuracy, outperforming human radiology residents (77.27% accuracy), and showed distinct performance patterns compared to human experts.

Conclusion: ReXVQA sets a new standard for evaluating radiological AI systems, with public leaderboards and detailed evaluation metrics, paving the way for AI systems mimicking expert-level clinical reasoning.

Abstract: We present ReXVQA, the largest and most comprehensive benchmark for visual
question answering (VQA) in chest radiology, comprising approximately 696,000
questions paired with 160,000 chest X-rays studies across training, validation,
and test sets. Unlike prior efforts that rely heavily on template based
queries, ReXVQA introduces a diverse and clinically authentic task suite
reflecting five core radiological reasoning skills: presence assessment,
location analysis, negation detection, differential diagnosis, and geometric
reasoning. We evaluate eight state-of-the-art multimodal large language models,
including MedGemma-4B-it, Qwen2.5-VL, Janus-Pro-7B, and Eagle2-9B. The
best-performing model (MedGemma) achieves 83.24% overall accuracy. To bridge
the gap between AI performance and clinical expertise, we conducted a
comprehensive human reader study involving 3 radiology residents on 200
randomly sampled cases. Our evaluation demonstrates that MedGemma achieved
superior performance (83.84% accuracy) compared to human readers (best
radiology resident: 77.27%), representing a significant milestone where AI
performance exceeds expert human evaluation on chest X-ray interpretation. The
reader study reveals distinct performance patterns between AI models and human
experts, with strong inter-reader agreement among radiologists while showing
more variable agreement patterns between human readers and AI models. ReXVQA
establishes a new standard for evaluating generalist radiological AI systems,
offering public leaderboards, fine-grained evaluation splits, structured
explanations, and category-level breakdowns. This benchmark lays the foundation
for next-generation AI systems capable of mimicking expert-level clinical
reasoning beyond narrow pathology classification. Our dataset will be
open-sourced at https://huggingface.co/datasets/rajpurkarlab/ReXVQA

</details>


### [6] [WorldPrediction: A Benchmark for High-level World Modeling and Long-horizon Procedural Planning](https://arxiv.org/abs/2506.04363)
*Delong Chen,Willy Chung,Yejin Bang,Ziwei Ji,Pascale Fung*

Main category: cs.CV

TL;DR: This study introduces WorldPrediction, a video-based benchmark to evaluate AI models' world modeling and procedural planning skills, emphasizing temporal and semantic abstraction. It assesses the models' ability to distinguish correct actions or sequences from distractors using visual observations, with a formal framework for reliability and robustness.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the gap in current AI models' ability to learn internal world models for action planning, particularly in diverse environments, and to provide a more comprehensive evaluation framework that goes beyond low-level world modeling and robotic motion planning.

Method: The method involves creating a video-based benchmark called WorldPrediction, which presents AI models with initial and final world states and asks them to choose the correct action or sequence of actions from a set of options, including counterfactual distractors. The benchmark uses visual observations and 'action equivalents' to prevent reliance on low-level cues.

Result: Current state-of-the-art models achieve only 57% accuracy on the WorldPrediction-WM task (distinguishing proper actions) and 38% on the WorldPrediction-PP task (ordering sequences of actions), while humans can solve both tasks perfectly.

Conclusion: The benchmark highlights the limitations of current AI models in performing complex procedural planning and world modeling tasks, and provides a new tool for researchers to evaluate and improve these capabilities in AI systems.

Abstract: Humans are known to have an internal "world model" that enables us to carry
out action planning based on world states. AI agents need to have such a world
model for action planning as well. It is not clear how current AI models,
especially generative models, are able to learn such world models and carry out
procedural planning in diverse environments. We introduce WorldPrediction, a
video-based benchmark for evaluating world modeling and procedural planning
capabilities of different AI models. In contrast to prior benchmarks that focus
primarily on low-level world modeling and robotic motion planning,
WorldPrediction is the first benchmark that emphasizes actions with temporal
and semantic abstraction. Given initial and final world states, the task is to
distinguish the proper action (WorldPrediction-WM) or the properly ordered
sequence of actions (WorldPrediction-PP) from a set of counterfactual
distractors. This discriminative task setup enable us to evaluate different
types of world models and planners and realize a thorough comparison across
different hypothesis. The benchmark represents states and actions using visual
observations. In order to prevent models from exploiting low-level continuity
cues in background scenes, we provide "action equivalents" - identical actions
observed in different contexts - as candidates for selection. This benchmark is
grounded in a formal framework of partially observable semi-MDP, ensuring
better reliability and robustness of the evaluation. We conduct extensive human
filtering and validation on our benchmark and show that current frontier models
barely achieve 57% accuracy on WorldPrediction-WM and 38% on WorldPrediction-PP
whereas humans are able to solve both tasks perfectly.

</details>


### [7] [Puck Localization Using Contextual Cues](https://arxiv.org/abs/2506.04365)
*Liam Salass,Jerrin Bright,Amir Nazemi,Yuhao Chen,John Zelek,David Clausi*

Main category: cs.CV

TL;DR: The paper presents PLUCC, a novel method for puck detection in ice hockey broadcasts that uses player behavior as contextual cues, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Puck detection in ice hockey broadcasts is challenging due to the puck's small size, frequent occlusions, motion blur, and varying camera viewpoints. Previous methods focus on appearance or motion cues but do not leverage player behavior, which can provide strong contextual information.

Method: PLUCC consists of a contextual encoder that uses player orientations and positions, a feature pyramid encoder for multiscale feature extraction, and a gating decoder that combines these features. It evaluates using a scale-invariant metric called Rink Space Localization Error (RSLE).

Result: PLUCC outperforms previous methods by achieving a 12.2% improvement in average precision and a 25% improvement in RSLE average precision on the PuckDataset.

Conclusion: The study highlights the importance of contextual cues in improving puck detection, with implications for broader sports analysis applications.

Abstract: Puck detection in ice hockey broadcast videos poses significant challenges
due to the puck's small size, frequent occlusions, motion blur, broadcast
artifacts, and scale inconsistencies due to varying camera zoom and broadcast
camera viewpoints. Prior works focus on appearance-based or motion-based cues
of the puck without explicitly modelling the cues derived from player
behaviour. Players consistently turn their bodies and direct their gaze toward
the puck. Motivated by this strong contextual cue, we propose Puck Localization
Using Contextual Cues (PLUCC), a novel approach for scale-aware and
context-driven single-frame puck detections. PLUCC consists of three
components: (a) a contextual encoder, which utilizes player orientations and
positioning as helpful priors; (b) a feature pyramid encoder, which extracts
multiscale features from the dual encoders; and (c) a gating decoder that
combines latent features with a channel gating mechanism. For evaluation, in
addition to standard average precision, we propose Rink Space Localization
Error (RSLE), a scale-invariant homography-based metric for removing
perspective bias from rink space evaluation. The experimental results of PLUCC
on the PuckDataset dataset demonstrated state-of-the-art detection performance,
surpassing previous baseline methods by an average precision improvement of
12.2\% and RSLE average precision of 25\%. Our research demonstrates the
critical role of contextual understanding in improving puck detection
performance, with broad implications for automated sports analysis.

</details>


### [8] [Fine-Tuning Video Transformers for Word-Level Bangla Sign Language: A Comparative Analysis for Classification Tasks](https://arxiv.org/abs/2506.04367)
*Jubayer Ahmed Bhuiyan Shawon,Hasan Mahmud,Kamrul Hasan*

Main category: cs.CV

TL;DR: The study fine-tunes video transformer architectures on BdSL datasets, achieving high accuracy in Bangla Sign Language recognition, with VideoMAE outperforming other models.


<details>
  <summary>Details</summary>
Motivation: To improve accessibility for the hearing-impaired community in Bangladesh by fine-tuning state-of-the-art video transformer architectures on BdSL datasets and evaluating their performance on various datasets.

Method: The study fine-tuned VideoMAE, ViViT, and TimeSformer on BdSLW60 and BdSLW401 datasets, applied data augmentation techniques, and used 10-fold stratified cross-validation for model selection, with signer-independent evaluation on held-out test data.

Result: Video transformer models significantly outperformed traditional approaches, with VideoMAE achieving the highest accuracies of 95.5% on BdSLW60 and 81.04% on BdSLW401.

Conclusion: The study demonstrates the strong potential of video transformer models, particularly VideoMAE, for scalable and accurate BdSL recognition, highlighting the importance of dataset size and video quality.

Abstract: Sign Language Recognition (SLR) involves the automatic identification and
classification of sign gestures from images or video, converting them into text
or speech to improve accessibility for the hearing-impaired community. In
Bangladesh, Bangla Sign Language (BdSL) serves as the primary mode of
communication for many individuals with hearing impairments. This study
fine-tunes state-of-the-art video transformer architectures -- VideoMAE, ViViT,
and TimeSformer -- on BdSLW60 (arXiv:2402.08635), a small-scale BdSL dataset
with 60 frequent signs. We standardized the videos to 30 FPS, resulting in
9,307 user trial clips. To evaluate scalability and robustness, the models were
also fine-tuned on BdSLW401 (arXiv:2503.02360), a large-scale dataset with 401
sign classes. Additionally, we benchmark performance against public datasets,
including LSA64 and WLASL. Data augmentation techniques such as random
cropping, horizontal flipping, and short-side scaling were applied to improve
model robustness. To ensure balanced evaluation across folds during model
selection, we employed 10-fold stratified cross-validation on the training set,
while signer-independent evaluation was carried out using held-out test data
from unseen users U4 and U8. Results show that video transformer models
significantly outperform traditional machine learning and deep learning
approaches. Performance is influenced by factors such as dataset size, video
quality, frame distribution, frame rate, and model architecture. Among the
models, the VideoMAE variant (MCG-NJU/videomae-base-finetuned-kinetics)
achieved the highest accuracies of 95.5% on the frame rate corrected BdSLW60
dataset and 81.04% on the front-facing signs of BdSLW401 -- demonstrating
strong potential for scalable and accurate BdSL recognition.

</details>


### [9] [Visualizing and Controlling Cortical Responses Using Voxel-Weighted Activation Maximization](https://arxiv.org/abs/2506.04379)
*Matthew W. Shinkle,Mark D. Lescroart*

Main category: cs.CV

TL;DR: This paper shows that activation maximization, a technique used to interpret deep neural networks, can be applied to DNN-based encoding models of the human brain, generating images that drive activity in specific brain regions and enabling better understanding of the human visual system.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the interpretability of DNN-based encoding models of the human visual system, which can predict brain responses to visual stimuli but offer limited insight into the specific features driving these responses.

Method: The method involves using activation maximization to generate images optimized for predicted fMRI responses in individual cortical voxels and whole brain regions, using a pretrained Inception V3 network and linear regression to predict fMRI responses.

Result: The generated images reliably drive activity in targeted brain regions across both low- and high-level visual areas and across subjects, demonstrating the effectiveness of using activation maximization for DNN-based encoding models.

Conclusion: The paper concludes that activation maximization can be successfully applied to DNN-based encoding models, addressing limitations of alternative approaches and enabling flexible characterization and modulation of responses in the human visual system.

Abstract: Deep neural networks (DNNs) trained on visual tasks develop feature
representations that resemble those in the human visual system. Although
DNN-based encoding models can accurately predict brain responses to visual
stimuli, they offer limited insight into the specific features driving these
responses. Here, we demonstrate that activation maximization -- a technique
designed to interpret vision DNNs -- can be applied to DNN-based encoding
models of the human brain. We extract and adaptively downsample activations
from multiple layers of a pretrained Inception V3 network, then use linear
regression to predict fMRI responses. This yields a full image-computable model
of brain responses. Next, we apply activation maximization to generate images
optimized for predicted responses in individual cortical voxels. We find that
these images contain visual characteristics that qualitatively correspond with
known selectivity and enable exploration of selectivity across the visual
cortex. We further extend our method to whole regions of interest (ROIs) of the
brain and validate its efficacy by presenting these images to human
participants in an fMRI study. We find that the generated images reliably drive
activity in targeted regions across both low- and high-level visual areas and
across subjects. These results demonstrate that activation maximization can be
successfully applied to DNN-based encoding models. By addressing key
limitations of alternative approaches that require natively generative models,
our approach enables flexible characterization and modulation of responses
across the human visual system.

</details>


### [10] [Is Perturbation-Based Image Protection Disruptive to Image Editing?](https://arxiv.org/abs/2506.04394)
*Qiuyu Tang,Bonor Ayambem,Mooi Choo Chuah,Aparna Bharati*

Main category: cs.CV

TL;DR: Current image protection methods using imperceptible perturbations do not fully prevent diffusion-based editing, and can sometimes lead to better-edited images, suggesting that perturbation-based methods are insufficient for robust image protection.


<details>
  <summary>Details</summary>
Motivation: To address the misuse of image generation models like Stable Diffusion, which can be used to spread misinformation and plagiarize copyrighted materials, the paper explores the effectiveness of current image protection methods that rely on adding imperceptible perturbations to images to obstruct diffusion-based editing.

Method: The researchers conducted experiments with various perturbation-based image protection methods across multiple domains (natural scene images and artworks) and editing tasks (image-to-image generation and style editing) to evaluate their effectiveness in preventing diffusion-based editing.

Result: The experiments revealed that in most scenarios, diffusion-based editing of protected images still generates desirable output images that adhere to the guidance prompts. Furthermore, adding noise to images may paradoxically increase their association with given text prompts during the generation process, leading to better resultant edits.

Conclusion: The findings suggest that perturbation-based methods are not a sufficient solution for robust image protection against diffusion-based editing, highlighting the need for alternative approaches.

Abstract: The remarkable image generation capabilities of state-of-the-art diffusion
models, such as Stable Diffusion, can also be misused to spread misinformation
and plagiarize copyrighted materials. To mitigate the potential risks
associated with image editing, current image protection methods rely on adding
imperceptible perturbations to images to obstruct diffusion-based editing. A
fully successful protection for an image implies that the output of editing
attempts is an undesirable, noisy image which is completely unrelated to the
reference image. In our experiments with various perturbation-based image
protection methods across multiple domains (natural scene images and artworks)
and editing tasks (image-to-image generation and style editing), we discover
that such protection does not achieve this goal completely. In most scenarios,
diffusion-based editing of protected images generates a desirable output image
which adheres precisely to the guidance prompt. Our findings suggest that
adding noise to images may paradoxically increase their association with given
text prompts during the generation process, leading to unintended consequences
such as better resultant edits. Hence, we argue that perturbation-based methods
may not provide a sufficient solution for robust image protection against
diffusion-based editing.

</details>


### [11] [Normalize Filters! Classical Wisdom for Deep Vision](https://arxiv.org/abs/2506.04401)
*Gustavo Perez,Stella X. Yu*

Main category: cs.CV

TL;DR: The paper introduces filter normalization in deep networks to address the issue of filter responses becoming distorted during atmospheric transfer, leading to incorrect outcomes. This method, inspired by batch normalization, ensures atmosphere-equivariant filters and improves performance on various benchmarks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of unnormalized convolutional filters in deep networks, which can lead to artifacts and incorrect responses when images undergo atmospheric transfer. The authors aim to integrate classical filtering principles into deep learning to enhance the robustness and generalization of these models.

Method: The proposed method involves normalizing the convolutional filters, followed by learnable scaling and shifting, similar to batch normalization. This ensures that the filters are atmosphere-equivariant and maintains co-domain symmetry. The method is applied to both convolutional neural networks and convolution-dependent vision transformers.

Result: The method achieves significant improvements on artificial and natural intensity variation benchmarks. Specifically, a ResNet34 model with the proposed normalization outperforms CLIP by a large margin. The analysis shows that unnormalized filters degrade performance, while filter normalization regularizes learning, promotes diversity, and enhances robustness and generalization.

Conclusion: The authors conclude that filter normalization is a simple yet effective modification that integrates classical filtering principles into deep learning. It improves the performance and robustness of deep networks, making them more reliable in various imaging conditions.

Abstract: Classical image filters, such as those for averaging or differencing, are
carefully normalized to ensure consistency, interpretability, and to avoid
artifacts like intensity shifts, halos, or ringing. In contrast, convolutional
filters learned end-to-end in deep networks lack such constraints. Although
they may resemble wavelets and blob/edge detectors, they are not normalized in
the same or any way. Consequently, when images undergo atmospheric transfer,
their responses become distorted, leading to incorrect outcomes. We address
this limitation by proposing filter normalization, followed by learnable
scaling and shifting, akin to batch normalization. This simple yet effective
modification ensures that the filters are atmosphere-equivariant, enabling
co-domain symmetry. By integrating classical filtering principles into deep
learning (applicable to both convolutional neural networks and
convolution-dependent vision transformers), our method achieves significant
improvements on artificial and natural intensity variation benchmarks. Our
ResNet34 could even outperform CLIP by a large margin. Our analysis reveals
that unnormalized filters degrade performance, whereas filter normalization
regularizes learning, promotes diversity, and improves robustness and
generalization.

</details>


### [12] [HMAR: Efficient Hierarchical Masked Auto-Regressive Image Generation](https://arxiv.org/abs/2506.04421)
*Hermann Kumbong,Xian Liu,Tsung-Yi Lin,Ming-Yu Liu,Xihui Liu,Ziwei Liu,Daniel Y. Fu,Christopher Ré,David W. Romero*

Main category: cs.CV

TL;DR: HMAR is a new image generation algorithm that addresses the issues of VAR by using next-scale prediction and masked generation, improving image quality and efficiency.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the visual quality and efficiency of image generation, addressing the limitations of previous methods such as VAR, which suffer from reduced image quality, superlinear sequence length scaling, and the need for retraining to change sampling schedules.

Method: HMAR reformulates next-scale prediction as a Markovian process, conditioning each resolution scale on its immediate predecessor and using a multi-step masked generation procedure to generate a subset of tokens in each step.

Result: HMAR matches or outperforms parameter-matched VAR, diffusion, and autoregressive baselines on ImageNet 256x256 and 512x512 benchmarks, and achieves faster training and inference times with a lower memory footprint.

Conclusion: HMAR provides additional flexibility, allowing for changes in sampling schedules without retraining and enabling zero-shot image editing.

Abstract: Visual Auto-Regressive modeling (VAR) has shown promise in bridging the speed
and quality gap between autoregressive image models and diffusion models. VAR
reformulates autoregressive modeling by decomposing an image into successive
resolution scales. During inference, an image is generated by predicting all
the tokens in the next (higher-resolution) scale, conditioned on all tokens in
all previous (lower-resolution) scales. However, this formulation suffers from
reduced image quality due to the parallel generation of all tokens in a
resolution scale; has sequence lengths scaling superlinearly in image
resolution; and requires retraining to change the sampling schedule.
  We introduce Hierarchical Masked Auto-Regressive modeling (HMAR), a new image
generation algorithm that alleviates these issues using next-scale prediction
and masked prediction to generate high-quality images with fast sampling. HMAR
reformulates next-scale prediction as a Markovian process, wherein the
prediction of each resolution scale is conditioned only on tokens in its
immediate predecessor instead of the tokens in all predecessor resolutions.
When predicting a resolution scale, HMAR uses a controllable multi-step masked
generation procedure to generate a subset of the tokens in each step. On
ImageNet 256x256 and 512x512 benchmarks, HMAR models match or outperform
parameter-matched VAR, diffusion, and autoregressive baselines. We develop
efficient IO-aware block-sparse attention kernels that allow HMAR to achieve
faster training and inference times over VAR by over 2.5x and 1.75x
respectively, as well as over 3x lower inference memory footprint. Finally,
HMAR yields additional flexibility over VAR; its sampling schedule can be
changed without further training, and it can be applied to image editing tasks
in a zero-shot manner.

</details>


### [13] [Photoreal Scene Reconstruction from an Egocentric Device](https://arxiv.org/abs/2506.04444)
*Zhaoyang Lv,Maurizio Monge,Ka Chen,Yufeng Zhu,Michael Goesele,Jakob Engel,Zhao Dong,Richard Newcombe*

Main category: cs.CV

TL;DR: This paper addresses the challenges of photorealistic scene reconstruction using egocentric devices, introducing VIBA for precise camera calibration and a physical image formation model for Gaussian Splatting, leading to improved PSNR.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the limitations of existing methodologies that often neglect crucial details in high dynamic range photorealistic scene reconstruction using egocentric devices.

Method: The paper introduces two key methods: 1) Visual-Inertial Bundle Adjustment (VIBA) to calibrate the precise timestamps and movement of the rolling-shutter RGB camera, and 2) a physical image formation model integrated into Gaussian Splatting to address sensor characteristics and dynamic ranges.

Result: The proposed methods result in a consistent visual enhancement of +1 dB in PSNR through VIBA, with an additional +1 dB achieved through the physical image formation model.

Conclusion: The study concludes that the proposed methods significantly improve the accuracy and quality of photorealistic scene reconstruction using egocentric devices, as validated through comprehensive evaluations on both Project Aria and Meta Quest3 devices.

Abstract: In this paper, we investigate the challenges associated with using egocentric
devices to photorealistic reconstruct the scene in high dynamic range. Existing
methodologies typically assume using frame-rate 6DoF pose estimated from the
device's visual-inertial odometry system, which may neglect crucial details
necessary for pixel-accurate reconstruction. This study presents two
significant findings. Firstly, in contrast to mainstream work treating RGB
camera as global shutter frame-rate camera, we emphasize the importance of
employing visual-inertial bundle adjustment (VIBA) to calibrate the precise
timestamps and movement of the rolling shutter RGB sensing camera in a high
frequency trajectory format, which ensures an accurate calibration of the
physical properties of the rolling-shutter camera. Secondly, we incorporate a
physical image formation model based into Gaussian Splatting, which effectively
addresses the sensor characteristics, including the rolling-shutter effect of
RGB cameras and the dynamic ranges measured by sensors. Our proposed
formulation is applicable to the widely-used variants of Gaussian Splats
representation. We conduct a comprehensive evaluation of our pipeline using the
open-source Project Aria device under diverse indoor and outdoor lighting
conditions, and further validate it on a Meta Quest3 device. Across all
experiments, we observe a consistent visual enhancement of +1 dB in PSNR by
incorporating VIBA, with an additional +1 dB achieved through our proposed
image formation model. Our complete implementation, evaluation datasets, and
recording profile are available at
http://www.projectaria.com/photoreal-reconstruction/

</details>


### [14] [Towards Large-Scale Pose-Invariant Face Recognition Using Face Defrontalization](https://arxiv.org/abs/2506.04496)
*Patrik Mesec,Alan Jović*

Main category: cs.CV

TL;DR: The paper proposes a novel method called face defrontalization to augment the training dataset of facial feature extraction models, leading to improved pose-invariant face recognition performance on large-scale datasets.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of pose-invariant face recognition, this paper aims to improve the robustness of face recognition systems across different head poses, particularly in real-life settings where existing methods are impractical.

Method: The proposed method involves training a face defrontalization model (adapted from the FFWM model) to augment the training data for a ResNet-50 facial feature extraction model, which is trained using ArcFace loss. The defrontalization step is applied during training but not during inference to avoid time overhead.

Result: The method outperformed existing approaches on three large-scale open-access datasets (LFW, AgeDB, and CFP), but not on the small Multi-PIE dataset for extreme poses (75 and 90 degrees). This suggests that the proposed method is more effective on larger datasets and that some existing methods may be overfitted to small datasets.

Conclusion: The proposed face defrontalization method effectively augments training data, leading to improved pose-invariant face recognition performance on large-scale datasets. However, it does not outperform existing methods on small datasets with extreme poses, indicating a need for further investigation and potential improvements.

Abstract: Face recognition under extreme head poses is a challenging task. Ideally, a
face recognition system should perform well across different head poses, which
is known as pose-invariant face recognition. To achieve pose invariance,
current approaches rely on sophisticated methods, such as face frontalization
and various facial feature extraction model architectures. However, these
methods are somewhat impractical in real-life settings and are typically
evaluated on small scientific datasets, such as Multi-PIE. In this work, we
propose the inverse method of face frontalization, called face
defrontalization, to augment the training dataset of facial feature extraction
model. The method does not introduce any time overhead during the inference
step. The method is composed of: 1) training an adapted face defrontalization
FFWM model on a frontal-profile pairs dataset, which has been preprocessed
using our proposed face alignment method; 2) training a ResNet-50 facial
feature extraction model based on ArcFace loss on a raw and randomly
defrontalized large-scale dataset, where defrontalization was performed with
our previously trained face defrontalization model. Our method was compared
with the existing approaches on four open-access datasets: LFW, AgeDB, CFP, and
Multi-PIE. Defrontalization shows improved results compared to models without
defrontalization, while the proposed adjustments show clear superiority over
the state-of-the-art face frontalization FFWM method on three larger
open-access datasets, but not on the small Multi-PIE dataset for extreme poses
(75 and 90 degrees). The results suggest that at least some of the current
methods may be overfitted to small datasets.

</details>


### [15] [FALO: Fast and Accurate LiDAR 3D Object Detection on Resource-Constrained Devices](https://arxiv.org/abs/2506.04499)
*Shizhong Han,Hsin-Pai Cheng,Hong Cai,Jihad Masri,Soyeb Nagori,Fatih Porikli*

Main category: cs.CV

TL;DR: A new LiDAR 3D object detection method, FALO, is proposed, offering high accuracy and fast inference suitable for resource-constrained edge devices.


<details>
  <summary>Details</summary>
Motivation: To address the high computational costs and irregular memory access patterns of existing LiDAR 3D object detection methods, which make them challenging to run on edge devices.

Method: FALO arranges sparse 3D voxels into a 1D sequence, processes them using ConvDotMix blocks, and introduces implicit grouping to balance tensor dimensions and account for the growing receptive field.

Result: FALO achieves competitive performance on benchmarks like nuScenes and Waymo, while being 1.6~9.8x faster than the latest SOTA on mobile GPUs and NPUs.

Conclusion: FALO is a hardware-friendly approach to LiDAR 3D detection, providing both high accuracy and fast inference speeds, suitable for deployment on compact, embedded devices.

Abstract: Existing LiDAR 3D object detection methods predominantely rely on sparse
convolutions and/or transformers, which can be challenging to run on
resource-constrained edge devices, due to irregular memory access patterns and
high computational costs. In this paper, we propose FALO, a hardware-friendly
approach to LiDAR 3D detection, which offers both state-of-the-art (SOTA)
detection accuracy and fast inference speed. More specifically, given the 3D
point cloud and after voxelization, FALO first arranges sparse 3D voxels into a
1D sequence based on their coordinates and proximity. The sequence is then
processed by our proposed ConvDotMix blocks, consisting of large-kernel
convolutions, Hadamard products, and linear layers. ConvDotMix provides
sufficient mixing capability in both spatial and embedding dimensions, and
introduces higher-order nonlinear interaction among spatial features.
Furthermore, when going through the ConvDotMix layers, we introduce implicit
grouping, which balances the tensor dimensions for more efficient inference and
takes into account the growing receptive field. All these operations are
friendly to run on resource-constrained platforms and proposed FALO can readily
deploy on compact, embedded devices. Our extensive evaluation on LiDAR 3D
detection benchmarks such as nuScenes and Waymo shows that FALO achieves
competitive performance. Meanwhile, FALO is 1.6~9.8x faster than the latest
SOTA on mobile Graphics Processing Unit (GPU) and mobile Neural Processing Unit
(NPU).

</details>


### [16] [AuthGuard: Generalizable Deepfake Detection via Language Guidance](https://arxiv.org/abs/2506.04501)
*Guangyu Shen,Zhihua Li,Xiang Xu,Tianchen Zhao,Zheng Zhang,Dongsheng An,Zhuowen Tu,Yifan Xing,Qin Zhang*

Main category: cs.CV

TL;DR: The paper proposes AuthGuard, a deepfake detection method that uses a vision encoder trained with language guidance to improve generalization and accuracy across different datasets and unseen forgeries.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current deepfake detection methods, which rely on statistical artifacts and struggle with new, unseen deepfake generation techniques, the authors introduce language guidance to incorporate human-like commonsense reasoning.

Method: The method combines a vision encoder trained with discriminative classification and image-text contrastive learning, leveraging text generated by generalist MLLMs. Data uncertainty learning is also integrated to improve robustness. The vision encoder interfaces with an LLM to enhance generalized and interpretable deepfake detection.

Result: AuthGuard achieves state-of-the-art deepfake detection accuracy, with AUC gains of 6.15% on the DFDC dataset and 16.68% on the DF40 dataset. It also significantly improves deepfake reasoning, with a 24.69% performance gain on the DDVQA dataset.

Conclusion: The proposed AuthGuard framework demonstrates significant improvements in deepfake detection and reasoning, making it more robust and accurate in both in-distribution and out-of-distribution settings.

Abstract: Existing deepfake detection techniques struggle to keep-up with the
ever-evolving novel, unseen forgeries methods. This limitation stems from their
reliance on statistical artifacts learned during training, which are often tied
to specific generation processes that may not be representative of samples from
new, unseen deepfake generation methods encountered at test time. We propose
that incorporating language guidance can improve deepfake detection
generalization by integrating human-like commonsense reasoning -- such as
recognizing logical inconsistencies and perceptual anomalies -- alongside
statistical cues. To achieve this, we train an expert deepfake vision encoder
by combining discriminative classification with image-text contrastive
learning, where the text is generated by generalist MLLMs using few-shot
prompting. This allows the encoder to extract both language-describable,
commonsense deepfake artifacts and statistical forgery artifacts from
pixel-level distributions. To further enhance robustness, we integrate data
uncertainty learning into vision-language contrastive learning, mitigating
noise in image-text supervision. Our expert vision encoder seamlessly
interfaces with an LLM, further enabling more generalized and interpretable
deepfake detection while also boosting accuracy. The resulting framework,
AuthGuard, achieves state-of-the-art deepfake detection accuracy in both
in-distribution and out-of-distribution settings, achieving AUC gains of 6.15%
on the DFDC dataset and 16.68% on the DF40 dataset. Additionally, AuthGuard
significantly enhances deepfake reasoning, improving performance by 24.69% on
the DDVQA dataset.

</details>


### [17] [Pruning Everything, Everywhere, All at Once](https://arxiv.org/abs/2506.04513)
*Gustavo Henrique do Nascimento,Ian Pons,Anna Helena Reali Costa,Artur Jordao*

Main category: cs.CV

TL;DR: A new method is proposed to simultaneously prune different structures within deep learning models, such as layers and neurons, using the Centered Kernel Alignment metric to maintain high representation similarity with the original model. The method iteratively prunes subnetworks, achieving significant FLOPs reduction and preserving or improving accuracy, while also enhancing robustness and reducing carbon emissions.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the limitations of existing pruning methods that either prune neurons or layers, but not both together, to achieve more efficient and compact models while maintaining or improving performance.

Method: The proposed method involves creating two candidate subnetworks, one pruned by layers and the other by neurons, and selecting the one with the highest representation similarity to the original model using the Centered Kernel Alignment metric. This process is iteratively applied to produce highly sparse models.

Result: The method outperforms state-of-the-art layer and filter pruning techniques, achieving 86.37% and 95.82% FLOPs reduction on ResNet56 and ResNet110, respectively, with minimal accuracy degradation or even improvement. It also enhances robustness to adversarial and out-of-distribution samples and reduces carbon emissions by up to 83.31%.

Conclusion: The proposed method opens a new chapter in pruning by enabling simultaneous layer and neuron pruning, significantly reducing computational complexity while preserving or improving model performance and contributing to GreenAI.

Abstract: Deep learning stands as the modern paradigm for solving cognitive tasks.
However, as the problem complexity increases, models grow deeper and
computationally prohibitive, hindering advancements in real-world and
resource-constrained applications. Extensive studies reveal that pruning
structures in these models efficiently reduces model complexity and improves
computational efficiency. Successful strategies in this sphere include removing
neurons (i.e., filters, heads) or layers, but not both together. Therefore,
simultaneously pruning different structures remains an open problem. To fill
this gap and leverage the benefits of eliminating neurons and layers at once,
we propose a new method capable of pruning different structures within a model
as follows. Given two candidate subnetworks (pruned models), one from layer
pruning and the other from neuron pruning, our method decides which to choose
by selecting the one with the highest representation similarity to its parent
(the network that generates the subnetworks) using the Centered Kernel
Alignment metric. Iteratively repeating this process provides highly sparse
models that preserve the original predictive ability. Throughout extensive
experiments on standard architectures and benchmarks, we confirm the
effectiveness of our approach and show that it outperforms state-of-the-art
layer and filter pruning techniques. At high levels of Floating Point
Operations reduction, most state-of-the-art methods degrade accuracy, whereas
our approach either improves it or experiences only a minimal drop. Notably, on
the popular ResNet56 and ResNet110, we achieve a milestone of 86.37% and 95.82%
FLOPs reduction. Besides, our pruned models obtain robustness to adversarial
and out-of-distribution samples and take an important step towards GreenAI,
reducing carbon emissions by up to 83.31%. Overall, we believe our work opens a
new chapter in pruning.

</details>


### [18] [EECD-Net: Energy-Efficient Crack Detection with Spiking Neural Networks and Gated Attention](https://arxiv.org/abs/2506.04526)
*Shuo Zhang*

Main category: cs.CV

TL;DR: The paper introduces EECD-Net, a multi-stage detection approach for road crack detection that improves accuracy and energy efficiency using SRCNN, SCU, and GAT modules. It achieves 98.6% accuracy and consumes 5.6 mJ, a 33% reduction in energy compared to baseline methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges of real-time road crack detection in resource-constrained environments, where smart terminal devices face issues with limited energy and low-resolution imaging.

Method: EECD-Net combines a Super-Resolution Convolutional Neural Network (SRCNN) to enhance image resolution, a Spike Convolution Unit (SCU) with Continuous Integrate-and-Fire (CIF) neurons to reduce power consumption, and a Gated Attention Transformer (GAT) module to capture both long-range and local crack patterns.

Result: The proposed EECD-Net achieves 98.6% detection accuracy on the CrackVision12K benchmark, surpassing state-of-the-art methods like Hybrid-Segmentor by 1.5%. It also demonstrates a 33% reduction in energy consumption compared to baseline implementations.

Conclusion: The paper presents a transformative approach to instrumentation-based crack detection, offering a scalable and low-power solution for real-time infrastructure monitoring in resource-constrained environments.

Abstract: Crack detection on road surfaces is a critical measurement technology in the
instrumentation domain, essential for ensuring infrastructure safety and
transportation reliability. However, due to limited energy and low-resolution
imaging, smart terminal devices struggle to maintain real-time monitoring
performance. To overcome these challenges, this paper proposes a multi-stage
detection approach for road crack detection, EECD-Net, to enhance accuracy and
energy efficiency of instrumentation. Specifically, the sophisticated
Super-Resolution Convolutional Neural Network (SRCNN) is employed to address
the inherent challenges of low-quality images, which effectively enhance image
resolution while preserving critical structural details. Meanwhile, a Spike
Convolution Unit (SCU) with Continuous Integrate-and-Fire (CIF) neurons is
proposed to convert these images into sparse pulse sequences, significantly
reducing power consumption. Additionally, a Gated Attention Transformer (GAT)
module is designed to strategically fuse multi-scale feature representations
through adaptive attention mechanisms, effectively capturing both long-range
dependencies and intricate local crack patterns, and significantly enhancing
detection robustness across varying crack morphologies. The experiments on the
CrackVision12K benchmark demonstrate that EECD-Net achieves a remarkable 98.6\%
detection accuracy, surpassing state-of-the-art counterparts such as
Hybrid-Segmentor by a significant 1.5\%. Notably, the EECD-Net maintains
exceptional energy efficiency, consuming merely 5.6 mJ, which is a substantial
33\% reduction compared to baseline implementations. This work pioneers a
transformative approach in instrumentation-based crack detection, offering a
scalable, low-power solution for real-time, large-scale infrastructure
monitoring in resource-constrained environments.

</details>


### [19] [Enhancing Frequency for Single Image Super-Resolution with Learnable Separable Kernels](https://arxiv.org/abs/2506.04555)
*Heng Tian*

Main category: cs.CV

TL;DR: The paper introduces Learnable Separable Kernels (LSKs) to directly enhance image frequency components in single-image super-resolution (SISR) tasks, reducing parameters and computational requirements while improving performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance the performance of SISR methods by directly manipulating image frequency components, addressing the limitations of existing approaches that rely on auxiliary structures.

Method: LSKs are rank-one matrices that decompose into orthogonal and mergeable one-dimensional kernels, directly enhancing image frequency components. They are integrated into baseline SISR methods to reduce parameters and computational load.

Result: Incorporating LSKs leads to a significant reduction of over 60% in parameters and computational requirements, while improving overall model performance, especially at higher upscaling factors.

Conclusion: LSKs effectively enhance image frequency components, reduce computational complexity, and improve SISR performance, making them a valuable addition to existing SISR methods.

Abstract: Existing approaches often enhance the performance of single-image
super-resolution (SISR) methods by incorporating auxiliary structures, such as
specialized loss functions, to indirectly boost the quality of low-resolution
images. In this paper, we propose a plug-and-play module called Learnable
Separable Kernels (LSKs), which are formally rank-one matrices designed to
directly enhance image frequency components. We begin by explaining why LSKs
are particularly suitable for SISR tasks from a frequency perspective. Baseline
methods incorporating LSKs demonstrate a significant reduction of over 60\% in
both the number of parameters and computational requirements. This reduction is
achieved through the decomposition of LSKs into orthogonal and mergeable
one-dimensional kernels. Additionally, we perform an interpretable analysis of
the feature maps generated by LSKs. Visualization results reveal the capability
of LSKs to enhance image frequency components effectively. Extensive
experiments show that incorporating LSKs not only reduces the number of
parameters and computational load but also improves overall model performance.
Moreover, these experiments demonstrate that models utilizing LSKs exhibit
superior performance, particularly as the upscaling factor increases.

</details>


### [20] [Perceptual Decoupling for Scalable Multi-modal Reasoning via Reward-Optimized Captioning](https://arxiv.org/abs/2506.04559)
*Yunhao Gou,Kai Chen,Zhili Liu,Lanqing Hong,Xin Jin,Zhenguo Li,James T. Kwok,Yu Zhang*

Main category: cs.CV

TL;DR: The paper introduces RACRO, a method that enhances the alignment between visual perception and reasoning in multi-modal large language models by using a reasoning-guided reinforcement learning strategy, achieving state-of-the-art performance and better scalability.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of extending complex reasoning capabilities from text-based LLMs to multi-modal LLMs, specifically focusing on maintaining accurate and informative visual-to-text conversion for downstream reasoning tasks.

Method: RACRO uses a reasoning-guided reinforcement learning approach to optimize the captioning process, ensuring that the generated captions are both faithful to the visual content and informative for reasoning tasks, effectively closing the perception-reasoning loop.

Result: Experiments on multi-modal math and science benchmarks demonstrate that RACRO achieves state-of-the-art average performance and enables better scalability and adaptability to more advanced reasoning LLMs without the need for costly retraining.

Conclusion: The paper concludes that RACRO significantly improves visual grounding and reasoning-optimized representations, making it a promising approach for enhancing multi-modal LLMs while maintaining efficiency and adaptability.

Abstract: Recent advances in slow-thinking language models (e.g., OpenAI-o1 and
DeepSeek-R1) have demonstrated remarkable abilities in complex reasoning tasks
by emulating human-like reflective cognition. However, extending such
capabilities to multi-modal large language models (MLLMs) remains challenging
due to the high cost of retraining vision-language alignments when upgrading
the underlying reasoner LLMs. A straightforward solution is to decouple
perception from reasoning, i.e., converting visual inputs into language
representations (e.g., captions) that are then passed to a powerful text-only
reasoner. However, this decoupling introduces a critical challenge: the visual
extractor must generate descriptions that are both faithful to the image and
informative enough to support accurate downstream reasoning. To address this,
we propose Reasoning-Aligned Perceptual Decoupling via Caption Reward
Optimization (RACRO) - a reasoning-guided reinforcement learning strategy that
aligns the extractor's captioning behavior with the reasoning objective. By
closing the perception-reasoning loop via reward-based optimization, RACRO
significantly enhances visual grounding and extracts reasoning-optimized
representations. Experiments on multi-modal math and science benchmarks show
that the proposed RACRO method achieves state-of-the-art average performance
while enabling superior scalability and plug-and-play adaptation to more
advanced reasoning LLMs without the necessity for costly multi-modal
re-alignment.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [21] [Zero-Shot Adaptation of Parameter-Efficient Fine-Tuning in Diffusion Models](https://arxiv.org/abs/2506.04244)
*Farzad Farhadzadeh,Debasmit Das,Shubhankar Borse,Fatih Porikli*

Main category: cs.AI

TL;DR: ProLoRA enables zero-shot adaptation of parameter-efficient fine-tuning in text-to-image models by transferring pre-trained low-rank adjustments without additional data, achieving comparable performance to models requiring retraining.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of traditional methods that require retraining with additional data when switching base models, which can be challenging due to data constraints.

Method: ProLoRA transfers pre-trained low-rank adjustments (e.g., LoRA) from a source to a target model by projecting the source adjustments into the target model's weight space, leveraging subspace and null space similarities, and selectively targeting aligned layers.

Result: Evaluations on established text-to-image models demonstrate successful knowledge transfer and comparable performance without retraining.

Conclusion: ProLoRA effectively facilitates zero-shot adaptation of parameter-efficient fine-tuning in text-to-image models, making it a valuable tool for model switching without the need for additional training data.

Abstract: We introduce ProLoRA, enabling zero-shot adaptation of parameter-efficient
fine-tuning in text-to-image diffusion models. ProLoRA transfers pre-trained
low-rank adjustments (e.g., LoRA) from a source to a target model without
additional training data. This overcomes the limitations of traditional methods
that require retraining when switching base models, often challenging due to
data constraints. ProLoRA achieves this via projection of source adjustments
into the target model's weight space, leveraging subspace and null space
similarities and selectively targeting aligned layers. Evaluations on
established text-to-image models demonstrate successful knowledge transfer and
comparable performance without retraining.

</details>


### [22] [Contextual Integrity in LLMs via Reasoning and Reinforcement Learning](https://arxiv.org/abs/2506.04245)
*Guangchen Lan,Huseyin A. Inan,Sahar Abdelnabi,Janardhan Kulkarni,Lukas Wutschitz,Reza Shokri,Christopher G. Brinton,Robert Sim*

Main category: cs.AI

TL;DR: The paper introduces a reinforcement learning framework to instill contextual integrity (CI) reasoning in AI agents, reducing inappropriate information disclosure while maintaining task performance.


<details>
  <summary>Details</summary>
Motivation: With the rise of autonomous agents, ensuring that they make contextually appropriate decisions about information disclosure is crucial. The paper aims to address this by developing methods to train AI agents to reason about CI.

Method: The authors first prompt large language models (LLMs) to reason explicitly about CI. They then extend this approach by developing a reinforcement learning (RL) framework that further trains the models to achieve CI. The training uses a synthetic dataset of about 700 diverse examples.

Result: The method significantly reduces inappropriate information disclosure while maintaining task performance across different model sizes and families. The improvements also transfer to established CI benchmarks like PrivacyLens, which evaluates privacy leakage in AI assistants.

Conclusion: The paper demonstrates that the proposed RL framework effectively instills CI reasoning in AI agents, improving their ability to make contextually appropriate decisions about information disclosure.

Abstract: As the era of autonomous agents making decisions on behalf of users unfolds,
ensuring contextual integrity (CI) -- what is the appropriate information to
share while carrying out a certain task -- becomes a central question to the
field. We posit that CI demands a form of reasoning where the agent needs to
reason about the context in which it is operating. To test this, we first
prompt LLMs to reason explicitly about CI when deciding what information to
disclose. We then extend this approach by developing a reinforcement learning
(RL) framework that further instills in models the reasoning necessary to
achieve CI. Using a synthetic, automatically created, dataset of only $\sim700$
examples but with diverse contexts and information disclosure norms, we show
that our method substantially reduces inappropriate information disclosure
while maintaining task performance across multiple model sizes and families.
Importantly, improvements transfer from this synthetic dataset to established
CI benchmarks such as PrivacyLens that has human annotations and evaluates
privacy leakage of AI assistants in actions and tool calls.

</details>


### [23] [Language-Guided Multi-Agent Learning in Simulations: A Unified Framework and Evaluation](https://arxiv.org/abs/2506.04251)
*Zhengyang Li*

Main category: cs.AI

TL;DR: Introduces LLM-MARL, a framework integrating large language models with multi-agent reinforcement learning to improve coordination, communication, and generalization in game environments, showing significant performance gains.


<details>
  <summary>Details</summary>
Motivation: To enhance coordination, communication, and generalization in multi-agent systems by leveraging the capabilities of large language models.

Method: The LLM-MARL framework includes Coordinator, Communicator, and Memory components, using PPO with a language-conditioned loss and LLM query gating for training.

Result: Demonstrates consistent improvements over baseline methods in win rate, coordination score, and zero-shot generalization across multiple game environments.

Conclusion: Contributes to the design of intelligent, cooperative agents in interactive simulations and opens avenues for leveraging LLMs in multi-agent systems for training, games, and human-AI collaboration.

Abstract: This paper introduces LLM-MARL, a unified framework that incorporates large
language models (LLMs) into multi-agent reinforcement learning (MARL) to
enhance coordination, communication, and generalization in simulated game
environments. The framework features three modular components of Coordinator,
Communicator, and Memory, which dynamically generate subgoals, facilitate
symbolic inter-agent messaging, and support episodic recall. Training combines
PPO with a language-conditioned loss and LLM query gating. LLM-MARL is
evaluated in Google Research Football, MAgent Battle, and StarCraft II. Results
show consistent improvements over MAPPO and QMIX in win rate, coordination
score, and zero-shot generalization. Ablation studies demonstrate that subgoal
generation and language-based messaging each contribute significantly to
performance gains. Qualitative analysis reveals emergent behaviors such as role
specialization and communication-driven tactics. By bridging language modeling
and policy learning, this work contributes to the design of intelligent,
cooperative agents in interactive simulations. It offers a path forward for
leveraging LLMs in multi-agent systems used for training, games, and human-AI
collaboration.

</details>


### [24] [A Graph-Retrieval-Augmented Generation Framework Enhances Decision-Making in the Circular Economy](https://arxiv.org/abs/2506.04252)
*Yang Zhao,Chengxiao Dai,Dusit Niyato,Chuan Fu Tan,Keyi Xiang,Yueyang Wang,Zhiquan Yeo,Daren Tan Zong Loong,Jonathan Low Zhaozhi,Eugene H. Z. HO*

Main category: cs.AI

TL;DR: CircuGraphRAG is a retrieval-augmented generation framework that leverages a domain-specific knowledge graph to improve the accuracy and efficiency of large language models in the context of sustainable manufacturing and circular economy planning.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the issue of large language models hallucinating industrial codes and emission factors, which can affect regulatory and investment decisions in sustainable manufacturing.

Method: The method involves using a retrieval-augmented generation (RAG) framework that integrates a domain-specific knowledge graph containing industrial and waste entities, classification codes, and GWP100 emission data. Natural language queries are translated into SPARQL to retrieve verified subgraphs, ensuring accuracy and traceability.

Result: CircuGraphRAG outperforms standalone LLMs and naive RAG in single-hop and multi-hop question answering, achieving ROUGE-L F1 scores of up to 1.0. It also improves efficiency by reducing response time and token usage.

Conclusion: CircuGraphRAG provides fact-checked, regulatory-ready support for circular economy planning, advancing reliable, low-carbon resource decision making.

Abstract: Large language models (LLMs) hold promise for sustainable manufacturing, but
often hallucinate industrial codes and emission factors, undermining regulatory
and investment decisions. We introduce CircuGraphRAG, a retrieval-augmented
generation (RAG) framework that grounds LLMs outputs in a domain-specific
knowledge graph for the circular economy. This graph connects 117,380
industrial and waste entities with classification codes and GWP100 emission
data, enabling structured multi-hop reasoning. Natural language queries are
translated into SPARQL and verified subgraphs are retrieved to ensure accuracy
and traceability. Compared with Standalone LLMs and Naive RAG, CircuGraphRAG
achieves superior performance in single-hop and multi-hop question answering,
with ROUGE-L F1 scores up to 1.0, while baseline scores below 0.08. It also
improves efficiency, halving the response time and reducing token usage by 16%
in representative tasks. CircuGraphRAG provides fact-checked, regulatory-ready
support for circular economy planning, advancing reliable, low-carbon resource
decision making.

</details>


### [25] [HADA: Human-AI Agent Decision Alignment Architecture](https://arxiv.org/abs/2506.04253)
*Tapio Pitkäranta,Leena Pitkäranta*

Main category: cs.AI

TL;DR: HADA is a reference architecture that aligns AI and legacy systems with organizational goals and values, demonstrated through a retail banking proof of concept showing improved accuracy, transparency, and ethical compliance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of aligning AI and legacy systems with organizational goals and values, ensuring that decisions are transparent, traceable, and ethically compliant.

Method: HADA uses role-specific stakeholder agents (business, data-science, audit, ethics, and customer) that expose conversational APIs, allowing stakeholders to query, steer, audit, or contest decisions. The architecture continuously propagates, logs, and versions alignment objectives, KPIs, and value constraints.

Result: The paper introduces HADA, a reference architecture that aligns both LLMs and legacy algorithms with organizational targets and values across various roles and horizons. It demonstrates the architecture's effectiveness through a cloud-native proof of concept in a retail banking scenario, showing improved accuracy, transparency, and ethical compliance.

Conclusion: The paper concludes by presenting HADA as an open-source architecture, a mid-range design theory for human-AI alignment in multi-agent systems, and empirical evidence of its effectiveness in improving decision-making processes.

Abstract: We present HADA (Human-AI Agent Decision Alignment), a protocol- and
framework agnostic reference architecture that keeps both large language model
(LLM) agents and legacy algorithms aligned with organizational targets and
values. HADA wraps any algorithm or LLM in role-specific stakeholder agents --
business, data-science, audit, ethics, and customer -- each exposing
conversational APIs so that technical and non-technical actors can query,
steer, audit, or contest every decision across strategic, tactical, and
real-time horizons. Alignment objectives, KPIs, and value constraints are
expressed in natural language and are continuously propagated, logged, and
versioned while thousands of heterogeneous agents run on different
orchestration stacks. A cloud-native proof of concept packages a production
credit-scoring model (getLoanDecision) and deploys it on
Docker/Kubernetes/Python; five scripted retail-bank scenarios show how target
changes, parameter tweaks, explanation requests, and ethics triggers flow end
to end through the architecture. Evaluation followed the Design-Science
Research Methodology. Walkthrough observation and log inspection demonstrated
complete coverage of six predefined objectives: every role could invoke
conversational control, trace KPIs and value constraints, detect and mitigate
ZIP-code bias, and reproduce full decision lineage, independent of the
underlying LLM or agent library. Contributions: (1) an open-source HADA
architecture, (2) a mid-range design theory for human-AI alignment in
multi-agent systems, and (3) empirical evidence that framework-agnostic,
protocol-compliant stakeholder agents improve accuracy, transparency, and
ethical compliance in real-world decision pipelines.

</details>


### [26] [Automated Skill Discovery for Language Agents through Exploration and Iterative Feedback](https://arxiv.org/abs/2506.04287)
*Yongjin Yang,Sinjae Kang,Juyong Lee,Dongjun Lee,Se-Young Yun,Kimin Lee*

Main category: cs.AI

TL;DR: EXIF is a novel framework that uses an exploration agent to generate a feasible skill dataset and iteratively train a target agent, improving performance and capabilities in environments like Webshop and Crafter without human intervention.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges in creating a training dataset for LLM agents, such as the need for significant human effort in manual trajectory collection and the generation of invalid or non-meaningful tasks by LLMs. EXIF aims to improve the feasibility of generated target behaviors while considering the agents' capabilities.

Method: The method proposed is an automatic skill discovery framework called EXIF for LLM-powered agents. It involves an exploration agent (Alice) that interacts with the environment to generate a feasible, environment-grounded skill dataset, which is used to train the target agent (Bob). The process includes an iterative feedback loop where Alice evaluates Bob's performance to identify areas for improvement, guiding the next round of exploration.

Result: Experiments on Webshop and Crafter show that EXIF can effectively discover meaningful skills and iteratively expand the capabilities of the trained agent without human intervention, leading to substantial performance improvements. Notably, setting Alice to the same model as Bob also improves performance, suggesting potential for a self-evolving system.

Conclusion: The EXIF framework demonstrates the ability to automatically discover and iteratively improve skills for LLM-powered agents, enhancing their capabilities in a closed-loop data generation process without human intervention. The potential for a self-evolving system is also highlighted.

Abstract: Training large language model (LLM) agents to acquire necessary skills and
perform diverse tasks within an environment is gaining interest as a means to
enable open-endedness. However, creating the training dataset for their skill
acquisition faces several challenges. Manual trajectory collection requires
significant human effort. Another approach, where LLMs directly propose tasks
to learn, is often invalid, as the LLMs lack knowledge of which tasks are
actually feasible. Moreover, the generated data may not provide a meaningful
learning signal, as agents often already perform well on the proposed tasks. To
address this, we propose a novel automatic skill discovery framework EXIF for
LLM-powered agents, designed to improve the feasibility of generated target
behaviors while accounting for the agents' capabilities. Our method adopts an
exploration-first strategy by employing an exploration agent (Alice) to train
the target agent (Bob) to learn essential skills in the environment.
Specifically, Alice first interacts with the environment to retrospectively
generate a feasible, environment-grounded skill dataset, which is then used to
train Bob. Crucially, we incorporate an iterative feedback loop, where Alice
evaluates Bob's performance to identify areas for improvement. This feedback
then guides Alice's next round of exploration, forming a closed-loop data
generation process. Experiments on Webshop and Crafter demonstrate EXIF's
ability to effectively discover meaningful skills and iteratively expand the
capabilities of the trained agent without any human intervention, achieving
substantial performance improvements. Interestingly, we observe that setting
Alice to the same model as Bob also notably improves performance, demonstrating
EXIF's potential for building a self-evolving system.

</details>


### [27] [A Statistical Physics of Language Model Reasoning](https://arxiv.org/abs/2506.04374)
*Jack David Carson,Amir Reisizadeh*

Main category: cs.AI

TL;DR: A statistical physics framework is introduced to model the reasoning dynamics of transformers, using a lower-dimensional manifold to represent hidden state trajectories and an SLDS model to capture latent reasoning regimes, enabling the simulation and prediction of reasoning transitions and failures.


<details>
  <summary>Details</summary>
Motivation: The motivation is to provide a mechanistic understanding of the emergent reasoning processes in transformer language models, which currently resist straightforward explanation.

Method: The method involves modeling the hidden state trajectories of transformer models as a stochastic dynamical system on a lower-dimensional manifold, using latent regime switching to capture different reasoning phases, and validating the model with empirical data from multiple models and benchmarks.

Result: The results show that a rank-40 projection explains approximately 50% of the variance in the empirical trajectories, and the model identifies four latent reasoning regimes.

Conclusion: The framework enables efficient simulation and analysis of reasoning dynamics, providing tools to predict critical transitions and failures in transformer models.

Abstract: Transformer LMs show emergent reasoning that resists mechanistic
understanding. We offer a statistical physics framework for continuous-time
chain-of-thought reasoning dynamics. We model sentence-level hidden state
trajectories as a stochastic dynamical system on a lower-dimensional manifold.
This drift-diffusion system uses latent regime switching to capture diverse
reasoning phases, including misaligned states or failures. Empirical
trajectories (8 models, 7 benchmarks) show a rank-40 projection (balancing
variance capture and feasibility) explains ~50% variance. We find four latent
reasoning regimes. An SLDS model is formulated and validated to capture these
features. The framework enables low-cost reasoning simulation, offering tools
to study and predict critical transitions like misaligned states or other LM
failures.

</details>


### [28] [Matter-of-Fact: A Benchmark for Verifying the Feasibility of Literature-Supported Claims in Materials Science](https://arxiv.org/abs/2506.04410)
*Peter Jansen,Samiah Hassan,Ruoyao Wang*

Main category: cs.AI

TL;DR: This paper introduces Matter-of-Fact, a dataset to determine the feasibility of scientific hypotheses, which can help reduce the cost of automated experiments.


<details>
  <summary>Details</summary>
Motivation: To address the high cost of running large-scale automated experiments, the paper aims to develop a method to filter hypotheses based on their feasibility.

Method: The method involves creating a dataset called Matter-of-Fact, which contains 8.4k claims from high-impact materials science topics, and evaluating baseline models for feasibility prediction.

Result: The baseline models, including retrieval-augmented generation and code generation, achieve a maximum performance of 72% on the task, while domain experts suggest that nearly all claims are feasible in reality.

Conclusion: The difficulty of the task for current models highlights the potential for significant advancements in accelerating scientific discovery by improving the ability to predict hypothesis feasibility.

Abstract: Contemporary approaches to assisted scientific discovery use language models
to automatically generate large numbers of potential hypothesis to test, while
also automatically generating code-based experiments to test those hypotheses.
While hypotheses can be comparatively inexpensive to generate, automated
experiments can be costly, particularly when run at scale (i.e. thousands of
experiments). Developing the capacity to filter hypotheses based on their
feasibility would allow discovery systems to run at scale, while increasing
their likelihood of making significant discoveries. In this work we introduce
Matter-of-Fact, a challenge dataset for determining the feasibility of
hypotheses framed as claims. Matter-of-Fact includes 8.4k claims extracted from
scientific articles spanning four high-impact contemporary materials science
topics, including superconductors, semiconductors, batteries, and aerospace
materials, while including qualitative and quantitative claims from
theoretical, experimental, and code/simulation results. We show that strong
baselines that include retrieval augmented generation over scientific
literature and code generation fail to exceed 72% performance on this task
(chance performance is 50%), while domain-expert verification suggests nearly
all are solvable -- highlighting both the difficulty of this task for current
models, and the potential to accelerate scientific discovery by making
near-term progress.

</details>


### [29] [Plugging Schema Graph into Multi-Table QA: A Human-Guided Framework for Reducing LLM Reliance](https://arxiv.org/abs/2506.04427)
*Xixi Wang,Miguel Costa,Jordanka Kovaceva,Shuai Wang,Francisco C. Pereira*

Main category: cs.AI

TL;DR: A new graph-based framework is proposed to improve multi-table QA by leveraging human-curated relational knowledge, enhancing schema linking and query interpretation in complex industrial datasets.


<details>
  <summary>Details</summary>
Motivation: Existing LLMs struggle with multi-table QA, especially in complex, real-world scenarios due to unreliable schema linking. The proposed method aims to enhance schema linking and reasoning for more accurate and interpretable answers.

Method: The method uses a graph-based framework with human-curated relational knowledge to encode schema links and join paths. It constructs reasoning chains by searching the graph and includes pruning and sub-path merging strategies to improve efficiency and coherence.

Result: Experiments on standard benchmarks and a large-scale, realistic dataset show that the proposed approach is effective in handling complex industrial tabular data.

Conclusion: This is the first multi-table QA system applied to complex industrial tabular data, demonstrating significant improvements over existing methods.

Abstract: Large language models (LLMs) have shown promise in table Question Answering
(Table QA). However, extending these capabilities to multi-table QA remains
challenging due to unreliable schema linking across complex tables. Existing
methods based on semantic similarity work well only on simplified hand-crafted
datasets and struggle to handle complex, real-world scenarios with numerous and
diverse columns. To address this, we propose a graph-based framework that
leverages human-curated relational knowledge to explicitly encode schema links
and join paths. Given a natural language query, our method searches this graph
to construct interpretable reasoning chains, aided by pruning and sub-path
merging strategies to enhance efficiency and coherence. Experiments on both
standard benchmarks and a realistic, large-scale dataset demonstrate the
effectiveness of our approach. To our knowledge, this is the first multi-table
QA system applied to truly complex industrial tabular data.

</details>


### [30] [An AI-Based Public Health Data Monitoring System](https://arxiv.org/abs/2506.04429)
*Ananya Joshi,Nolan Gormley,Richa Gadgil,Tina Townes,Roni Rosenfeld,Bryan Wilder*

Main category: cs.AI

TL;DR: A scalable ranking-based monitoring system using AI anomaly detection methods has been deployed to monitor up to 5,000,000 data points daily, resulting in a 54x increase in reviewer speed efficiency compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of traditional alert-based monitoring systems in handling large volumes of public health data, which include the need for constant threshold resetting and potential application lag.

Method: The development of a ranking-based monitoring system that leverages AI anomaly detection methods, deployed through a multi-year interdisciplinary collaboration.

Result: The deployed system significantly improved monitoring objectives, with a 54x increase in reviewer speed efficiency compared to traditional alert-based methods.

Conclusion: The study highlights the potential of human-centered AI to enhance public health decision-making, demonstrating significant improvements in monitoring large volumes of health data.

Abstract: Public health experts need scalable approaches to monitor large volumes of
health data (e.g., cases, hospitalizations, deaths) for outbreaks or data
quality issues. Traditional alert-based monitoring systems struggle with modern
public health data monitoring systems for several reasons, including that
alerting thresholds need to be constantly reset and the data volumes may cause
application lag. Instead, we propose a ranking-based monitoring paradigm that
leverages new AI anomaly detection methods. Through a multi-year
interdisciplinary collaboration, the resulting system has been deployed at a
national organization to monitor up to 5,000,000 data points daily. A
three-month longitudinal deployed evaluation revealed a significant improvement
in monitoring objectives, with a 54x increase in reviewer speed efficiency
compared to traditional alert-based methods. This work highlights the potential
of human-centered AI to transform public health decision-making.

</details>


### [31] [Matching Markets Meet LLMs: Algorithmic Reasoning with Ranked Preferences](https://arxiv.org/abs/2506.04478)
*Hadi Hosseini,Samarth Khanna,Ronak Singh*

Main category: cs.AI

TL;DR: LLMs struggle with handling ranked preferences and complex algorithms in combinatorial domains, particularly in large markets. Fine-tuning with LoRA helps in small markets but not in large ones.


<details>
  <summary>Details</summary>
Motivation: To explore the limitations of LLMs in handling ranked preferences and structured algorithms in combinatorial domains like matching markets, which are crucial for applications such as resource allocation and ride-sharing.

Method: Evaluate state-of-the-art LLMs on a hierarchy of preference-based reasoning tasks, including stable-matching generation, instability detection, instability resolution, and preference queries, to systematically identify their limitations.

Result: Top-performing LLMs struggle with resolving instability in large markets, often failing to identify blocking pairs or execute algorithms iteratively. Fine-tuning with LoRA improves performance in small markets but not in large ones.

Conclusion: More sophisticated strategies are needed to improve LLMs' reasoning capabilities with larger-context inputs in combinatorial domains.

Abstract: The rise of Large Language Models (LLMs) has driven progress in reasoning
tasks -- from program synthesis to scientific hypothesis generation -- yet
their ability to handle ranked preferences and structured algorithms in
combinatorial domains remains underexplored. We study matching markets, a core
framework behind applications like resource allocation and ride-sharing, which
require reconciling individual ranked preferences to ensure stable outcomes. We
evaluate several state-of-the-art models on a hierarchy of preference-based
reasoning tasks -- ranging from stable-matching generation to instability
detection, instability resolution, and fine-grained preference queries -- to
systematically expose their logical and algorithmic limitations in handling
ranked inputs. Surprisingly, even top-performing models with advanced reasoning
struggle to resolve instability in large markets, often failing to identify
blocking pairs or execute algorithms iteratively. We further show that
parameter-efficient fine-tuning (LoRA) significantly improves performance in
small markets, but fails to bring about a similar improvement on large
instances, suggesting the need for more sophisticated strategies to improve
LLMs' reasoning with larger-context inputs.

</details>


### [32] [CogMath: Assessing LLMs' Authentic Mathematical Ability from a Human Cognitive Perspective](https://arxiv.org/abs/2506.04481)
*Jiayu Liu,Zhenya Huang,Wei Dai,Cheng Cheng,Jinze Wu,Jing Sha,Song Li,Qi Liu,Shijin Wang,Enhong Chen*

Main category: cs.AI

TL;DR: The paper proposes CogMath, a framework to comprehensively evaluate LLMs' mathematical abilities, revealing that their capabilities are often overestimated and providing insights into their strengths and weaknesses.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of existing evaluation methods for LLMs in mathematical tasks, which only measure overall accuracy and are insufficient for assessing true capabilities.

Method: The paper introduces CogMath, a framework that evaluates LLMs' mathematical abilities through 3 stages: problem comprehension, problem solving, and solution summarization, using 9 fine-grained evaluation dimensions. An 'Inquiry-Judge-Reference' multi-agent system is employed to assess LLMs' mastery in each dimension.

Result: Applying CogMath to three benchmarks, it was found that the mathematical capabilities of 7 mainstream LLMs are overestimated by 30%-40%. The framework also identifies specific strengths and weaknesses of the LLMs in various dimensions.

Conclusion: CogMath provides a more comprehensive and accurate evaluation of LLMs' mathematical abilities, revealing overestimations and offering insights to improve their reasoning skills.

Abstract: Although large language models (LLMs) show promise in solving complex
mathematical tasks, existing evaluation paradigms rely solely on a coarse
measure of overall answer accuracy, which are insufficient for assessing their
authentic capabilities. In this paper, we propose \textbf{CogMath}, which
comprehensively assesses LLMs' mathematical abilities through the lens of human
cognition. Specifically, inspired by psychological theories, CogMath formalizes
human reasoning process into 3 stages: \emph{problem comprehension},
\emph{problem solving}, and \emph{solution summarization}. Within these stages,
we investigate perspectives such as numerical calculation, knowledge, and
counterfactuals, and design a total of 9 fine-grained evaluation dimensions. In
each dimension, we develop an ``\emph{Inquiry}-\emph{Judge}-\emph{Reference}''
multi-agent system to generate inquiries that assess LLMs' mastery from this
dimension. An LLM is considered to truly master a problem only when excelling
in all inquiries from the 9 dimensions. By applying CogMath on three
benchmarks, we reveal that the mathematical capabilities of 7 mainstream LLMs
are overestimated by 30\%-40\%. Moreover, we locate their strengths and
weaknesses across specific stages/dimensions, offering in-depth insights to
further enhance their reasoning abilities.

</details>


### [33] ["Don't Do That!": Guiding Embodied Systems through Large Language Model-based Constraint Generation](https://arxiv.org/abs/2506.04500)
*Aladin Djuhera,Amin Seffo,Masataro Asai,Holger Boche*

Main category: cs.AI

TL;DR: A framework called STPR uses large language models to translate natural language constraints into executable Python functions for robotic navigation, ensuring full compliance with constraints and short runtimes.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of translating complex, informally expressed natural language constraints into a formal description that can be used in robotic navigation planning algorithms.

Method: STPR leverages large language models to generate Python functions from natural language instructions, converting constraints into structured and transparent code to avoid complex reasoning and potential hallucinations.

Result: Experiments in a simulated Gazebo environment demonstrate that STPR ensures full compliance with various constraints and scenarios while maintaining short runtimes. It also works with smaller, code-specific language models, making it applicable to a wide range of compact models at low inference cost.

Conclusion: STPR effectively translates complex natural language constraints into executable functions for robotic navigation, ensuring compliance and efficiency, and is applicable to a variety of language models.

Abstract: Recent advancements in large language models (LLMs) have spurred interest in
robotic navigation that incorporates complex spatial, mathematical, and
conditional constraints from natural language into the planning problem. Such
constraints can be informal yet highly complex, making it challenging to
translate into a formal description that can be passed on to a planning
algorithm. In this paper, we propose STPR, a constraint generation framework
that uses LLMs to translate constraints (expressed as instructions on ``what
not to do'') into executable Python functions. STPR leverages the LLM's strong
coding capabilities to shift the problem description from language into
structured and transparent code, thus circumventing complex reasoning and
avoiding potential hallucinations. We show that these LLM-generated functions
accurately describe even complex mathematical constraints, and apply them to
point cloud representations with traditional search algorithms. Experiments in
a simulated Gazebo environment show that STPR ensures full compliance across
several constraints and scenarios, while having short runtimes. We also verify
that STPR can be used with smaller, code-specific LLMs, making it applicable to
a wide range of compact models at low inference cost.

</details>


### [34] [Schema Generation for Large Knowledge Graphs Using Large Language Models](https://arxiv.org/abs/2506.04512)
*Bohui Zhang,Yuan He,Lydia Pintscher,Albert Meroño Peñuela,Elena Simperl*

Main category: cs.AI

TL;DR: This paper explores the use of large language models (LLMs) for automatic schema generation in the Semantic Web and natural language processing, introducing datasets and metrics to evaluate their performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to reduce the substantial involvement of knowledge engineers and domain experts in schema creation by leveraging the capabilities of LLMs in ontology engineering.

Method: The method involves using LLMs to generate validating schemas in Shape Expressions (ShEx) from local and global information in knowledge graphs (KGs), and introducing two datasets (YAGO Schema and Wikidata EntitySchema) along with evaluation metrics.

Result: Experiments show that LLMs have strong potential in producing high-quality ShEx schemas, demonstrating their effectiveness in scalable, automated schema generation for large KGs.

Conclusion: The paper concludes that LLMs can significantly contribute to automated schema generation, and the introduced benchmark sets a new challenge for structured generation tasks involving syntactically rich formalisms.

Abstract: Schemas are vital for ensuring data quality in the Semantic Web and natural
language processing. Traditionally, their creation demands substantial
involvement from knowledge engineers and domain experts. Leveraging the
impressive capabilities of large language models (LLMs) in related tasks like
ontology engineering, we explore automatic schema generation using LLMs. To
bridge the resource gap, we introduce two datasets: YAGO Schema and Wikidata
EntitySchema, along with evaluation metrics. The LLM-based pipelines
effectively utilize local and global information from knowledge graphs (KGs) to
generate validating schemas in Shape Expressions (ShEx). Experiments
demonstrate LLMs' strong potential in producing high-quality ShEx schemas,
paving the way for scalable, automated schema generation for large KGs.
Furthermore, our benchmark introduces a new challenge for structured
generation, pushing the limits of LLMs on syntactically rich formalisms.

</details>


### [35] [OpenAg: Democratizing Agricultural Intelligence](https://arxiv.org/abs/2506.04571)
*Srikanth Thudumu,Jason Fisher*

Main category: cs.AI

TL;DR: OpenAg is a framework that integrates domain-specific models, knowledge graphs, multi-agent systems, and causal explainability to provide context-aware and actionable agricultural insights, targeting the needs of smallholder farmers.


<details>
  <summary>Details</summary>
Motivation: Current agricultural intelligence systems lack contextual understanding, explainability, and adaptability, especially for smallholder farmers with limited resources. General-purpose LLMs often provide generic or unrealistic recommendations.

Method: OpenAg combines: (i) a unified agricultural knowledge base; (ii) a neural agricultural knowledge graph; (iii) an adaptive multi-agent reasoning system; and (iv) a causal transparency mechanism to ensure interpretable and relevant recommendations.

Result: The framework is designed to deliver context-aware, explainable, and actionable insights, bridging the gap between scientific knowledge and the practical expertise of farmers.

Conclusion: OpenAg aims to support scalable and locally relevant agricultural decision-making, particularly for smallholder farmers, by leveraging advanced AI and knowledge representation technologies.

Abstract: Agriculture is undergoing a major transformation driven by artificial
intelligence (AI), machine learning, and knowledge representation technologies.
However, current agricultural intelligence systems often lack contextual
understanding, explainability, and adaptability, especially for smallholder
farmers with limited resources. General-purpose large language models (LLMs),
while powerful, typically lack the domain-specific knowledge and contextual
reasoning needed for practical decision support in farming. They tend to
produce recommendations that are too generic or unrealistic for real-world
applications. To address these challenges, we present OpenAg, a comprehensive
framework designed to advance agricultural artificial general intelligence
(AGI). OpenAg combines domain-specific foundation models, neural knowledge
graphs, multi-agent reasoning, causal explainability, and adaptive transfer
learning to deliver context-aware, explainable, and actionable insights. The
system includes: (i) a unified agricultural knowledge base that integrates
scientific literature, sensor data, and farmer-generated knowledge; (ii) a
neural agricultural knowledge graph for structured reasoning and inference;
(iii) an adaptive multi-agent reasoning system where AI agents specialize and
collaborate across agricultural domains; and (iv) a causal transparency
mechanism that ensures AI recommendations are interpretable, scientifically
grounded, and aligned with real-world constraints. OpenAg aims to bridge the
gap between scientific knowledge and the tacit expertise of experienced farmers
to support scalable and locally relevant agricultural decision-making.

</details>


### [36] [Judicial Permission](https://arxiv.org/abs/2506.04610)
*Guido Governatori,Antonino Rotolo*

Main category: cs.AI

TL;DR: The paper explores the importance of weak permissions in criminal trials and proposes a dialogue game model to address them, considering various proof standards and argumentation semantics.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the understanding and handling of judicial permissions in criminal trials, which can influence the outcome of cases.

Method: The method involves the development of a dialogue game model that incorporates different standards of proof and argumentation semantics to systematically address judicial permissions.

Result: The paper likely presents insights into how the dialogue game model can better clarify and manage weak permissions in criminal trials, potentially leading to more fair and just outcomes.

Conclusion: The conclusion likely emphasizes the potential benefits of using the dialogue game model in judicial settings and suggests further research or practical applications.

Abstract: This paper examines the significance of weak permissions in criminal trials
(\emph{judicial permission}). It introduces a dialogue game model to
systematically address judicial permissions, considering different standards of
proof and argumentation semantics.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [37] [GEM: Empowering LLM for both Embedding Generation and Language Understanding](https://arxiv.org/abs/2506.04344)
*Caojin Zhang,Qiang Zhang,Ke Li,Sai Vidyaranya Nuthalapati,Benyu Zhang,Jason Liu,Serena Li,Lizhu Zhang,Xiangjun Fan*

Main category: cs.CL

TL;DR: The paper proposes a self-supervised method called Generative Embedding large language Model (GEM) that enables large decoder-only LLMs to generate high-quality text embeddings while retaining their original capabilities.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitation of using separate embedding models in applications like RAG, which can complicate the system and introduce discrepancies between the embedding model and LLMs.

Method: The method involves inserting special tokens into the text and manipulating the attention mask to generate summarization embeddings. This can be integrated into post-training or fine-tuning stages of existing LLMs.

Result: The approach significantly improves the performance of LLMs on text embedding benchmarks (MTEB) while having a minimal impact on NLP benchmarks (MMLU).

Conclusion: GEM empowers LLMs with state-of-the-art text embedding capabilities while maintaining their original performance.

Abstract: Large decoder-only language models (LLMs) have achieved remarkable success in
generation and reasoning tasks, where they generate text responses given
instructions. However, many applications, e.g., retrieval augmented generation
(RAG), still rely on separate embedding models to generate text embeddings,
which can complicate the system and introduce discrepancies in understanding of
the query between the embedding model and LLMs. To address this limitation, we
propose a simple self-supervised approach, Generative Embedding large language
Model (GEM), that enables any large decoder-only LLM to generate high-quality
text embeddings while maintaining its original text generation and reasoning
capabilities. Our method inserts new special token(s) into a text body, and
generates summarization embedding of the text by manipulating the attention
mask. This method could be easily integrated into post-training or fine tuning
stages of any existing LLMs. We demonstrate the effectiveness of our approach
by applying it to two popular LLM families, ranging from 1B to 8B parameters,
and evaluating the transformed models on both text embedding benchmarks (MTEB)
and NLP benchmarks (MMLU). The results show that our proposed method
significantly improves the original LLMs on MTEB while having a minimal impact
on MMLU. Our strong results indicate that our approach can empower LLMs with
state-of-the-art text embedding capabilities while maintaining their original
NLP performance

</details>


### [38] [Effects of Speaker Count, Duration, and Accent Diversity on Zero-Shot Accent Robustness in Low-Resource ASR](https://arxiv.org/abs/2506.04364)
*Zheng-Xin Yong,Vineel Pratap,Michael Auli,Jean Maillard*

Main category: cs.CL

TL;DR: This paper studies the impact of training data variables (number of speakers, audio duration per speaker, and accent diversity) on ASR robustness to unseen accents, suggesting increasing the number of speakers as the most beneficial approach.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve ASR robustness to a wide range of accents, including unseen ones, in low-resource training scenarios.

Method: The study systematically varies the number of speakers, the audio duration per speaker, and the diversity of accents in the training data to evaluate their impact on ASR performance with unseen accents.

Result: Increasing the number of speakers is more beneficial than increasing the audio duration per speaker or prioritizing accent diversity. More speakers also enable greater performance gains from increasing training hours.

Conclusion: Practitioners should prioritize increasing the number of speakers in ASR training data for new languages to enhance robustness to unseen accents.

Abstract: To build an automatic speech recognition (ASR) system that can serve everyone
in the world, the ASR needs to be robust to a wide range of accents including
unseen accents. We systematically study how three different variables in
training data -- the number of speakers, the audio duration per each individual
speaker, and the diversity of accents -- affect ASR robustness towards unseen
accents in a low-resource training regime. We observe that for a fixed number
of ASR training hours, it is more beneficial to increase the number of speakers
(which means each speaker contributes less) than the number of hours
contributed per speaker. We also observe that more speakers enables ASR
performance gains from scaling number of hours. Surprisingly, we observe
minimal benefits to prioritizing speakers with different accents when the
number of speakers is controlled. Our work suggests that practitioners should
prioritize increasing the speaker count in ASR training data composition for
new languages.

</details>


### [39] [Mechanistic Decomposition of Sentence Representations](https://arxiv.org/abs/2506.04373)
*Matthieu Tehenan,Vikram Natarajan,Jonathan Michala,Milton Lin,Juri Opitz*

Main category: cs.CL

TL;DR: The paper proposes a method to make sentence embeddings more interpretable by decomposing them into token-level components using dictionary learning.


<details>
  <summary>Details</summary>
Motivation: To address the lack of interpretability in sentence embeddings, which are central to NLP and AI systems.

Method: Uses dictionary learning on token-level representations to decompose sentence embeddings and analyze the latent features that contribute to the final sentence representation.

Result: The method reveals that many semantic and syntactic aspects are linearly encoded in sentence embeddings, providing insights into their internal structure.

Conclusion: The proposed method bridges token-level interpretability with sentence-level analysis, leading to more transparent and controllable sentence representations.

Abstract: Sentence embeddings are central to modern NLP and AI systems, yet little is
known about their internal structure. While we can compare these embeddings
using measures such as cosine similarity, the contributing features are not
human-interpretable, and the content of an embedding seems untraceable, as it
is masked by complex neural transformations and a final pooling operation that
combines individual token embeddings. To alleviate this issue, we propose a new
method to mechanistically decompose sentence embeddings into interpretable
components, by using dictionary learning on token-level representations. We
analyze how pooling compresses these features into sentence representations,
and assess the latent features that reside in a sentence embedding. This
bridges token-level mechanistic interpretability with sentence-level analysis,
making for more transparent and controllable representations. In our studies,
we obtain several interesting insights into the inner workings of sentence
embedding spaces, for instance, that many semantic and syntactic aspects are
linearly encoded in the embeddings.

</details>


### [40] [Hierarchical Text Classification Using Contrastive Learning Informed Path Guided Hierarchy](https://arxiv.org/abs/2506.04381)
*Neeraj Agrawal,Saurabh Kumar,Priyanka Bhatt,Tanishka Agarwal*

Main category: cs.CL

TL;DR: Proposes HTC-CLIP, a method combining hierarchy-aware text representation and text-informed path-guided hierarchy using contrastive learning, achieving improved performance on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing HTC models by combining two complementary approaches to better capture the characteristics of label hierarchy.

Method: Introduces HTC-CLIP, which uses contrastive learning to learn hierarchy-aware text representation and text-informed path-guided hierarchy representation, and combines two sets of class probability distributions during inference.

Result: Achieved a 0.99 - 2.37% improvement in Macro F1 score on two public benchmark datasets compared to existing state-of-the-art models.

Conclusion: The proposed HTC-CLIP effectively combines the strengths of existing approaches, leading to better performance in hierarchical text classification.

Abstract: Hierarchical Text Classification (HTC) has recently gained traction given the
ability to handle complex label hierarchy. This has found applications in
domains like E- commerce, customer care and medicine industry among other
real-world applications. Existing HTC models either encode label hierarchy
separately and mix it with text encoding or guide the label hierarchy structure
in the text encoder. Both approaches capture different characteristics of label
hierarchy and are complementary to each other. In this paper, we propose a
Hierarchical Text Classification using Contrastive Learning Informed Path
guided hierarchy (HTC-CLIP), which learns hierarchy-aware text representation
and text informed path guided hierarchy representation using contrastive
learning. During the training of HTC-CLIP, we learn two different sets of class
probabilities distributions and during inference, we use the pooled output of
both probabilities for each class to get the best of both representations. Our
results show that the two previous approaches can be effectively combined into
one architecture to achieve improved performance. Tests on two public benchmark
datasets showed an improvement of 0.99 - 2.37% in Macro F1 score using HTC-CLIP
over the existing state-of-the-art models.

</details>


### [41] [MELABenchv1: Benchmarking Large Language Models against Smaller Fine-Tuned Models for Low-Resource Maltese NLP](https://arxiv.org/abs/2506.04385)
*Kurt Micallef,Claudia Borg*

Main category: cs.CL

TL;DR: An evaluation of 55 large language models on Maltese, a low-resource language, reveals that smaller fine-tuned models often outperform larger ones, and prior exposure to Maltese is crucial for performance. Fine-tuning, though costly, results in better and more cost-effective models.


<details>
  <summary>Details</summary>
Motivation: The motivation is to assess the effectiveness of large language models on low-resource languages, specifically Maltese, and to understand the factors that influence their performance.

Method: The study evaluates 55 publicly available LLMs on a newly introduced benchmark covering 11 discriminative and generative tasks in Maltese, analyzing the impact of factors like model size, fine-tuning, and prior exposure to the language.

Result: Many models performed poorly, especially on generative tasks, with smaller fine-tuned models often performing better. Prior exposure to Maltese during pre-training and instruction-tuning was the most significant factor for performance.

Conclusion: The study recommends that researchers working with low-resource languages consider more traditional language modeling approaches, such as fine-tuning, to achieve better performance and lower inference costs. The work also emphasizes the need for more inclusive language technologies.

Abstract: Large Language Models (LLMs) have demonstrated remarkable performance across
various Natural Language Processing (NLP) tasks, largely due to their
generalisability and ability to perform tasks without additional training.
However, their effectiveness for low-resource languages remains limited. In
this study, we evaluate the performance of 55 publicly available LLMs on
Maltese, a low-resource language, using a newly introduced benchmark covering
11 discriminative and generative tasks. Our experiments highlight that many
models perform poorly, particularly on generative tasks, and that smaller
fine-tuned models often perform better across all tasks. From our
multidimensional analysis, we investigate various factors impacting
performance. We conclude that prior exposure to Maltese during pre-training and
instruction-tuning emerges as the most important factor. We also examine the
trade-offs between fine-tuning and prompting, highlighting that while
fine-tuning requires a higher initial cost, it yields better performance and
lower inference costs. Through this work, we aim to highlight the need for more
inclusive language technologies and recommend that researchers working with
low-resource languages consider more "traditional" language modelling
approaches.

</details>


### [42] [Building a Few-Shot Cross-Domain Multilingual NLU Model for Customer Care](https://arxiv.org/abs/2506.04389)
*Saurabh Kumar,Sourav Bansal,Neeraj Agrawal,Priyanka Bhatt*

Main category: cs.CL

TL;DR: The paper proposes an embedder-cum-classifier model that leverages a few labeled samples and multilingual knowledge distillation to improve intent classification in e-commerce customer care across different domains, achieving a 20-23% accuracy increase over existing methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of generalizing intent classifiers across different e-commerce domains (e.g., geographies, channels, languages) with limited annotated data, which is a significant bottleneck in practical applications.

Method: The method involves training a domain-specific sentence embedder using supervised fine-tuning with isotropic regularizers, followed by multilingual knowledge distillation to generalize the embedder across multiple domains. The embedder is then paired with a simple linear classifier for deployment in new domains.

Result: The experiments on Canada and Mexico e-commerce Customer Care datasets show that the proposed model outperforms existing state-of-the-art pre-trained models by 20-23% in few-shot intent detection tasks.

Conclusion: The paper concludes that the proposed embedder-cum-classifier model effectively generalizes across different domains with limited labeled data, significantly improving intent classification accuracy in e-commerce customer care.

Abstract: Customer care is an essential pillar of the e-commerce shopping experience
with companies spending millions of dollars each year, employing automation and
human agents, across geographies (like US, Canada, Mexico, Chile), channels
(like Chat, Interactive Voice Response (IVR)), and languages (like English,
Spanish). SOTA pre-trained models like multilingual-BERT, fine-tuned on
annotated data have shown good performance in downstream tasks relevant to
Customer Care. However, model performance is largely subject to the
availability of sufficient annotated domain-specific data. Cross-domain
availability of data remains a bottleneck, thus building an intent classifier
that generalizes across domains (defined by channel, geography, and language)
with only a few annotations, is of great practical value. In this paper, we
propose an embedder-cum-classifier model architecture which extends
state-of-the-art domain-specific models to other domains with only a few
labeled samples. We adopt a supervised fine-tuning approach with isotropic
regularizers to train a domain-specific sentence embedder and a multilingual
knowledge distillation strategy to generalize this embedder across multiple
domains. The trained embedder, further augmented with a simple linear
classifier can be deployed for new domains. Experiments on Canada and Mexico
e-commerce Customer Care dataset with few-shot intent detection show an
increase in accuracy by 20-23% against the existing state-of-the-art
pre-trained models.

</details>


### [43] [MedAgentGym: Training LLM Agents for Code-Based Medical Reasoning at Scale](https://arxiv.org/abs/2506.04405)
*Ran Xu,Yuchen Zhuang,Yishan Zhong,Yue Yu,Xiangru Tang,Hang Wu,May D. Wang,Peifeng Ruan,Donghan Yang,Tao Wang,Guanghua Xiao,Carl Yang,Yang Xie,Wenqi Shi*

Main category: cs.CL

TL;DR: The paper introduces MedAgentGYM, a training environment for enhancing medical reasoning in LLMs, with extensive benchmarks and performance improvements.


<details>
  <summary>Details</summary>
Motivation: To improve the medical reasoning capabilities of LLMs by providing a comprehensive training environment and benchmarking platform.

Method: Developed MedAgentGYM, a dataset of 72,413 task instances across 129 categories, using real-world biomedical scenarios. Tasks are encapsulated in executable coding environments with detailed descriptions, interactive feedback, and verifiable ground-truth annotations.

Result: Med-Copilot-7B, a 7B parameter model, achieved significant performance gains through supervised fine-tuning (+36.44%) and continued reinforcement learning (+42.47%), making it competitive with gpt-4.

Conclusion: MedAgentGYM is a valuable resource for developing and benchmarking LLM-based coding assistants in biomedical research and practice, offering expandable and accessible training environments.

Abstract: We introduce MedAgentGYM, the first publicly available training environment
designed to enhance coding-based medical reasoning capabilities in large
language model (LLM) agents. MedAgentGYM comprises 72,413 task instances across
129 categories derived from authentic real-world biomedical scenarios. Tasks
are encapsulated within executable coding environments, each featuring detailed
task descriptions, interactive feedback mechanisms, verifiable ground-truth
annotations, and scalable training trajectory generation. Extensive
benchmarking of over 30 LLMs reveals a notable performance disparity between
commercial API-based models and open-source counterparts. Leveraging
MedAgentGYM, Med-Copilot-7B achieves substantial performance gains through
supervised fine-tuning (+36.44%) and continued reinforcement learning
(+42.47%), emerging as an affordable and privacy-preserving alternative
competitive with gpt-4o. By offering both a comprehensive benchmark and
accessible, expandable training resources within unified execution
environments, MedAgentGYM delivers an integrated platform to develop LLM-based
coding assistants for advanced biomedical research and practice.

</details>


### [44] [Unpacking Let Alone: Human-Scale Models Generalize to a Rare Construction in Form but not Meaning](https://arxiv.org/abs/2506.04408)
*Wesley Scivetti,Tatsuya Aoyama,Ethan Wilcox,Nathan Schneider*

Main category: cs.CL

TL;DR: This study tests human-scale LMs on their knowledge of the English LET-ALONE construction, finding that while LMs are sensitive to form, they fail to generalize its meaning correctly.


<details>
  <summary>Details</summary>
Motivation: To understand the extent to which human-scale pre-trained language models can generalize from frequent to rare grammatical constructions, particularly in terms of both form and meaning.

Method: The researchers created a synthetic benchmark to test human-scale transformer LMs on their knowledge of the form and meaning of the rare English LET-ALONE construction.

Result: The LMs demonstrated sensitivity to the form of the construction even when related constructions were filtered out, but they did not correctly generalize its meaning.

Conclusion: The results highlight an asymmetry in the sample efficiency of current language model architectures between language form and meaning, which is not observed in human language learners.

Abstract: Humans have a remarkable ability to acquire and understand grammatical
phenomena that are seen rarely, if ever, during childhood. Recent evidence
suggests that language models with human-scale pretraining data may possess a
similar ability by generalizing from frequent to rare constructions. However,
it remains an open question how widespread this generalization ability is, and
to what extent this knowledge extends to meanings of rare constructions, as
opposed to just their forms. We fill this gap by testing human-scale
transformer language models on their knowledge of both the form and meaning of
the (rare and quirky) English LET-ALONE construction. To evaluate our LMs we
construct a bespoke synthetic benchmark that targets syntactic and semantic
properties of the construction. We find that human-scale LMs are sensitive to
form, even when related constructions are filtered from the dataset. However,
human-scale LMs do not make correct generalizations about LET-ALONE's meaning.
These results point to an asymmetry in the current architectures' sample
efficiency between language form and meaning, something which is not present in
human language learners.

</details>


### [45] [Counterfactual reasoning: an analysis of in-context emergence](https://arxiv.org/abs/2506.05188)
*Moritz Miller,Bernhard Schölkopf,Siyuan Guo*

Main category: cs.CL

TL;DR: This paper explores in-context counterfactual reasoning in large-scale neural language models, demonstrating their ability to predict consequences of changes in hypothetical scenarios, particularly in a linear regression task involving noise abduction. The study also provides insights into the factors driving performance in Transformers and suggests potential applications in counterfactual story generation.


<details>
  <summary>Details</summary>
Motivation: To understand the capabilities of large-scale neural language models in counterfactual reasoning, which involves predicting outcomes under hypothetical scenarios, and to explore the underlying mechanisms and potential applications.

Method: The authors study counterfactual reasoning in a controlled synthetic setup using a linear regression task that requires noise abduction. They analyze the performance of language models in this task and investigate the impact of self-attention, model depth, and data diversity in pre-training.

Result: Language models are capable of counterfactual reasoning in the controlled setup, with performance influenced by self-attention, model depth, and data diversity. The findings extend to sequential data, suggesting potential for counterfactual story generation.

Conclusion: The study provides evidence that large-scale language models can perform in-context counterfactual reasoning, which could have broader implications for tasks requiring hypothetical reasoning and story generation.

Abstract: Large-scale neural language models (LMs) exhibit remarkable performance in
in-context learning: the ability to learn and reason the input context on the
fly without parameter update. This work studies in-context counterfactual
reasoning in language models, that is, to predict the consequences of changes
under hypothetical scenarios. We focus on studying a well-defined synthetic
setup: a linear regression task that requires noise abduction, where accurate
prediction is based on inferring and copying the contextual noise from factual
observations. We show that language models are capable of counterfactual
reasoning in this controlled setup and provide insights that counterfactual
reasoning for a broad class of functions can be reduced to a transformation on
in-context observations; we find self-attention, model depth, and data
diversity in pre-training drive performance in Transformers. More
interestingly, our findings extend beyond regression tasks and show that
Transformers can perform noise abduction on sequential data, providing
preliminary evidence on the potential for counterfactual story generation. Our
code is available under
https://github.com/moXmiller/counterfactual-reasoning.git .

</details>


### [46] [Empaths at SemEval-2025 Task 11: Retrieval-Augmented Approach to Perceived Emotions Prediction](https://arxiv.org/abs/2506.04409)
*Lev Morozov,Aleksandr Mogilevskii,Alexander Shirnin*

Main category: cs.CL

TL;DR: EmoRAG is a system for multi-label emotion detection in text that achieves comparable results to the best systems without additional model training, using an ensemble of models.


<details>
  <summary>Details</summary>
Motivation: To detect perceived emotions in text for the SemEval-2025 Task 11, Subtask A, focusing on predicting the emotions of the speaker from a text snippet.

Method: Uses an ensemble of models to predict emotions without additional model training.

Result: Achieves results comparable to the best performing systems, while being more efficient, scalable, and easier to implement.

Conclusion: EmoRAG is a competitive and practical solution for multi-label emotion detection in text.

Abstract: This paper describes EmoRAG, a system designed to detect perceived emotions
in text for SemEval-2025 Task 11, Subtask A: Multi-label Emotion Detection. We
focus on predicting the perceived emotions of the speaker from a given text
snippet, labeling it with emotions such as joy, sadness, fear, anger, surprise,
and disgust. Our approach does not require additional model training and only
uses an ensemble of models to predict emotions. EmoRAG achieves results
comparable to the best performing systems, while being more efficient,
scalable, and easier to implement.

</details>


### [47] [Zero-Shot Open-Schema Entity Structure Discovery](https://arxiv.org/abs/2506.04458)
*Xueqiang Xu,Jinfeng Xiao,James Barry,Mohab Elkaref,Jiaru Zou,Pengcheng Jiang,Yunyi Zhang,Max Giammona,Geeth de Mel,Jiawei Han*

Main category: cs.CL

TL;DR: Proposes ZOES, a zero-shot open-schema entity structure extraction method that improves LLMs' ability to extract complete entity structures without predefined schemas or annotated datasets.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing LLM-based methods in entity structure extraction, which often rely on predefined schemas or annotated datasets and can lead to incomplete results.

Method: ZOES operates through three main steps: enrichment (expanding the initial entity structure), refinement (improving the accuracy of the extracted structure), and unification (integrating multiple structures into a coherent one).

Result: Experiments show that ZOES enhances LLMs' ability to extract more complete entity structures across three different domains, demonstrating its effectiveness and generalizability.

Conclusion: The enrichment, refinement, and unification mechanism in ZOES can be a principled approach to improving the quality of LLM-based entity structure discovery in various scenarios.

Abstract: Entity structure extraction, which aims to extract entities and their
associated attribute-value structures from text, is an essential task for text
understanding and knowledge graph construction. Existing methods based on large
language models (LLMs) typically rely heavily on predefined entity attribute
schemas or annotated datasets, often leading to incomplete extraction results.
To address these challenges, we introduce Zero-Shot Open-schema Entity
Structure Discovery (ZOES), a novel approach to entity structure extraction
that does not require any schema or annotated samples. ZOES operates via a
principled mechanism of enrichment, refinement, and unification, based on the
insight that an entity and its associated structure are mutually reinforcing.
Experiments demonstrate that ZOES consistently enhances LLMs' ability to
extract more complete entity structures across three different domains,
showcasing both the effectiveness and generalizability of the method. These
findings suggest that such an enrichment, refinement, and unification mechanism
may serve as a principled approach to improving the quality of LLM-based entity
structure discovery in various scenarios.

</details>


### [48] [Watermarking Degrades Alignment in Language Models: Analysis and Mitigation](https://arxiv.org/abs/2506.04462)
*Apurv Verma,NhatHai Phan,Shubhendu Trivedi*

Main category: cs.CL

TL;DR: This paper analyzes the impact of two watermarking techniques (Gumbel and KGW) on the alignment properties of large language models (LLMs), revealing two degradation patterns: guard attenuation and guard amplification. It introduces Alignment Resampling (AR), an inference-time method that restores alignment by using an external reward model, demonstrating that sampling 2-4 watermarked generations can recover or surpass baseline alignment scores while maintaining watermark detectability.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand how watermarking techniques affect the truthfulness, safety, and helpfulness of aligned LLMs and to propose a method to mitigate any negative effects, ensuring the responsible deployment of watermarked LLMs.

Method: The study conducts experiments on four aligned LLMs to analyze the impact of Gumbel and KGW watermarking techniques. It identifies two degradation patterns and proposes Alignment Resampling (AR), an inference-time method that uses an external reward model to restore alignment. The paper also establishes a theoretical lower bound on the improvement in expected reward score with increased sample size and tests the effectiveness of AR with modified Gumbel watermarking.

Result: The experiments reveal two degradation patterns (guard attenuation and guard amplification) and show that AR effectively recovers or surpasses baseline alignment scores with just 2-4 watermarked generations. The modified Gumbel implementation maintains strong watermark detectability while improving response diversity.

Conclusion: The paper concludes that there is a critical balance between watermark strength and model alignment. AR provides a simple inference-time solution to responsibly deploy watermarked LLMs, ensuring that alignment properties are maintained without compromising watermark detectability.

Abstract: Watermarking techniques for large language models (LLMs) can significantly
impact output quality, yet their effects on truthfulness, safety, and
helpfulness remain critically underexamined. This paper presents a systematic
analysis of how two popular watermarking approaches-Gumbel and KGW-affect these
core alignment properties across four aligned LLMs. Our experiments reveal two
distinct degradation patterns: guard attenuation, where enhanced helpfulness
undermines model safety, and guard amplification, where excessive caution
reduces model helpfulness. These patterns emerge from watermark-induced shifts
in token distribution, surfacing the fundamental tension that exists between
alignment objectives.
  To mitigate these degradations, we propose Alignment Resampling (AR), an
inference-time sampling method that uses an external reward model to restore
alignment. We establish a theoretical lower bound on the improvement in
expected reward score as the sample size is increased and empirically
demonstrate that sampling just 2-4 watermarked generations effectively recovers
or surpasses baseline (unwatermarked) alignment scores. To overcome the limited
response diversity of standard Gumbel watermarking, our modified implementation
sacrifices strict distortion-freeness while maintaining robust detectability,
ensuring compatibility with AR. Experimental results confirm that AR
successfully recovers baseline alignment in both watermarking approaches, while
maintaining strong watermark detectability. This work reveals the critical
balance between watermark strength and model alignment, providing a simple
inference-time solution to responsibly deploy watermarked LLMs in practice.

</details>


### [49] [Aligning Large Language Models with Implicit Preferences from User-Generated Content](https://arxiv.org/abs/2506.04463)
*Zhaoxuan Tan,Zheng Li,Tianyi Liu,Haodong Wang,Hyokun Yun,Ming Zeng,Pei Chen,Zhihan Zhang,Yifan Gao,Ruijie Wang,Priyanka Nigam,Bing Yin,Meng Jiang*

Main category: cs.CL

TL;DR: The paper introduces PUGC, a framework that uses user-generated content (UGC) to generate preference data for aligning large language models (LLMs) with human values, improving the quality of generated responses and achieving better performance over traditional methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the high cost and scalability issues associated with existing preference learning methods for LLMs, which rely on curated data from humans or advanced LLMs.

Method: PUGC leverages implicit human preferences in unlabeled user-generated content (UGC) by transforming UGC into user queries, generating responses from the policy model, and using the UGC as a reference text for response scoring.

Result: Experimental results on Alpaca Eval 2 show a 9.37% performance improvement over traditional methods, achieving a 35.93% state-of-the-art length-controlled win rate using Mistral-7B-Instruct. Additional studies demonstrate gains in reward quality, domain-specific alignment, robustness to UGC quality, and theory of mind capabilities.

Conclusion: The PUGC framework improves the quality and scalability of preference data, enabling better alignment of LLMs with human preferences and enhancing various aspects of model performance.

Abstract: Learning from preference feedback is essential for aligning large language
models (LLMs) with human values and improving the quality of generated
responses. However, existing preference learning methods rely heavily on
curated data from humans or advanced LLMs, which is costly and difficult to
scale. In this work, we present PUGC, a novel framework that leverages implicit
human Preferences in unlabeled User-Generated Content (UGC) to generate
preference data. Although UGC is not explicitly created to guide LLMs in
generating human-preferred responses, it often reflects valuable insights and
implicit preferences from its creators that has the potential to address
readers' questions. PUGC transforms UGC into user queries and generates
responses from the policy model. The UGC is then leveraged as a reference text
for response scoring, aligning the model with these implicit preferences. This
approach improves the quality of preference data while enabling scalable,
domain-specific alignment. Experimental results on Alpaca Eval 2 show that
models trained with DPO and PUGC achieve a 9.37% performance improvement over
traditional methods, setting a 35.93% state-of-the-art length-controlled win
rate using Mistral-7B-Instruct. Further studies highlight gains in reward
quality, domain-specific alignment effectiveness, robustness against UGC
quality, and theory of mind capabilities. Our code and dataset are available at
https://zhaoxuan.info/PUGC.github.io/

</details>


### [50] [SQLens: An End-to-End Framework for Error Detection and Correction in Text-to-SQL](https://arxiv.org/abs/2506.04494)
*Yue Gong,Chuan Lei,Xiao Qin,Kapil Vaidya,Balakrishnan Narayanaswamy,Tim Kraska*

Main category: cs.CL

TL;DR: SQLens is a framework that enhances the reliability of LLM-generated SQL queries by detecting and correcting semantic errors, improving error detection by 25.78% and execution accuracy by up to 20%.


<details>
  <summary>Details</summary>
Motivation: LLMs have shown promise in text-to-SQL tasks but often generate semantically incorrect queries, leading to a need for reliable error detection and correction.

Method: SQLens integrates error signals from the database and the LLM to detect and correct semantic errors in SQL clauses, guiding query corrections based on these signals.

Result: SQLens outperforms the best LLM-based self-evaluation method by 25.78% in F1 for error detection and improves execution accuracy by up to 20% on two public benchmarks.

Conclusion: SQLens provides a robust solution for enhancing the reliability of LLM-generated SQL queries, significantly improving both error detection and execution accuracy.

Abstract: Text-to-SQL systems translate natural language (NL) questions into SQL
queries, enabling non-technical users to interact with structured data. While
large language models (LLMs) have shown promising results on the text-to-SQL
task, they often produce semantically incorrect yet syntactically valid
queries, with limited insight into their reliability. We propose SQLens, an
end-to-end framework for fine-grained detection and correction of semantic
errors in LLM-generated SQL. SQLens integrates error signals from both the
underlying database and the LLM to identify potential semantic errors within
SQL clauses. It further leverages these signals to guide query correction.
Empirical results on two public benchmarks show that SQLens outperforms the
best LLM-based self-evaluation method by 25.78% in F1 for error detection, and
improves execution accuracy of out-of-the-box text-to-SQL systems by up to 20%.

</details>


### [51] [DRE: An Effective Dual-Refined Method for Integrating Small and Large Language Models in Open-Domain Dialogue Evaluation](https://arxiv.org/abs/2506.04516)
*Kun Zhao,Bohao Yang,Chen Tang,Siyuan Dai,Haoteng Tang,Chenghua Lin,Liang Zhan*

Main category: cs.CL

TL;DR: The paper introduces SLIDE and DRE, methods that integrate SLMs and LLMs to improve dialogue evaluation, showing better alignment with human judgment.


<details>
  <summary>Details</summary>
Motivation: To leverage the strengths of both SLMs (robust in ambiguous scenarios) and LLMs (effective with negative examples) to improve the reliability of dialogue evaluation.

Method: SLIDE integrates SLMs and LLMs via adaptive weighting, while DRE uses SLM-generated insights to guide initial LLM evaluations and SLM-derived adjustments to refine LLM scores.

Result: Experiments demonstrate that DRE outperforms existing methods and aligns better with human judgment across diverse benchmarks.

Conclusion: Combining SLMs and LLMs can lead to more reliable evaluation tools, especially for open-ended tasks like dialogue evaluation.

Abstract: Large Language Models (LLMs) excel at many tasks but struggle with ambiguous
scenarios where multiple valid responses exist, often yielding unreliable
results. Conversely, Small Language Models (SLMs) demonstrate robustness in
such scenarios but are susceptible to misleading or adversarial inputs. We
observed that LLMs handle negative examples effectively, while SLMs excel with
positive examples. To leverage their complementary strengths, we introduce
SLIDE (Small and Large Integrated for Dialogue Evaluation), a method
integrating SLMs and LLMs via adaptive weighting. Building on SLIDE, we further
propose a Dual-Refinement Evaluation (DRE) method to enhance SLM-LLM
integration: (1) SLM-generated insights guide the LLM to produce initial
evaluations; (2) SLM-derived adjustments refine the LLM's scores for improved
accuracy. Experiments demonstrate that DRE outperforms existing methods,
showing stronger alignment with human judgment across diverse benchmarks. This
work illustrates how combining small and large models can yield more reliable
evaluation tools, particularly for open-ended tasks such as dialogue
evaluation.

</details>


### [52] [Please Translate Again: Two Simple Experiments on Whether Human-Like Reasoning Helps Translation](https://arxiv.org/abs/2506.04521)
*Di Wu,Seth Aycock,Christof Monz*

Main category: cs.CL

TL;DR: This paper investigates the effectiveness of decomposing translation into multiple steps using Large Language Models (LLMs) and finds that simply prompting LLMs to 'translate again' yields better results than using multi-step prompts.


<details>
  <summary>Details</summary>
Motivation: The motivation is to scrutinize the effectiveness of the step-by-step decomposition approach in LLM-based translation, which has been claimed to improve performance.

Method: The researchers empirically evaluate the performance of LLMs using multi-step prompts for translation and compare it with the performance achieved by simply prompting the LLMs to 'translate again'.

Result: The results show no clear evidence that explicitly decomposing the translation process improves performance, and that the 'translate again' approach yields even better results.

Conclusion: The paper concludes that the effectiveness of Chain-of-Thought (CoT) reasoning in translation is not well understood and invites future work to explore the factors contributing to its effectiveness.

Abstract: Large Language Models (LLMs) demonstrate strong reasoning capabilities for
many tasks, often by explicitly decomposing the task via Chain-of-Thought (CoT)
reasoning. Recent work on LLM-based translation designs hand-crafted prompts to
decompose translation, or trains models to incorporate intermediate
steps.~\textit{Translating Step-by-step}~\citep{briakou2024translating}, for
instance, introduces a multi-step prompt with decomposition and refinement of
translation with LLMs, which achieved state-of-the-art results on WMT24. In
this work, we scrutinise this strategy's effectiveness. Empirically, we find no
clear evidence that performance gains stem from explicitly decomposing the
translation process, at least for the models on test; and we show that simply
prompting LLMs to ``translate again'' yields even better results than
human-like step-by-step prompting. Our analysis does not rule out the role of
reasoning, but instead invites future work exploring the factors for CoT's
effectiveness in the context of translation.

</details>


### [53] [Is It JUST Semantics? A Case Study of Discourse Particle Understanding in LLMs](https://arxiv.org/abs/2506.04534)
*William Sheffield,Kanishka Misra,Valentina Pyatkin,Ashwini Deo,Kyle Mahowald,Junyi Jessy Li*

Main category: cs.CL

TL;DR: This paper examines the ability of large language models (LLMs) to distinguish the nuanced meanings of the English discourse particle 'just', revealing that while LLMs can differentiate broader categories, they struggle with finer semantic distinctions.


<details>
  <summary>Details</summary>
Motivation: The motivation is to assess the understanding of fine-grained semantic distinctions in discourse particles by LLMs, focusing on the well-studied particle 'just' to highlight the limitations and capabilities of these models.

Method: The study uses data meticulously created and labeled by expert linguists to evaluate the performance of LLMs in distinguishing the various senses of 'just'.

Result: The findings show that while LLMs can differentiate between broader categories of 'just', they struggle with capturing more subtle nuances.

Conclusion: The paper concludes that there is a significant gap in the ability of LLMs to fully understand and capture the nuanced meanings of discourse particles like 'just'.

Abstract: Discourse particles are crucial elements that subtly shape the meaning of
text. These words, often polyfunctional, give rise to nuanced and often quite
disparate semantic/discourse effects, as exemplified by the diverse uses of the
particle "just" (e.g., exclusive, temporal, emphatic). This work investigates
the capacity of LLMs to distinguish the fine-grained senses of English "just",
a well-studied example in formal semantics, using data meticulously created and
labeled by expert linguists. Our findings reveal that while LLMs exhibit some
ability to differentiate between broader categories, they struggle to fully
capture more subtle nuances, highlighting a gap in their understanding of
discourse particles.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [54] [On the Spherical Dirichlet Distribution: Corrections and Results](https://arxiv.org/abs/2506.04441)
*Jose H Guardiola*

Main category: stat.ME

TL;DR: This paper corrects an error in a previous study, updates derivations, and introduces a Spherical-Dirichlet Distribution for data on the positive orthant of the hypersphere, presenting its properties and applications.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address a technical error in a previous study and to introduce a novel probability distribution, the Spherical-Dirichlet Distribution, which is particularly useful for data in data mining and gene expression analysis.

Method: The paper develops the Spherical-Dirichlet Distribution, derives its basic properties (normalizing constants and moments), and explores its relationships with other distributions. It also obtains estimators using classical inferential statistics (method of moments and maximum likelihood).

Result: The paper presents two applications: one with simulated data and another with real text mining data. Both applications are fitted using the Spherical-Dirichlet Distribution, and the results are discussed.

Conclusion: The Spherical-Dirichlet Distribution is a valuable tool for modeling data on the positive orthant of the hypersphere, particularly in data mining and gene expression analysis. The paper corrects previous errors and provides a comprehensive exploration of the distribution's properties and applications.

Abstract: This note corrects a technical error in Guardiola (2020, Journal of
Statistical Distributions and Applications), presents updated derivations, and
offers an extended discussion of the properties of the spherical Dirichlet
distribution. Today, data mining and gene expressions are at the forefront of
modern data analysis. Here we introduce a novel probability distribution that
is applicable in these fields. This paper develops the proposed
Spherical-Dirichlet Distribution designed to fit vectors located at the
positive orthant of the hypersphere, as it is often the case for data in these
fields, avoiding unnecessary probability mass. Basic properties of the proposed
distribution, including normalizing constants and moments are developed.
Relationships with other distributions are also explored. Estimators based on
classical inferential statistics, such as method of moments and maximum
likelihood estimators are obtained. Two applications are developed: the first
one uses simulated data, and the second uses a real text mining example. Both
examples are fitted using the proposed Spherical-Dirichlet Distribution and
their results are discussed.

</details>


### [55] [Robust Estimation in Step-Stress Experiments under Exponential Lifetime Distributions](https://arxiv.org/abs/2506.04445)
*María Jaenada,Juan Manuel Millán,Leandro Pardo*

Main category: stat.ME

TL;DR: The paper proposes using Minimum Density Power Divergence Estimators (MDPDEs) as a robust alternative to the traditional maximum likelihood estimator (MLE) in step-stress accelerated life tests (ALTs) for high-reliability products, demonstrating its efficiency and robustness through simulation and real data analysis.


<details>
  <summary>Details</summary>
Motivation: High-reliability products often require impractically long testing times under normal operating conditions to obtain sufficient failure data. Accelerated life tests (ALTs) and step-stress experiments are used to induce earlier failures, but traditional methods like MLE can be sensitive to outlying data, necessitating a more robust approach.

Method: The paper introduces MDPDEs, a robust alternative to MLE, for parameter estimation in step-stress ALTs. The method is based on mixed distributions and its theoretical properties, including asymptotic distribution, are derived under exponential lifetime assumptions.

Result: Simulation studies show that MDPDEs provide a good balance between efficiency and robustness. The method's applicability is further demonstrated using real data, indicating its potential for practical use in reliability testing.

Conclusion: MDPDEs offer an attractive compromise between efficiency and robustness in step-stress ALTs, making them a valuable tool for analyzing high-reliability products with accelerated testing methods.

Abstract: Many modern products exhibit high reliability, often resulting in long times
to failure. Consequently, conducting experiments under normal operating
conditions may require an impractically long duration to obtain sufficient
failure data for reliable statistical inference. As an alternative, accelerated
life tests (ALTs) are employed to induce earlier failures and thereby reduce
testing time. In step-stress experiments a stress factor that accelerates
product degradation is identified and systematically increased to provoke early
failures. The stress level is increased at predetermined time points and
maintained constant between these intervals. Failure data observed under
increased levels of stress is statistically analyzed, and results are then
extrapolate to normal operating conditions.
  Classical estimation methods such analysis rely on the maximum likelihood
estimator (MLE) which is know to be very efficient, but lack robustness in the
presence of outlying data. In this work, Minimum Density Power Divergence
Estimators (MDPDEs) are proposed as a robust alternative, demonstrating an
appealing compromise between efficiency and robustness. The MDPDE based on
mixed distributions is developed, and its theoretical properties, including the
expression for the asymptotic distribution of the model parameters, are derived
under exponential lifetime assumptions. The good performance of the proposed
method is evaluated through simulation studies, and its applicability is
demonstrated using real data.

</details>


### [56] [The Spurious Factor Dilemma: Robust Inference in Heavy-Tailed Elliptical Factor Models](https://arxiv.org/abs/2506.05116)
*Jiang Hu,Jiahui Xie,Yangchun Zhang,Wang Zhou*

Main category: stat.ME

TL;DR: The paper proposes a novel fluctuation magnification algorithm to accurately determine the number of factors in Elliptical Factor Models (EFM) with heavy-tailed data, addressing the issue of overestimation by standard methods.


<details>
  <summary>Details</summary>
Motivation: Standard methods often overestimate the number of factors in factor models when data exhibit heavy-tailed randomness, misinterpreting noise-induced outliers as genuine factors. This paper aims to address this challenge.

Method: The paper introduces a fluctuation magnification algorithm that distinguishes between real and spurious eigenvalues by observing their behavior under magnifying perturbations. Real factors exhibit less fluctuation (stabilizing asymptotically) compared to spurious eigenvalues from heavy-tailed noise.

Result: Simulation studies and real data analysis show that the proposed method effectively and accurately selects the number of common factors in heavy-tailed EFMs, outperforming existing methods, especially in scenarios with pronounced heavy-tailedness.

Conclusion: The fluctuation magnification algorithm provides a robust solution for identifying the true number of factors in heavy-tailed data, improving the reliability of factor models in economics and finance.

Abstract: Factor models are essential tools for analyzing high-dimensional data,
particularly in economics and finance. However, standard methods for
determining the number of factors often overestimate the true number when data
exhibit heavy-tailed randomness, misinterpreting noise-induced outliers as
genuine factors. This paper addresses this challenge within the framework of
Elliptical Factor Models (EFM), which accommodate both heavy tails and
potential non-linear dependencies common in real-world data. We demonstrate
theoretically and empirically that heavy-tailed noise generates spurious
eigenvalues that mimic true factor signals. To distinguish these, we propose a
novel methodology based on a fluctuation magnification algorithm. We show that
under magnifying perturbations, the eigenvalues associated with real factors
exhibit significantly less fluctuation (stabilizing asymptotically) compared to
spurious eigenvalues arising from heavy-tailed effects. This differential
behavior allows the identification and detection of the true and spurious
factors. We develop a formal testing procedure based on this principle and
apply it to the problem of accurately selecting the number of common factors in
heavy-tailed EFMs. Simulation studies and real data analysis confirm the
effectiveness of our approach compared to existing methods, particularly in
scenarios with pronounced heavy-tailedness.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [57] [Transformers Meet In-Context Learning: A Universal Approximation Theory](https://arxiv.org/abs/2506.05200)
*Gen Li,Yuchen Jiao,Yu Huang,Yuting Wei,Yuxin Chen*

Main category: cs.LG

TL;DR: The paper develops a theory for understanding in-context learning in transformers, focusing on universal function approximation rather than algorithm approximation, allowing transformers to perform tasks reliably with few examples without further weight updates.


<details>
  <summary>Details</summary>
Motivation: To better understand and formalize the mechanism behind in-context learning in transformers, which allows them to perform new tasks using only a few examples in the prompt without fine-tuning.

Method: The authors construct a transformer that can perform reliable predictions for any class of functions (tasks) given a few in-context examples, using a universal function approximation approach rather than approximating optimization algorithms.

Result: The constructed transformer can perform tasks reliably with few in-context examples, providing approximation guarantees that are not limited by the effectiveness of optimization algorithms, extending beyond convex problems and linear function classes.

Conclusion: The paper’s approach offers a new perspective on how transformers can learn general-purpose representations and adapt dynamically to in-context examples, contributing to a deeper understanding of in-context learning in large language models.

Abstract: Modern large language models are capable of in-context learning, the ability
to perform new tasks at inference time using only a handful of input-output
examples in the prompt, without any fine-tuning or parameter updates. We
develop a universal approximation theory to better understand how transformers
enable in-context learning. For any class of functions (each representing a
distinct task), we demonstrate how to construct a transformer that, without any
further weight updates, can perform reliable prediction given only a few
in-context examples. In contrast to much of the recent literature that frames
transformers as algorithm approximators -- i.e., constructing transformers to
emulate the iterations of optimization algorithms as a means to approximate
solutions of learning problems -- our work adopts a fundamentally different
approach rooted in universal function approximation. This alternative approach
offers approximation guarantees that are not constrained by the effectiveness
of the optimization algorithms being approximated, thereby extending far beyond
convex problems and linear function classes. Our construction sheds light on
how transformers can simultaneously learn general-purpose representations and
adapt dynamically to in-context examples.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [58] [Classification of Extremal Dependence in Financial Markets via Bootstrap Inference](https://arxiv.org/abs/2506.04656)
*Qian Hui,Sidney I. Resnick,Tiandong Wang*

Main category: math.ST

TL;DR: The paper uses a bootstrap-based method to analyze the extremal dependence in U.S. and Chinese stock markets, finding more isolated clustering in the U.S. and more interconnected patterns in China.


<details>
  <summary>Details</summary>
Motivation: The motivation is to accurately identify the extremal dependence structure in multivariate heavy-tailed data, which is crucial for financial applications and risk management.

Method: The paper employs a bootstrap-based testing procedure to analyze the extremal dependence structure in multivariate heavy-tailed data, specifically focusing on the absolute log returns of U.S. S&P 500 and Chinese A-share stocks.

Result: The analysis shows more isolated clustering of dependent assets in the U.S. compared to China, which exhibits a more interconnected pattern. Strong extremal linkages are identified in sectors like materials, consumer staples, and consumer discretionary.

Conclusion: The testing procedure is effective for large-scale empirical applications, revealing distinct patterns of extremal dependence between the U.S. and Chinese markets.

Abstract: Accurately identifying the extremal dependence structure in multivariate
heavy-tailed data is a fundamental yet challenging task, particularly in
financial applications. Following a recently proposed bootstrap-based testing
procedure, we apply the methodology to absolute log returns of U.S. S&P 500 and
Chinese A-share stocks over a time period well before the U.S. election in
2024. The procedure reveals more isolated clustering of dependent assets in the
U.S. economy compared with China which exhibits different characteristics and a
more interconnected pattern of extremal dependence. Cross-market analysis
identifies strong extremal linkages in sectors such as materials, consumer
staples and consumer discretionary, highlighting the effectiveness of the
testing procedure for large-scale empirical applications.

</details>


### [59] [A dimension reduction for extreme types of directed dependence](https://arxiv.org/abs/2506.04825)
*Sebastian Fuchs,Carsten Limbach*

Main category: math.ST

TL;DR: This paper translates extreme variants of directed dependence, typically formulated for random vector (X,Y), into the Markov product (Y,Y') to better understand the dimension reduction principle.


<details>
  <summary>Details</summary>
Motivation: To gain deeper insight into the dimension reduction principle used in dependence measures, translating extreme forms of directed dependence into Markov product representation is necessary.

Method: The paper reformulates extreme variants of directed dependence in terms of the Markov product (Y,Y'), where Y' is a conditionally independent copy of Y given X, to facilitate estimation via nearest neighbor methods.

Result: The translation into the Markov product representation provides a clearer understanding of the dimension reduction principle and its application in estimating various dependence measures.

Conclusion: This translation enhances the theoretical foundation for using the dimension reduction principle and nearest neighbor methods to estimate complex dependence measures between predictor variables and a response variable.

Abstract: In recent years, a variety of novel measures of dependence have been
introduced being capable of characterizing diverse types of directed
dependence, hence diverse types of how a number of predictor variables
$\mathbf{X} = (X_1, \dots, X_p)$, $p \in \mathbb{N}$, may affect a response
variable $Y$. This includes perfect dependence of $Y$ on $\mathbf{X}$ and
independence between $\mathbf{X}$ and $Y$, but also less well-known concepts
such as zero-explainability, stochastic comparability and complete separation.
Certain such measures offer a representation in terms of the Markov product
$(Y,Y')$, with $Y'$ being a conditionally independent copy of $Y$ given
$\mathbf{X}$. This dimension reduction principle allows these measures to be
estimated via the powerful nearest neighbor based estimation principle
introduced in [4]. To achieve a deeper insight into the dimension reduction
principle, this paper aims at translating the extreme variants of directed
dependence, typically formulated in terms of the random vector
$(\mathbf{X},Y)$, into the Markov product $(Y,Y')$.

</details>


### [60] [kTULA: A Langevin sampling algorithm with improved KL bounds under super-linear log-gradients](https://arxiv.org/abs/2506.04878)
*Iosif Lytras,Sotirios Sabanis,Ying Zhang*

Main category: math.ST

TL;DR: This paper addresses the challenge of sampling from distributions with super-linearly growing log-gradients in deep learning, proposing a novel algorithm called kTULA with improved convergence rates in KL divergence and Wasserstein-2 distance.


<details>
  <summary>Details</summary>
Motivation: The global Lipschitz continuity condition is often not satisfied in deep learning, leading to the need for efficient sampling methods for distributions with super-linearly growing log-gradients.

Method: The authors propose kTULA, a tamed Langevin dynamics-based algorithm, and establish a non-asymptotic convergence bound in KL divergence with a rate of 2-ε (ε > 0). They also derive an improved error bound in Wasserstein-2 distance.

Result: kTULA achieves the best-known convergence rate in KL divergence and provides an improved error bound in Wasserstein-2 distance, which can be used to give theoretical guarantees for solving associated optimization problems.

Conclusion: The proposed kTULA algorithm is applied to sampling from a high-dimensional double-well potential distribution and to an optimization problem involving a neural network, demonstrating its effectiveness and providing theoretical guarantees for its performance.

Abstract: Motivated by applications in deep learning, where the global Lipschitz
continuity condition is often not satisfied, we examine the problem of sampling
from distributions with super-linearly growing log-gradients. We propose a
novel tamed Langevin dynamics-based algorithm, called kTULA, to solve the
aforementioned sampling problem, and provide a theoretical guarantee for its
performance. More precisely, we establish a non-asymptotic convergence bound in
Kullback-Leibler (KL) divergence with the best-known rate of convergence equal
to $2-\overline{\epsilon}$, $\overline{\epsilon}>0$, which significantly
improves relevant results in existing literature. This enables us to obtain an
improved non-asymptotic error bound in Wasserstein-2 distance, which can be
used to further derive a non-asymptotic guarantee for kTULA to solve the
associated optimization problems. To illustrate the applicability of kTULA, we
apply the proposed algorithm to the problem of sampling from a high-dimensional
double-well potential distribution and to an optimization problem involving a
neural network. We show that our main results can be used to provide
theoretical guarantees for the performance of kTULA.

</details>


### [61] [At the edge of Donsker's Theorem: Asymptotics of multiscale scan statistics](https://arxiv.org/abs/2506.05112)
*Johann Köhne,Fabian Mies*

Main category: math.ST

TL;DR: The paper develops a feasible multiscale test method for nonparametric inference, addressing the challenge of critical values dependence on noise distribution, and demonstrates its application to nonstationary nonlinear time series and real-world data like the 2025 Iberian peninsula power blackout.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the limitations of existing multiscale testing procedures, which are either statistically infeasible or asymptotically sub-optimal, and to create a method that is robust to different noise distributions.

Method: The paper introduces a feasible multiscale test by using weak convergence arguments and replacing the additive multiscale penalty with a multiplicative weighting. This method is applied to nonstationary nonlinear time series through a tailored bootstrap scheme.

Result: The new methodology achieves optimal detection properties and is applicable to various inference problems such as signal discovery, goodness-of-fit testing, and multiple changepoint detection. The method is demonstrated with an analysis of the April 2025 power blackout on the Iberian peninsula.

Conclusion: The paper concludes with the development of a novel functional central limit theorem in Hölder spaces and a new form of thresholded weak convergence, which are key to the success of the proposed multiscale test in handling nonstationary data.

Abstract: For nonparametric inference about a function, multiscale testing procedures
resolve the need for bandwidth selection and achieve asymptotically optimal
detection performance against a broad range of alternatives. However, critical
values strongly depend on the noise distribution, and we argue that existing
methods are either statistically infeasible, or asymptotically sub-optimal. To
address this methodological challenge, we show how to develop a feasible
multiscale test via weak convergence arguments, by replacing the additive
multiscale penalty with a multiplicative weighting. This new theoretical
foundation preserves the optimal detection properties of multiscale tests and
extends their applicability to nonstationary nonlinear time series via a
tailored bootstrap scheme. Inference for signal discovery, goodness-of-fit
testing of regression functions, and multiple changepoint detection is studied
in detail, and we apply the new methodology to analyze the April 2025 power
blackout on the Iberian peninsula. Our methodology is enabled by a novel
functional central limit in H\"older spaces with critical modulus of
continuity, where Donsker's theorem fails to hold due to lack of tightness.
Probabilistically, we discover a novel form of thresholded weak convergence
that holds only in the upper support of the distribution.

</details>


### [62] [Statistical microlocal analysis in two-dimensional X-ray CT](https://arxiv.org/abs/2506.05113)
*Anuj Abhishek,Alexander Katsevich,James W. Webber*

Main category: math.ST

TL;DR: The paper introduces Statistical Microlocal Analysis (SMA) to assess edge detectability in 2D X-ray CT images, accounting for noise and finite sampling step size, and validates the method through simulations.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of conventional microlocal analysis, which does not account for noise and finite sampling step size in image reconstruction, and to provide a robust method for edge detection in practical imaging scenarios.

Method: The method involves developing a statistical hypothesis testing framework called Statistical Microlocal Analysis (SMA) to determine if image edges are detectable from reconstructed images. The approach characterizes the reconstructed image in local neighborhoods and quantifies edge detectability using statistical power.

Result: The method is validated through simulations, showing strong agreement between theoretical predictions and experimental observations.

Conclusion: The paper concludes that SMA is a valuable extension of classical microlocal analysis, providing a practical and theoretically sound method for assessing edge detectability in 2D X-ray CT images, even in the presence of noise and finite sampling step size.

Abstract: In many imaging applications it is important to assess how well the edges of
the original object, $f$, are resolved in an image, $f^\text{rec}$,
reconstructed from the measured data, $g$. In this paper we consider the case
of image reconstruction in 2D X-ray Computed Tomography (CT). Let $f$ be a
function describing the object being scanned, and $g=Rf + \eta$ be the Radon
transform data in $\mathbb{R}^2$ corrupted by noise, $\eta$, and sampled with
step size $\sim\epsilon$. Conventional microlocal analysis provides conditions
for edge detectability based on the scanner geometry in the case of continuous,
noiseless data (when $\eta = 0$), but does not account for noise and finite
sampling step size. We develop a novel technique called \emph{Statistical
Microlocal Analysis} (SMA), which uses a statistical hypothesis testing
framework to determine if an image edge (singularity) of $f$ is detectable from
$f^\text{rec}$, and we quantify edge detectability using the statistical power
of the test. Our approach is based on the theory we developed in
\cite{AKW2024_1}, which provides a characterization of $f^\text{rec}$ in local
$O(\epsilon)$-size neighborhoods when $\eta \neq 0$. We derive a statistical
test for the presence and direction of an edge microlocally given the magnitude
of $\eta$ and data sampling step size. Using the properties of the null
distribution of the test, we quantify the uncertainty of the edge magnitude and
direction. We validate our theory using simulations, which show strong
agreement between our predictions and experimental observations. Our work is
not only of practical value, but of theoretical value as well. SMA is a natural
extension of classical microlocal analysis theory which accounts for practical
measurement imperfections, such as noise and finite step size, at the highest
possible resolution compatible with the data.

</details>
