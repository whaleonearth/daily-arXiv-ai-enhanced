<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 65]
- [cs.AI](#cs.AI) [Total: 19]
- [cs.CL](#cs.CL) [Total: 42]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.SD](#cs.SD) [Total: 2]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.DB](#cs.DB) [Total: 1]
- [math.ST](#math.ST) [Total: 1]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.LG](#cs.LG) [Total: 34]
- [cs.CY](#cs.CY) [Total: 2]
- [stat.ML](#stat.ML) [Total: 2]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 1]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [math.DG](#math.DG) [Total: 1]
- [cs.DM](#cs.DM) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Moment Sampling in Video LLMs for Long-Form Video QA](https://arxiv.org/abs/2507.00033)
*Mustafa Chasmai,Gauri Jagatap,Gouthaman KV,Grant Van Horn,Subhransu Maji,Andrea Fanelli*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Recent advancements in video large language models (Video LLMs) have
significantly advanced the field of video question answering (VideoQA). While
existing methods perform well on short videos, they often struggle with
long-range reasoning in longer videos. To scale Video LLMs for longer video
content, frame sub-sampling (selecting frames at regular intervals) is commonly
used. However, this approach is suboptimal, often leading to the loss of
crucial frames or the inclusion of redundant information from multiple similar
frames. Missing key frames impairs the model's ability to answer questions
accurately, while redundant frames lead the model to focus on irrelevant video
segments and increase computational resource consumption. In this paper, we
investigate the use of a general-purpose text-to-video moment retrieval model
to guide the frame sampling process. We propose "moment sampling", a novel,
model-agnostic approach that enables the model to select the most relevant
frames according to the context of the question. Specifically, we employ a
lightweight moment retrieval model to prioritize frame selection. By focusing
on the frames most pertinent to the given question, our method enhances
long-form VideoQA performance in Video LLMs. Through extensive experiments on
four long-form VideoQA datasets, using four state-of-the-art Video LLMs, we
demonstrate the effectiveness of the proposed approach.

</details>


### [2] [Catastrophic Forgetting Mitigation via Discrepancy-Weighted Experience Replay](https://arxiv.org/abs/2507.00042)
*Xinrun Xu,Jianwen Yang,Qiuhong Zhang,Zhanbiao Lian,Zhiming Ding,Shan Jiang*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Continually adapting edge models in cloud-edge collaborative object detection
for traffic monitoring suffers from catastrophic forgetting, where models lose
previously learned knowledge when adapting to new data distributions. This is
especially problematic in dynamic traffic environments characterised by
periodic variations (e.g., day/night, peak hours), where past knowledge remains
valuable. Existing approaches like experience replay and visual prompts offer
some mitigation, but struggle to effectively prioritize and leverage historical
data for optimal knowledge retention and adaptation. Specifically, simply
storing and replaying all historical data can be inefficient, while treating
all historical experiences as equally important overlooks their varying
relevance to the current domain. This paper proposes ER-EMU, an edge model
update algorithm based on adaptive experience replay, to address these
limitations. ER-EMU utilizes a limited-size experience buffer managed using a
First-In-First-Out (FIFO) principle, and a novel Domain Distance Metric-based
Experience Selection (DDM-ES) algorithm. DDM-ES employs the multi-kernel
maximum mean discrepancy (MK-MMD) to quantify the dissimilarity between target
domains, prioritizing the selection of historical data that is most dissimilar
to the current target domain. This ensures training diversity and facilitates
the retention of knowledge from a wider range of past experiences, while also
preventing overfitting to the new domain. The experience buffer is also updated
using a simple random sampling strategy to maintain a balanced representation
of previous domains. Experiments on the Bellevue traffic video dataset,
involving repeated day/night cycles, demonstrate that ER-EMU consistently
improves the performance of several state-of-the-art cloud-edge collaborative
object detection frameworks.

</details>


### [3] [MR-CLIP: Efficient Metadata-Guided Learning of MRI Contrast Representations](https://arxiv.org/abs/2507.00043)
*Mehmet Yigit Avci,Pedro Borges,Paul Wright,Mehmet Yigitsoy,Sebastien Ourselin,Jorge Cardoso*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Accurate interpretation of Magnetic Resonance Imaging scans in clinical
systems is based on a precise understanding of image contrast. This contrast is
primarily governed by acquisition parameters, such as echo time and repetition
time, which are stored in the DICOM metadata. To simplify contrast
identification, broad labels such as T1-weighted or T2-weighted are commonly
used, but these offer only a coarse approximation of the underlying acquisition
settings. In many real-world datasets, such labels are entirely missing,
leaving raw acquisition parameters as the only indicators of contrast. Adding
to this challenge, the available metadata is often incomplete, noisy, or
inconsistent. The lack of reliable and standardized metadata complicates tasks
such as image interpretation, retrieval, and integration into clinical
workflows. Furthermore, robust contrast-aware representations are essential to
enable more advanced clinical applications, such as achieving
modality-invariant representations and data harmonization. To address these
challenges, we propose MR-CLIP, a multimodal contrastive learning framework
that aligns MR images with their DICOM metadata to learn contrast-aware
representations, without relying on manual labels. Trained on a diverse
clinical dataset that spans various scanners and protocols, MR-CLIP captures
contrast variations across acquisitions and within scans, enabling
anatomy-invariant representations. We demonstrate its effectiveness in
cross-modal retrieval and contrast classification, highlighting its scalability
and potential for further clinical applications. The code and weights are
publicly available at https://github.com/myigitavci/MR-CLIP.

</details>


### [4] [HistoART: Histopathology Artifact Detection and Reporting Tool](https://arxiv.org/abs/2507.00044)
*Seyed Kahaki,Alexander R. Webber,Ghada Zamzmi,Adarsh Subbaswamy,Rucha Deshpande,Aldo Badano*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In modern cancer diagnostics, Whole Slide Imaging (WSI) is widely used to
digitize tissue specimens for detailed, high-resolution examination; however,
other diagnostic approaches, such as liquid biopsy and molecular testing, are
also utilized based on the cancer type and clinical context. While WSI has
revolutionized digital histopathology by enabling automated, precise analysis,
it remains vulnerable to artifacts introduced during slide preparation and
scanning. These artifacts can compromise downstream image analysis. To address
this challenge, we propose and compare three robust artifact detection
approaches for WSIs: (1) a foundation model-based approach (FMA) using a
fine-tuned Unified Neural Image (UNI) architecture, (2) a deep learning
approach (DLA) built on a ResNet50 backbone, and (3) a knowledge-based approach
(KBA) leveraging handcrafted features from texture, color, and frequency-based
metrics. The methods target six common artifact types: tissue folds,
out-of-focus regions, air bubbles, tissue damage, marker traces, and blood
contamination. Evaluations were conducted on 50,000+ image patches from diverse
scanners (Hamamatsu, Philips, Leica Aperio AT2) across multiple sites. The FMA
achieved the highest patch-wise AUROC of 0.995 (95% CI [0.994, 0.995]),
outperforming the ResNet50-based method (AUROC: 0.977, 95% CI [0.977, 0.978])
and the KBA (AUROC: 0.940, 95% CI [0.933, 0.946]). To translate detection into
actionable insights, we developed a quality report scorecard that quantifies
high-quality patches and visualizes artifact distributions.

</details>


### [5] [CaughtCheating: Is Your MLLM a Good Cheating Detective? Exploring the Boundary of Visual Perception and Reasoning](https://arxiv.org/abs/2507.00045)
*Ming Li,Chenguang Wang,Yijun Liang,Xiyao Wang,Yuhang Zhou,Xiyang Wu,Yuqing Zhang,Ruiyi Zhang,Tianyi Zhou*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Recent agentic Multi-Modal Large Language Models (MLLMs) such as GPT-o3 have
achieved near-ceiling scores on various existing benchmarks, motivating a
demand for more challenging test tasks. These MLLMs have been reported to excel
in a few expert-level tasks for humans, e.g., GeoGuesser, reflecting their
potential as a detective who can notice minuscule cues in an image and weave
them into coherent, situational explanations, leading to a reliable answer. But
can they match the performance of excellent human detectives? To answer this
question, we investigate some hard scenarios where GPT-o3 can still handle, and
find a common scenario where o3's performance drops to nearly zero, which we
name CaughtCheating. It is inspired by the social media requests that ask
others to detect suspicious clues from photos shared by the poster's partner.
We conduct extensive experiments and analysis to understand why existing MLLMs
lack sufficient capability to solve this kind of task. CaughtCheating provides
a class of challenging visual perception and reasoning tasks with great value
and practical usage. Success in these tasks paves the way for MLLMs to acquire
human-level detective perception and reasoning capabilities.

</details>


### [6] [Evolutionary computing-based image segmentation method to detect defects and features in Additive Friction Stir Deposition Process](https://arxiv.org/abs/2507.00046)
*Akshansh Mishra,Eyob Mesele Sefene,Shivraman Thapliyal*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This work proposes an evolutionary computing-based image segmentation
approach for analyzing soundness in Additive Friction Stir Deposition (AFSD)
processes. Particle Swarm Optimization (PSO) was employed to determine optimal
segmentation thresholds for detecting defects and features in multilayer AFSD
builds. The methodology integrates gradient magnitude analysis with distance
transforms to create novel attention-weighted visualizations that highlight
critical interface regions. Five AFSD samples processed under different
conditions were analyzed using multiple visualization techniques i.e.
self-attention maps, and multi-channel visualization. These complementary
approaches reveal subtle material transition zones and potential defect regions
which were not readily observable through conventional imaging. The PSO
algorithm automatically identified optimal threshold values (ranging from
156-173) for each sample, enabling precise segmentation of material interfaces.
The multi-channel visualization technique effectively combines boundary
information (red channel), spatial relationships (green channel), and material
density data (blue channel) into cohesive representations that quantify
interface quality. The results demonstrate that attention-based analysis
successfully identifies regions of incomplete bonding and inhomogeneities in
AFSD joints, providing quantitative metrics for process optimization and
quality assessment of additively manufactured components.

</details>


### [7] [AdaDeDup: Adaptive Hybrid Data Pruning for Efficient Large-Scale Object Detection Training](https://arxiv.org/abs/2507.00049)
*Feiyang Kang,Nadine Chang,Maying Shen,Marc T. Law,Rafid Mahmood,Ruoxi Jia,Jose M. Alvarez*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The computational burden and inherent redundancy of large-scale datasets
challenge the training of contemporary machine learning models. Data pruning
offers a solution by selecting smaller, informative subsets, yet existing
methods struggle: density-based approaches can be task-agnostic, while
model-based techniques may introduce redundancy or prove computationally
prohibitive. We introduce Adaptive De-Duplication (AdaDeDup), a novel hybrid
framework that synergistically integrates density-based pruning with
model-informed feedback in a cluster-adaptive manner. AdaDeDup first partitions
data and applies an initial density-based pruning. It then employs a proxy
model to evaluate the impact of this initial pruning within each cluster by
comparing losses on kept versus pruned samples. This task-aware signal
adaptively adjusts cluster-specific pruning thresholds, enabling more
aggressive pruning in redundant clusters while preserving critical data in
informative ones. Extensive experiments on large-scale object detection
benchmarks (Waymo, COCO, nuScenes) using standard models (BEVFormer, Faster
R-CNN) demonstrate AdaDeDup's advantages. It significantly outperforms
prominent baselines, substantially reduces performance degradation (e.g., over
54% versus random sampling on Waymo), and achieves near-original model
performance while pruning 20% of data, highlighting its efficacy in enhancing
data efficiency for large-scale model training. Code is open-sourced.

</details>


### [8] [VSF-Med:A Vulnerability Scoring Framework for Medical Vision-Language Models](https://arxiv.org/abs/2507.00052)
*Binesh Sadanandan,Vahid Behzadan*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Vision Language Models (VLMs) hold great promise for streamlining
labour-intensive medical imaging workflows, yet systematic security evaluations
in clinical settings remain scarce. We introduce VSF--Med, an end-to-end
vulnerability-scoring framework for medical VLMs that unites three novel
components: (i) a rich library of sophisticated text-prompt attack templates
targeting emerging threat vectors; (ii) imperceptible visual perturbations
calibrated by structural similarity (SSIM) thresholds to preserve clinical
realism; and (iii) an eight-dimensional rubric evaluated by two independent
judge LLMs, whose raw scores are consolidated via z-score normalization to
yield a 0--32 composite risk metric. Built entirely on publicly available
datasets and accompanied by open-source code, VSF--Med synthesizes over 30,000
adversarial variants from 5,000 radiology images and enables reproducible
benchmarking of any medical VLM with a single command. Our consolidated
analysis reports mean z-score shifts of $0.90\sigma$ for
persistence-of-attack-effects, $0.74\sigma$ for prompt-injection effectiveness,
and $0.63\sigma$ for safety-bypass success across state-of-the-art VLMs.
Notably, Llama-3.2-11B-Vision-Instruct exhibits a peak vulnerability increase
of $1.29\sigma$ for persistence-of-attack-effects, while GPT-4o shows increases
of $0.69\sigma$ for that same vector and $0.28\sigma$ for prompt-injection
attacks.

</details>


### [9] [MANTA: Cross-Modal Semantic Alignment and Information-Theoretic Optimization for Long-form Multimodal Understanding](https://arxiv.org/abs/2507.00068)
*Ziqi Zhong,Daniel Tang*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: While multi-modal learning has advanced significantly, current approaches
often treat modalities separately, creating inconsistencies in representation
and reasoning. We introduce MANTA (Multi-modal Abstraction and Normalization
via Textual Alignment), a theoretically-grounded framework that unifies visual
and auditory inputs into a structured textual space for seamless processing
with large language models. MANTA addresses four key challenges: (1) semantic
alignment across modalities with information-theoretic optimization, (2)
adaptive temporal synchronization for varying information densities, (3)
hierarchical content representation for multi-scale understanding, and (4)
context-aware retrieval of sparse information from long sequences. We formalize
our approach within a rigorous mathematical framework, proving its optimality
for context selection under token constraints. Extensive experiments on the
challenging task of Long Video Question Answering show that MANTA improves
state-of-the-art models by up to 22.6% in overall accuracy, with particularly
significant gains (27.3%) on videos exceeding 30 minutes. Additionally, we
demonstrate MANTA's superiority on temporal reasoning tasks (23.8% improvement)
and cross-modal understanding (25.1% improvement). Our framework introduces
novel density estimation techniques for redundancy minimization while
preserving rare signals, establishing new foundations for unifying multimodal
representations through structured text.

</details>


### [10] [An efficient plant disease detection using transfer learning approach](https://arxiv.org/abs/2507.00070)
*Bosubabu Sambana,Hillary Sunday Nnadi,Mohd Anas Wajid,Nwosu Ogochukwu Fidelia,Claudia Camacho-Zuñiga,Henry Dozie Ajuzie,Edeh Michael Onyema*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Plant diseases pose significant challenges to farmers and the agricultural
sector at large. However, early detection of plant diseases is crucial to
mitigating their effects and preventing widespread damage, as outbreaks can
severely impact the productivity and quality of crops. With advancements in
technology, there are increasing opportunities for automating the monitoring
and detection of disease outbreaks in plants. This study proposed a system
designed to identify and monitor plant diseases using a transfer learning
approach. Specifically, the study utilizes YOLOv7 and YOLOv8, two
state-ofthe-art models in the field of object detection. By fine-tuning these
models on a dataset of plant leaf images, the system is able to accurately
detect the presence of Bacteria, Fungi and Viral diseases such as Powdery
Mildew, Angular Leaf Spot, Early blight and Tomato mosaic virus. The model's
performance was evaluated using several metrics, including mean Average
Precision (mAP), F1-score, Precision, and Recall, yielding values of 91.05,
89.40, 91.22, and 87.66, respectively. The result demonstrates the superior
effectiveness and efficiency of YOLOv8 compared to other object detection
methods, highlighting its potential for use in modern agricultural practices.
The approach provides a scalable, automated solution for early any plant
disease detection, contributing to enhanced crop yield, reduced reliance on
manual monitoring, and supporting sustainable agricultural practices.

</details>


### [11] [Diffusion-Based Image Augmentation for Semantic Segmentation in Outdoor Robotics](https://arxiv.org/abs/2507.00153)
*Peter Mortimer,Mirko Maehlisch*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The performance of leaning-based perception algorithms suffer when deployed
in out-of-distribution and underrepresented environments. Outdoor robots are
particularly susceptible to rapid changes in visual scene appearance due to
dynamic lighting, seasonality and weather effects that lead to scenes
underrepresented in the training data of the learning-based perception system.
In this conceptual paper, we focus on preparing our autonomous vehicle for
deployment in snow-filled environments. We propose a novel method for
diffusion-based image augmentation to more closely represent the deployment
environment in our training data. Diffusion-based image augmentations rely on
the public availability of vision foundation models learned on internet-scale
datasets. The diffusion-based image augmentations allow us to take control over
the semantic distribution of the ground surfaces in the training data and to
fine-tune our model for its deployment environment. We employ open vocabulary
semantic segmentation models to filter out augmentation candidates that contain
hallucinations. We believe that diffusion-based image augmentations can be
extended to many other environments apart from snow surfaces, like sandy
environments and volcanic terrains.

</details>


### [12] [FreeLong++: Training-Free Long Video Generation via Multi-band SpectralFusion](https://arxiv.org/abs/2507.00162)
*Yu Lu,Yi Yang*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Recent advances in video generation models have enabled high-quality short
video generation from text prompts. However, extending these models to longer
videos remains a significant challenge, primarily due to degraded temporal
consistency and visual fidelity. Our preliminary observations show that naively
applying short-video generation models to longer sequences leads to noticeable
quality degradation. Further analysis identifies a systematic trend where
high-frequency components become increasingly distorted as video length grows,
an issue we term high-frequency distortion. To address this, we propose
FreeLong, a training-free framework designed to balance the frequency
distribution of long video features during the denoising process. FreeLong
achieves this by blending global low-frequency features, which capture holistic
semantics across the full video, with local high-frequency features extracted
from short temporal windows to preserve fine details. Building on this,
FreeLong++ extends FreeLong dual-branch design into a multi-branch architecture
with multiple attention branches, each operating at a distinct temporal scale.
By arranging multiple window sizes from global to local, FreeLong++ enables
multi-band frequency fusion from low to high frequencies, ensuring both
semantic continuity and fine-grained motion dynamics across longer video
sequences. Without any additional training, FreeLong++ can be plugged into
existing video generation models (e.g. Wan2.1 and LTX-Video) to produce longer
videos with substantially improved temporal consistency and visual fidelity. We
demonstrate that our approach outperforms previous methods on longer video
generation tasks (e.g. 4x and 8x of native length). It also supports coherent
multi-prompt video generation with smooth scene transitions and enables
controllable video generation using long depth or pose sequences.

</details>


### [13] [SelvaBox: A high-resolution dataset for tropical tree crown detection](https://arxiv.org/abs/2507.00170)
*Hugo Baudchon,Arthur Ouaknine,Martin Weiss,Mélisande Teng,Thomas R. Walla,Antoine Caron-Guay,Christopher Pal,Etienne Laliberté*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Detecting individual tree crowns in tropical forests is essential to study
these complex and crucial ecosystems impacted by human interventions and
climate change. However, tropical crowns vary widely in size, structure, and
pattern and are largely overlapping and intertwined, requiring advanced remote
sensing methods applied to high-resolution imagery. Despite growing interest in
tropical tree crown detection, annotated datasets remain scarce, hindering
robust model development. We introduce SelvaBox, the largest open-access
dataset for tropical tree crown detection in high-resolution drone imagery. It
spans three countries and contains more than 83,000 manually labeled crowns -
an order of magnitude larger than all previous tropical forest datasets
combined. Extensive benchmarks on SelvaBox reveal two key findings: (1)
higher-resolution inputs consistently boost detection accuracy; and (2) models
trained exclusively on SelvaBox achieve competitive zero-shot detection
performance on unseen tropical tree crown datasets, matching or exceeding
competing methods. Furthermore, jointly training on SelvaBox and three other
datasets at resolutions from 3 to 10 cm per pixel within a unified
multi-resolution pipeline yields a detector ranking first or second across all
evaluated datasets. Our dataset, code, and pre-trained weights are made public.

</details>


### [14] [Graph-Based Deep Learning for Component Segmentation of Maize Plants](https://arxiv.org/abs/2507.00182)
*J. I. Ruíz,A. Méndez,E. Rodríguez*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In precision agriculture, one of the most important tasks when exploring crop
production is identifying individual plant components. There are several
attempts to accomplish this task by the use of traditional 2D imaging, 3D
reconstructions, and Convolutional Neural Networks (CNN). However, they have
several drawbacks when processing 3D data and identifying individual plant
components. Therefore, in this work, we propose a novel Deep Learning
architecture to detect components of individual plants on Light Detection and
Ranging (LiDAR) 3D Point Cloud (PC) data sets. This architecture is based on
the concept of Graph Neural Networks (GNN), and feature enhancing with
Principal Component Analysis (PCA). For this, each point is taken as a vertex
and by the use of a K-Nearest Neighbors (KNN) layer, the edges are established,
thus representing the 3D PC data set. Subsequently, Edge-Conv layers are used
to further increase the features of each point. Finally, Graph Attention
Networks (GAT) are applied to classify visible phenotypic components of the
plant, such as the leaf, stem, and soil. This study demonstrates that our
graph-based deep learning approach enhances segmentation accuracy for
identifying individual plant components, achieving percentages above 80% in the
IoU average, thus outperforming other existing models based on point clouds.

</details>


### [15] [Computer Vision for Objects used in Group Work: Challenges and Opportunities](https://arxiv.org/abs/2507.00224)
*Changsoo Jung,Sheikh Mannan,Jack Fitzgerald,Nathaniel Blanchard*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Interactive and spatially aware technologies are transforming educational
frameworks, particularly in K-12 settings where hands-on exploration fosters
deeper conceptual understanding. However, during collaborative tasks, existing
systems often lack the ability to accurately capture real-world interactions
between students and physical objects. This issue could be addressed with
automatic 6D pose estimation, i.e., estimation of an object's position and
orientation in 3D space from RGB images or videos. For collaborative groups
that interact with physical objects, 6D pose estimates allow AI systems to
relate objects and entities. As part of this work, we introduce FiboSB, a novel
and challenging 6D pose video dataset featuring groups of three participants
solving an interactive task featuring small hand-held cubes and a weight scale.
This setup poses unique challenges for 6D pose because groups are holistically
recorded from a distance in order to capture all participants -- this, coupled
with the small size of the cubes, makes 6D pose estimation inherently
non-trivial. We evaluated four state-of-the-art 6D pose estimation methods on
FiboSB, exposing the limitations of current algorithms on collaborative group
work. An error analysis of these methods reveals that the 6D pose methods'
object detection modules fail. We address this by fine-tuning YOLO11-x for
FiboSB, achieving an overall mAP_50 of 0.898. The dataset, benchmark results,
and analysis of YOLO11-x errors presented here lay the groundwork for
leveraging the estimation of 6D poses in difficult collaborative contexts.

</details>


### [16] [VOCAL: Visual Odometry via ContrAstive Learning](https://arxiv.org/abs/2507.00243)
*Chi-Yao Huang,Zeel Bhatt,Yezhou Yang*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Breakthroughs in visual odometry (VO) have fundamentally reshaped the
landscape of robotics, enabling ultra-precise camera state estimation that is
crucial for modern autonomous systems. Despite these advances, many
learning-based VO techniques rely on rigid geometric assumptions, which often
fall short in interpretability and lack a solid theoretical basis within fully
data-driven frameworks. To overcome these limitations, we introduce VOCAL
(Visual Odometry via ContrAstive Learning), a novel framework that reimagines
VO as a label ranking challenge. By integrating Bayesian inference with a
representation learning framework, VOCAL organizes visual features to mirror
camera states. The ranking mechanism compels similar camera states to converge
into consistent and spatially coherent representations within the latent space.
This strategic alignment not only bolsters the interpretability of the learned
features but also ensures compatibility with multimodal data sources. Extensive
evaluations on the KITTI dataset highlight VOCAL's enhanced interpretability
and flexibility, pushing VO toward more general and explainable spatial
intelligence.

</details>


### [17] [Developing Lightweight DNN Models With Limited Data For Real-Time Sign Language Recognition](https://arxiv.org/abs/2507.00248)
*Nikita Nikitin,Eugene Fomin*

Main category: cs.CV

TL;DR: A lightweight DNN framework for real-time sign language recognition with high accuracy and low latency on edge devices.


<details>
  <summary>Details</summary>
Motivation: To address challenges in sign language recognition such as data scarcity, high computational costs, and frame rate discrepancies.

Method: Encoding sign language parameters into vectorized inputs and using MediaPipe for landmark extraction with a lightweight DNN architecture.

Result: Achieved 92% accuracy in isolated sign recognition with less than 10ms latency on edge devices.

Conclusion: The framework is effective for real-time sign language recognition and has been successfully integrated into a web application.

Abstract: We present a novel framework for real-time sign language recognition using
lightweight DNNs trained on limited data. Our system addresses key challenges
in sign language recognition, including data scarcity, high computational
costs, and discrepancies in frame rates between training and inference
environments. By encoding sign language specific parameters, such as handshape,
palm orientation, movement, and location into vectorized inputs, and leveraging
MediaPipe for landmark extraction, we achieve highly separable input data
representations. Our DNN architecture, optimized for sub 10MB deployment,
enables accurate classification of 343 signs with less than 10ms latency on
edge devices. The data annotation platform 'slait data' facilitates structured
labeling and vector extraction. Our model achieved 92% accuracy in isolated
sign recognition and has been integrated into the 'slait ai' web application,
where it demonstrates stable inference.

</details>


### [18] [GazeTarget360: Towards Gaze Target Estimation in 360-Degree for Robot Perception](https://arxiv.org/abs/2507.00253)
*Zhuangzhuang Dai,Vincent Gbouna Zakka,Luis J. Manso,Chen Li*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Enabling robots to understand human gaze target is a crucial step to allow
capabilities in downstream tasks, for example, attention estimation and
movement anticipation in real-world human-robot interactions. Prior works have
addressed the in-frame target localization problem with data-driven approaches
by carefully removing out-of-frame samples. Vision-based gaze estimation
methods, such as OpenFace, do not effectively absorb background information in
images and cannot predict gaze target in situations where subjects look away
from the camera. In this work, we propose a system to address the problem of
360-degree gaze target estimation from an image in generalized visual scenes.
The system, named GazeTarget360, integrates conditional inference engines of an
eye-contact detector, a pre-trained vision encoder, and a multi-scale-fusion
decoder. Cross validation results show that GazeTarget360 can produce accurate
and reliable gaze target predictions in unseen scenarios. This makes a
first-of-its-kind system to predict gaze targets from realistic camera footage
which is highly efficient and deployable. Our source code is made publicly
available at: https://github.com/zdai257/DisengageNet.

</details>


### [19] [VirtualFencer: Generating Fencing Bouts based on Strategies Extracted from In-the-Wild Videos](https://arxiv.org/abs/2507.00261)
*Zhiyin Lin,Purvi Goel,Joy Yun,C. Karen Liu,Joao Pedro Araujo*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Fencing is a sport where athletes engage in diverse yet strategically logical
motions. While most motions fall into a few high-level actions (e.g. step,
lunge, parry), the execution can vary widely-fast vs. slow, large vs. small,
offensive vs. defensive. Moreover, a fencer's actions are informed by a
strategy that often comes in response to the opponent's behavior. This
combination of motion diversity with underlying two-player strategy motivates
the application of data-driven modeling to fencing. We present VirtualFencer, a
system capable of extracting 3D fencing motion and strategy from in-the-wild
video without supervision, and then using that extracted knowledge to generate
realistic fencing behavior. We demonstrate the versatile capabilities of our
system by having it (i) fence against itself (self-play), (ii) fence against a
real fencer's motion from online video, and (iii) fence interactively against a
professional fencer.

</details>


### [20] [Room Scene Discovery and Grouping in Unstructured Vacation Rental Image Collections](https://arxiv.org/abs/2507.00263)
*Vignesh Ram Nithin Kappagantula,Shayan Hassantabar*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The rapid growth of vacation rental (VR) platforms has led to an increasing
volume of property images, often uploaded without structured categorization.
This lack of organization poses significant challenges for travelers attempting
to understand the spatial layout of a property, particularly when multiple
rooms of the same type are present. To address this issue, we introduce an
effective approach for solving the room scene discovery and grouping problem,
as well as identifying bed types within each bedroom group. This grouping is
valuable for travelers to comprehend the spatial organization, layout, and the
sleeping configuration of the property. We propose a computationally efficient
machine learning pipeline characterized by low latency and the ability to
perform effectively with sample-efficient learning, making it well-suited for
real-time and data-scarce environments. The pipeline integrates a supervised
room-type detection model, a supervised overlap detection model to identify the
overlap similarity between two images, and a clustering algorithm to group the
images of the same space together using the similarity scores. Additionally,
the pipeline maps each bedroom group to the corresponding bed types specified
in the property's metadata, based on the visual content present in the group's
images using a Multi-modal Large Language Model (MLLM) model. We evaluate the
aforementioned models individually and also assess the pipeline in its
entirety, observing strong performance that significantly outperforms
established approaches such as contrastive learning and clustering with
pretrained embeddings.

</details>


### [21] [Self-Supervised Multiview Xray Matching](https://arxiv.org/abs/2507.00287)
*Mohamad Dabboussi,Malo Huard,Yann Gousseau,Pietro Gori*

Main category: cs.CV

TL;DR: This paper introduces a self-supervised method for creating a many-to-many correspondence matrix between synthetic X-ray views using DRR, improving multi-view fracture detection without manual annotations.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of establishing robust correspondences between different X-ray views for accurate clinical evaluation, without relying on manual annotations.

Method: The method involves generating synthetic X-ray views from unannotated CT volumes using DRR and using a transformer-based approach to predict correspondences across multiple views.

Result: The results show that using correspondences from synthetic X-ray views enhances multi-view fracture detection on real data, with improved performance in multi-view classification.

Conclusion: The conclusion is that the self-supervised pipeline effectively creates correspondences without manual annotation, providing a useful pretraining strategy for multi-view fracture detection in real clinical settings.

Abstract: Accurate interpretation of multi-view radiographs is crucial for diagnosing
fractures, muscular injuries, and other anomalies. While significant advances
have been made in AI-based analysis of single images, current methods often
struggle to establish robust correspondences between different X-ray views, an
essential capability for precise clinical evaluations. In this work, we present
a novel self-supervised pipeline that eliminates the need for manual annotation
by automatically generating a many-to-many correspondence matrix between
synthetic X-ray views. This is achieved using digitally reconstructed
radiographs (DRR), which are automatically derived from unannotated CT volumes.
Our approach incorporates a transformer-based training phase to accurately
predict correspondences across two or more X-ray views. Furthermore, we
demonstrate that learning correspondences among synthetic X-ray views can be
leveraged as a pretraining strategy to enhance automatic multi-view fracture
detection on real data. Extensive evaluations on both synthetic and real X-ray
datasets show that incorporating correspondences improves performance in
multi-view fracture classification.

</details>


### [22] [Reducing Variability of Multiple Instance Learning Methods for Digital Pathology](https://arxiv.org/abs/2507.00292)
*Ali Mammadov,Loïc Le Folgoc,Guillaume Hocquet,Pietro Gori*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Digital pathology has revolutionized the field by enabling the digitization
of tissue samples into whole slide images (WSIs). However, the high resolution
and large size of WSIs present significant challenges when it comes to applying
Deep Learning models. As a solution, WSIs are often divided into smaller
patches with a global label (\textit{i.e., diagnostic}) per slide, instead of a
(too) costly pixel-wise annotation. By treating each slide as a bag of patches,
Multiple Instance Learning (MIL) methods have emerged as a suitable solution
for WSI classification. A major drawback of MIL methods is their high
variability in performance across different runs, which can reach up to 10-15
AUC points on the test set, making it difficult to compare different MIL
methods reliably. This variability mainly comes from three factors: i) weight
initialization, ii) batch (shuffling) ordering, iii) and learning rate. To
address that, we introduce a Multi-Fidelity, Model Fusion strategy for MIL
methods. We first train multiple models for a few epochs and average the most
stable and promising ones based on validation scores. This approach can be
applied to any existing MIL model to reduce performance variability. It also
simplifies hyperparameter tuning and improves reproducibility while maintaining
computational efficiency. We extensively validate our approach on WSI
classification tasks using 2 different datasets, 3 initialization strategies
and 5 MIL methods, for a total of more than 2000 experiments.

</details>


### [23] [Beyond Low-Rank Tuning: Model Prior-Guided Rank Allocation for Effective Transfer in Low-Data and Large-Gap Regimes](https://arxiv.org/abs/2507.00327)
*Chuyan Zhang,Kefan Wang,Yun Gu*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Low-Rank Adaptation (LoRA) has proven effective in reducing computational
costs while maintaining performance comparable to fully fine-tuned foundation
models across various tasks. However, its fixed low-rank structure restricts
its adaptability in scenarios with substantial domain gaps, where higher ranks
are often required to capture domain-specific complexities. Current adaptive
LoRA methods attempt to overcome this limitation by dynamically expanding or
selectively allocating ranks, but these approaches frequently depend on
computationally intensive techniques such as iterative pruning, rank searches,
or additional regularization. To address these challenges, we introduce Stable
Rank-Guided Low-Rank Adaptation (SR-LoRA), a novel framework that utilizes the
stable rank of pre-trained weight matrices as a natural prior for layer-wise
rank allocation. By leveraging the stable rank, which reflects the intrinsic
dimensionality of the weights, SR-LoRA enables a principled and efficient
redistribution of ranks across layers, enhancing adaptability without incurring
additional search costs. Empirical evaluations on few-shot tasks with
significant domain gaps show that SR-LoRA consistently outperforms recent
adaptive LoRA variants, achieving a superior trade-off between performance and
efficiency. Our code is available at
https://github.com/EndoluminalSurgicalVision-IMR/SR-LoRA.

</details>


### [24] [MammoTracker: Mask-Guided Lesion Tracking in Temporal Mammograms](https://arxiv.org/abs/2507.00328)
*Xuan Liu,Yinhao Ren,Marc D. Ryser,Lars J. Grimm,Joseph Y. Lo*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Accurate lesion tracking in temporal mammograms is essential for monitoring
breast cancer progression and facilitating early diagnosis. However, automated
lesion correspondence across exams remains a challenges in computer-aided
diagnosis (CAD) systems, limiting their effectiveness. We propose MammoTracker,
a mask-guided lesion tracking framework that automates lesion localization
across consecutively exams. Our approach follows a coarse-to-fine strategy
incorporating three key modules: global search, local search, and score
refinement. To support large-scale training and evaluation, we introduce a new
dataset with curated prior-exam annotations for 730 mass and calcification
cases from the public EMBED mammogram dataset, yielding over 20000 lesion
pairs, making it the largest known resource for temporal lesion tracking in
mammograms. Experimental results demonstrate that MammoTracker achieves 0.455
average overlap and 0.509 accuracy, surpassing baseline models by 8%,
highlighting its potential to enhance CAD-based lesion progression analysis.
Our dataset will be available at
https://gitlab.oit.duke.edu/railabs/LoGroup/mammotracker.

</details>


### [25] [Populate-A-Scene: Affordance-Aware Human Video Generation](https://arxiv.org/abs/2507.00334)
*Mengyi Shan,Zecheng He,Haoyu Ma,Felix Juefei-Xu,Peizhao Zhang,Tingbo Hou,Ching-Yao Chuang*

Main category: cs.CV

TL;DR: This paper investigates whether a video generation model can act as an interactive world simulator by enabling it to predict human-environment interactions from a single scene image, without needing explicit conditions like bounding boxes or body poses.


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore the potential of text-to-video models in understanding and simulating human-environment interactions, which could lead to more natural and coherent video generation without relying on explicit human data.

Method: The method involves fine-tuning a video generation model to insert a person into a scene based on a prompt, ensuring coherent behavior, appearance, and scene harmonization. The approach uses cross-attention heatmaps to infer human affordance from a single image without labeled datasets.

Result: The study shows that pre-trained video models have inherent affordance perception, allowing them to generate videos with accurate human-environment interactions without explicit conditions.

Conclusion: The paper concludes that video generation models can be effectively repurposed as interactive world simulators by leveraging their pre-trained knowledge of affordance, opening new possibilities for video synthesis and interaction modeling.

Abstract: Can a video generation model be repurposed as an interactive world simulator?
We explore the affordance perception potential of text-to-video models by
teaching them to predict human-environment interaction. Given a scene image and
a prompt describing human actions, we fine-tune the model to insert a person
into the scene, while ensuring coherent behavior, appearance, harmonization,
and scene affordance. Unlike prior work, we infer human affordance for video
generation (i.e., where to insert a person and how they should behave) from a
single scene image, without explicit conditions like bounding boxes or body
poses. An in-depth study of cross-attention heatmaps demonstrates that we can
uncover the inherent affordance perception of a pre-trained video model without
labeled affordance datasets.

</details>


### [26] [Training for X-Ray Vision: Amodal Segmentation, Amodal Content Completion, and View-Invariant Object Representation from Multi-Camera Video](https://arxiv.org/abs/2507.00339)
*Alexander Moore,Amar Saini,Kylie Cancilla,Doug Poland,Carmen Carrano*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Amodal segmentation and amodal content completion require using object priors
to estimate occluded masks and features of objects in complex scenes. Until
now, no data has provided an additional dimension for object context: the
possibility of multiple cameras sharing a view of a scene. We introduce
MOVi-MC-AC: Multiple Object Video with Multi-Cameras and Amodal Content, the
largest amodal segmentation and first amodal content dataset to date. Cluttered
scenes of generic household objects are simulated in multi-camera video.
MOVi-MC-AC contributes to the growing literature of object detection, tracking,
and segmentation by including two new contributions to the deep learning for
computer vision world. Multiple Camera (MC) settings where objects can be
identified and tracked between various unique camera perspectives are rare in
both synthetic and real-world video. We introduce a new complexity to synthetic
video by providing consistent object ids for detections and segmentations
between both frames and multiple cameras each with unique features and motion
patterns on a single scene. Amodal Content (AC) is a reconstructive task in
which models predict the appearance of target objects through occlusions. In
the amodal segmentation literature, some datasets have been released with
amodal detection, tracking, and segmentation labels. While other methods rely
on slow cut-and-paste schemes to generate amodal content pseudo-labels, they do
not account for natural occlusions present in the modal masks. MOVi-MC-AC
provides labels for ~5.8 million object instances, setting a new maximum in the
amodal dataset literature, along with being the first to provide ground-truth
amodal content. The full dataset is available at
https://huggingface.co/datasets/Amar-S/MOVi-MC-AC ,

</details>


### [27] [CGEarthEye:A High-Resolution Remote Sensing Vision Foundation Model Based on the Jilin-1 Satellite Constellation](https://arxiv.org/abs/2507.00356)
*Zhiwei Yi,Xin Cheng,Jingyu Ma,Ruifei Zhu,Junwei Tian,Yuanxiu Zhou,Xinge Zhao,Hongzhe Li*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Deep learning methods have significantly advanced the development of
intelligent rinterpretation in remote sensing (RS), with foundational model
research based on large-scale pre-training paradigms rapidly reshaping various
domains of Earth Observation (EO). However, compared to the open accessibility
and high spatiotemporal coverage of medium-resolution data, the limited
acquisition channels for ultra-high-resolution optical RS imagery have
constrained the progress of high-resolution remote sensing vision foundation
models (RSVFM). As the world's largest sub-meter-level commercial RS satellite
constellation, the Jilin-1 constellation possesses abundant sub-meter-level
image resources. This study proposes CGEarthEye, a RSVFM framework specifically
designed for Jilin-1 satellite characteristics, comprising five backbones with
different parameter scales with totaling 2.1 billion parameters. To enhance the
representational capacity of the foundation model, we developed JLSSD, the
first 15-million-scale multi-temporal self-supervised learning (SSL) dataset
featuring global coverage with quarterly temporal sampling within a single
year, constructed through multi-level representation clustering and sampling
strategies. The framework integrates seasonal contrast, augmentation-based
contrast, and masked patch token contrastive strategies for pre-training.
Comprehensive evaluations across 10 benchmark datasets covering four typical RS
tasks demonstrate that the CGEarthEye consistently achieves state-of-the-art
(SOTA) performance. Further analysis reveals CGEarthEye's superior
characteristics in feature visualization, model convergence, parameter
efficiency, and practical mapping applications. This study anticipates that the
exceptional representation capabilities of CGEarthEye will facilitate broader
and more efficient applications of Jilin-1 data in traditional EO application.

</details>


### [28] [GDGS: 3D Gaussian Splatting Via Geometry-Guided Initialization And Dynamic Density Control](https://arxiv.org/abs/2507.00363)
*Xingjun Wang,Lianlei Shan*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We propose a method to enhance 3D Gaussian Splatting (3DGS)~\cite{Kerbl2023},
addressing challenges in initialization, optimization, and density control.
Gaussian Splatting is an alternative for rendering realistic images while
supporting real-time performance, and it has gained popularity due to its
explicit 3D Gaussian representation. However, 3DGS heavily depends on accurate
initialization and faces difficulties in optimizing unstructured Gaussian
distributions into ordered surfaces, with limited adaptive density control
mechanism proposed so far. Our first key contribution is a geometry-guided
initialization to predict Gaussian parameters, ensuring precise placement and
faster convergence. We then introduce a surface-aligned optimization strategy
to refine Gaussian placement, improving geometric accuracy and aligning with
the surface normals of the scene. Finally, we present a dynamic adaptive
density control mechanism that adjusts Gaussian density based on regional
complexity, for visual fidelity. These innovations enable our method to achieve
high-fidelity real-time rendering and significant improvements in visual
quality, even in complex scenes. Our method demonstrates comparable or superior
results to state-of-the-art methods, rendering high-fidelity images in real
time.

</details>


### [29] [An Improved U-Net Model for Offline handwriting signature denoising](https://arxiv.org/abs/2507.00365)
*Wanghui Xiao*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Handwriting signatures, as an important means of identity recognition, are
widely used in multiple fields such as financial transactions, commercial
contracts and personal affairs due to their legal effect and uniqueness. In
forensic science appraisals, the analysis of offline handwriting signatures
requires the appraiser to provide a certain number of signature samples, which
are usually derived from various historical contracts or archival materials.
However, the provided handwriting samples are often mixed with a large amount
of interfering information, which brings severe challenges to handwriting
identification work. This study proposes a signature handwriting denoising
model based on the improved U-net structure, aiming to enhance the robustness
of the signature recognition system. By introducing discrete wavelet transform
and PCA transform, the model's ability to suppress noise has been enhanced. The
experimental results show that this modelis significantly superior to the
traditional methods in denoising effect, can effectively improve the clarity
and readability of the signed images, and provide more reliable technical
support for signature analysis and recognition.

</details>


### [30] [Out-of-Distribution Detection with Adaptive Top-K Logits Integration](https://arxiv.org/abs/2507.00368)
*Hikaru Shijo,Yutaka Yoshihama,Kenichi Yadani,Norifumi Murata*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Neural networks often make overconfident predictions from out-of-distribution
(OOD) samples. Detection of OOD data is therefore crucial to improve the safety
of machine learning. The simplest and most powerful method for OOD detection is
MaxLogit, which uses the model's maximum logit to provide an OOD score. We have
discovered that, in addition to the maximum logit, some other logits are also
useful for OOD detection. Based on this finding, we propose a new method called
ATLI (Adaptive Top-k Logits Integration), which adaptively determines effective
top-k logits that are specific to each model and combines the maximum logit
with the other top-k logits. In this study we evaluate our proposed method
using ImageNet-1K benchmark. Extensive experiments showed our proposed method
to reduce the false positive rate (FPR95) by 6.73% compared to the MaxLogit
approach, and decreased FPR95 by an additional 2.67% compared to other
state-of-the-art methods.

</details>


### [31] [PlantSegNeRF: A few-shot, cross-dataset method for plant 3D instance point cloud reconstruction via joint-channel NeRF with multi-view image instance matching](https://arxiv.org/abs/2507.00371)
*Xin Yang,Ruiming Du,Hanyang Huang,Jiayang Xie,Pengyao Xie,Leisen Fang,Ziyue Guo,Nanjun Jiang,Yu Jiang,Haiyan Cen*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Organ segmentation of plant point clouds is a prerequisite for the
high-resolution and accurate extraction of organ-level phenotypic traits.
Although the fast development of deep learning has boosted much research on
segmentation of plant point clouds, the existing techniques for organ
segmentation still face limitations in resolution, segmentation accuracy, and
generalizability across various plant species. In this study, we proposed a
novel approach called plant segmentation neural radiance fields (PlantSegNeRF),
aiming to directly generate high-precision instance point clouds from
multi-view RGB image sequences for a wide range of plant species. PlantSegNeRF
performed 2D instance segmentation on the multi-view images to generate
instance masks for each organ with a corresponding ID. The multi-view instance
IDs corresponding to the same plant organ were then matched and refined using a
specially designed instance matching module. The instance NeRF was developed to
render an implicit scene, containing color, density, semantic and instance
information. The implicit scene was ultimately converted into high-precision
plant instance point clouds based on the volume density. The results proved
that in semantic segmentation of point clouds, PlantSegNeRF outperformed the
commonly used methods, demonstrating an average improvement of 16.1%, 18.3%,
17.8%, and 24.2% in precision, recall, F1-score, and IoU compared to the
second-best results on structurally complex datasets. More importantly,
PlantSegNeRF exhibited significant advantages in plant point cloud instance
segmentation tasks. Across all plant datasets, it achieved average improvements
of 11.7%, 38.2%, 32.2% and 25.3% in mPrec, mRec, mCov, mWCov, respectively.
This study extends the organ-level plant phenotyping and provides a
high-throughput way to supply high-quality 3D data for the development of
large-scale models in plant science.

</details>


### [32] [Efficient Depth- and Spatially-Varying Image Simulation for Defocus Deblur](https://arxiv.org/abs/2507.00372)
*Xinge Yang,Chuong Nguyen,Wenbin Wang,Kaizhang Kang,Wolfgang Heidrich,Xiaoxing Li*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Modern cameras with large apertures often suffer from a shallow depth of
field, resulting in blurry images of objects outside the focal plane. This
limitation is particularly problematic for fixed-focus cameras, such as those
used in smart glasses, where adding autofocus mechanisms is challenging due to
form factor and power constraints. Due to unmatched optical aberrations and
defocus properties unique to each camera system, deep learning models trained
on existing open-source datasets often face domain gaps and do not perform well
in real-world settings. In this paper, we propose an efficient and scalable
dataset synthesis approach that does not rely on fine-tuning with real-world
data. Our method simultaneously models depth-dependent defocus and spatially
varying optical aberrations, addressing both computational complexity and the
scarcity of high-quality RGB-D datasets. Experimental results demonstrate that
a network trained on our low resolution synthetic images generalizes
effectively to high resolution (12MP) real-world images across diverse scenes.

</details>


### [33] [Customizable ROI-Based Deep Image Compression](https://arxiv.org/abs/2507.00373)
*Ian Jin,Fanxin Xia,Feng Ding,Xinfeng Zhang,Meiqin Liu,Yao Zhao,Weisi Lin,Lili Meng*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Region of Interest (ROI)-based image compression optimizes bit allocation by
prioritizing ROI for higher-quality reconstruction. However, as the users
(including human clients and downstream machine tasks) become more diverse,
ROI-based image compression needs to be customizable to support various
preferences. For example, different users may define distinct ROI or require
different quality trade-offs between ROI and non-ROI. Existing ROI-based image
compression schemes predefine the ROI, making it unchangeable, and lack
effective mechanisms to balance reconstruction quality between ROI and non-ROI.
This work proposes a paradigm for customizable ROI-based deep image
compression. First, we develop a Text-controlled Mask Acquisition (TMA) module,
which allows users to easily customize their ROI for compression by just
inputting the corresponding semantic \emph{text}. It makes the encoder
controlled by text. Second, we design a Customizable Value Assign (CVA)
mechanism, which masks the non-ROI with a changeable extent decided by users
instead of a constant one to manage the reconstruction quality trade-off
between ROI and non-ROI. Finally, we present a Latent Mask Attention (LMA)
module, where the latent spatial prior of the mask and the latent
Rate-Distortion Optimization (RDO) prior of the image are extracted and fused
in the latent space, and further used to optimize the latent representation of
the source image. Experimental results demonstrate that our proposed
customizable ROI-based deep image compression paradigm effectively addresses
the needs of customization for ROI definition and mask acquisition as well as
the reconstruction quality trade-off management between the ROI and non-ROI.

</details>


### [34] [MedDiff-FT: Data-Efficient Diffusion Model Fine-tuning with Structural Guidance for Controllable Medical Image Synthesis](https://arxiv.org/abs/2507.00377)
*Jianhao Xie,Ziang Zhang,Zhenyu Weng,Yuesheng Zhu,Guibo Luo*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Recent advancements in deep learning for medical image segmentation are often
limited by the scarcity of high-quality training data.While diffusion models
provide a potential solution by generating synthetic images, their
effectiveness in medical imaging remains constrained due to their reliance on
large-scale medical datasets and the need for higher image quality. To address
these challenges, we present MedDiff-FT, a controllable medical image
generation method that fine-tunes a diffusion foundation model to produce
medical images with structural dependency and domain specificity in a
data-efficient manner. During inference, a dynamic adaptive guiding mask
enforces spatial constraints to ensure anatomically coherent synthesis, while a
lightweight stochastic mask generator enhances diversity through hierarchical
randomness injection. Additionally, an automated quality assessment protocol
filters suboptimal outputs using feature-space metrics, followed by mask
corrosion to refine fidelity. Evaluated on five medical segmentation
datasets,MedDiff-FT's synthetic image-mask pairs improve SOTA method's
segmentation performance by an average of 1% in Dice score. The framework
effectively balances generation quality, diversity, and computational
efficiency, offering a practical solution for medical data augmentation. The
code is available at https://github.com/JianhaoXie1/MedDiff-FT.

</details>


### [35] [Learning Dense Feature Matching via Lifting Single 2D Image to 3D Space](https://arxiv.org/abs/2507.00392)
*Yingping Liang,Yutao Hu,Wenqi Shao,Ying Fu*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Feature matching plays a fundamental role in many computer vision tasks, yet
existing methods heavily rely on scarce and clean multi-view image collections,
which constrains their generalization to diverse and challenging scenarios.
Moreover, conventional feature encoders are typically trained on single-view 2D
images, limiting their capacity to capture 3D-aware correspondences. In this
paper, we propose a novel two-stage framework that lifts 2D images to 3D space,
named as \textbf{Lift to Match (L2M)}, taking full advantage of large-scale and
diverse single-view images. To be specific, in the first stage, we learn a
3D-aware feature encoder using a combination of multi-view image synthesis and
3D feature Gaussian representation, which injects 3D geometry knowledge into
the encoder. In the second stage, a novel-view rendering strategy, combined
with large-scale synthetic data generation from single-view images, is employed
to learn a feature decoder for robust feature matching, thus achieving
generalization across diverse domains. Extensive experiments demonstrate that
our method achieves superior generalization across zero-shot evaluation
benchmarks, highlighting the effectiveness of the proposed framework for robust
feature matching.

</details>


### [36] [Few-shot Classification as Multi-instance Verification: Effective Backbone-agnostic Transfer across Domains](https://arxiv.org/abs/2507.00401)
*Xin Xu,Eibe Frank,Geoffrey Holmes*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We investigate cross-domain few-shot learning under the constraint that
fine-tuning of backbones (i.e., feature extractors) is impossible or infeasible
-- a scenario that is increasingly common in practical use cases. Handling the
low-quality and static embeddings produced by frozen, "black-box" backbones
leads to a problem representation of few-shot classification as a series of
multiple instance verification (MIV) tasks. Inspired by this representation, we
introduce a novel approach to few-shot domain adaptation, named the "MIV-head",
akin to a classification head that is agnostic to any pretrained backbone and
computationally efficient. The core components designed for the MIV-head, when
trained on few-shot data from a target domain, collectively yield strong
performance on test data from that domain. Importantly, it does so without
fine-tuning the backbone, and within the "meta-testing" phase. Experimenting
under various settings and on an extension of the Meta-dataset benchmark for
cross-domain few-shot image classification, using representative off-the-shelf
convolutional neural network and vision transformer backbones pretrained on
ImageNet1K, we show that the MIV-head achieves highly competitive accuracy when
compared to state-of-the-art "adapter" (or partially fine-tuning) methods
applied to the same backbones, while incurring substantially lower adaptation
cost. We also find well-known "classification head" approaches lag far behind
in terms of accuracy. Ablation study empirically justifies the core components
of our approach. We share our code at https://github.com/xxweka/MIV-head.

</details>


### [37] [DiGA3D: Coarse-to-Fine Diffusional Propagation of Geometry and Appearance for Versatile 3D Inpainting](https://arxiv.org/abs/2507.00429)
*Jingyi Pan,Dan Xu,Qiong Luo*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Developing a unified pipeline that enables users to remove, re-texture, or
replace objects in a versatile manner is crucial for text-guided 3D inpainting.
However, there are still challenges in performing multiple 3D inpainting tasks
within a unified framework: 1) Single reference inpainting methods lack
robustness when dealing with views that are far from the reference view. 2)
Appearance inconsistency arises when independently inpainting multi-view images
with 2D diffusion priors; 3) Geometry inconsistency limits performance when
there are significant geometric changes in the inpainting regions. To tackle
these challenges, we introduce DiGA3D, a novel and versatile 3D inpainting
pipeline that leverages diffusion models to propagate consistent appearance and
geometry in a coarse-to-fine manner. First, DiGA3D develops a robust strategy
for selecting multiple reference views to reduce errors during propagation.
Next, DiGA3D designs an Attention Feature Propagation (AFP) mechanism that
propagates attention features from the selected reference views to other views
via diffusion models to maintain appearance consistency. Furthermore, DiGA3D
introduces a Texture-Geometry Score Distillation Sampling (TG-SDS) loss to
further improve the geometric consistency of inpainted 3D scenes. Extensive
experiments on multiple 3D inpainting tasks demonstrate the effectiveness of
our method. The project page is available at https://rorisis.github.io/DiGA3D/.

</details>


### [38] [MFH: Marrying Frequency Domain with Handwritten Mathematical Expression Recognition](https://arxiv.org/abs/2507.00430)
*Huanxin Yang,Qiwen Wang*

Main category: cs.CV

TL;DR: The paper proposes MFH, a method that uses frequency domain analysis with DCT to improve HMER, achieving high accuracy on CROHME datasets.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of complex formula structures and character layouts in HMER through frequency domain analysis.

Method: Incorporating DCT for frequency domain analysis to assist structural recognition in HMER.

Result: MFH-CoMER achieved 61.66%/62.07%/63.72% accuracy on CROHME 2014/2016/2019 test sets.

Conclusion: Frequency domain information effectively enhances HMER performance, with consistent improvements across baseline models.

Abstract: Handwritten mathematical expression recognition (HMER) suffers from complex
formula structures and character layouts in sequence prediction. In this paper,
we incorporate frequency domain analysis into HMER and propose a method that
marries frequency domain with HMER (MFH), leveraging the discrete cosine
transform (DCT). We emphasize the structural analysis assistance of frequency
information for recognizing mathematical formulas. When implemented on various
baseline models, our network exhibits a consistent performance enhancement,
demonstrating the efficacy of frequency domain information. Experiments show
that our MFH-CoMER achieves noteworthy accuracyrates of 61.66%/62.07%/63.72% on
the CROHME 2014/2016/2019 test sets. The source code is available at
https://github.com/Hryxyhe/MFH.

</details>


### [39] [Latent Posterior-Mean Rectified Flow for Higher-Fidelity Perceptual Face Restoration](https://arxiv.org/abs/2507.00447)
*Xin Luo,Menglin Zhang,Yunwei Lan,Tianyu Zhang,Rui Li,Chang Liu,Dong Liu*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The Perception-Distortion tradeoff (PD-tradeoff) theory suggests that face
restoration algorithms must balance perceptual quality and fidelity. To achieve
minimal distortion while maintaining perfect perceptual quality, Posterior-Mean
Rectified Flow (PMRF) proposes a flow based approach where source distribution
is minimum distortion estimations. Although PMRF is shown to be effective, its
pixel-space modeling approach limits its ability to align with human
perception, where human perception is defined as how humans distinguish between
two image distributions. In this work, we propose Latent-PMRF, which
reformulates PMRF in the latent space of a variational autoencoder (VAE),
facilitating better alignment with human perception during optimization. By
defining the source distribution on latent representations of minimum
distortion estimation, we bound the minimum distortion by the VAE's
reconstruction error. Moreover, we reveal the design of VAE is crucial, and our
proposed VAE significantly outperforms existing VAEs in both reconstruction and
restoration. Extensive experiments on blind face restoration demonstrate the
superiority of Latent-PMRF, offering an improved PD-tradeoff compared to
existing methods, along with remarkable convergence efficiency, achieving a
5.79X speedup over PMRF in terms of FID. Our code will be available as
open-source.

</details>


### [40] [ATSTrack: Enhancing Visual-Language Tracking by Aligning Temporal and Spatial Scales](https://arxiv.org/abs/2507.00454)
*Yihao Zhen,Qiang Wang,Yu Qiao,Liangqiong Qu,Huijie Fan*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: A main challenge of Visual-Language Tracking (VLT) is the misalignment
between visual inputs and language descriptions caused by target movement.
Previous trackers have explored many effective feature modification methods to
preserve more aligned features. However, an important yet unexplored factor
ultimately hinders their capability, which is the inherent differences in the
temporal and spatial scale of information between visual and language inputs.
To address this issue, we propose a novel visual-language tracker that enhances
the effect of feature modification by \textbf{A}ligning \textbf{T}emporal and
\textbf{S}patial scale of different input components, named as
\textbf{ATSTrack}. Specifically, we decompose each language description into
phrases with different attributes based on their temporal and spatial
correspondence with visual inputs, and modify their features in a fine-grained
manner. Moreover, we introduce a Visual-Language token that comprises modified
linguistic information from the previous frame to guide the model to extract
visual features that are more relevant to language description, thereby
reducing the impact caused by the differences in spatial scale. Experimental
results show that our proposed ATSTrack achieves performance comparable to
existing methods. Our code will be released.

</details>


### [41] [Unleashing the Potential of All Test Samples: Mean-Shift Guided Test-Time Adaptation](https://arxiv.org/abs/2507.00462)
*Jizhou Han,Chenhao Ding,SongLin Dong,Yuhang He,Xinyuan Gao,Yihong Gong*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Visual-language models (VLMs) like CLIP exhibit strong generalization but
struggle with distribution shifts at test time. Existing training-free
test-time adaptation (TTA) methods operate strictly within CLIP's original
feature space, relying on high-confidence samples while overlooking the
potential of low-confidence ones. We propose MS-TTA, a training-free approach
that enhances feature representations beyond CLIP's space using a single-step
k-nearest neighbors (kNN) Mean-Shift. By refining all test samples, MS-TTA
improves feature compactness and class separability, leading to more stable
adaptation. Additionally, a cache of refined embeddings further enhances
inference by providing Mean Shift enhanced logits. Extensive evaluations on OOD
and cross-dataset benchmarks demonstrate that MS-TTA consistently outperforms
state-of-the-art training-free TTA methods, achieving robust adaptation without
requiring additional training.

</details>


### [42] [Bisecle: Binding and Separation in Continual Learning for Video Language Understanding](https://arxiv.org/abs/2507.00469)
*Yue Tan,Xiaoqian Hu,Hao Xue,Celso De Melo,Flora D. Salim*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Frontier vision-language models (VLMs) have made remarkable improvements in
video understanding tasks. However, real-world videos typically exist as
continuously evolving data streams (e.g., dynamic scenes captured by wearable
glasses), necessitating models to continually adapt to shifting data
distributions and novel scenarios. Considering the prohibitive computational
costs of fine-tuning models on new tasks, usually, a small subset of parameters
is updated while the bulk of the model remains frozen. This poses new
challenges to existing continual learning frameworks in the context of large
multimodal foundation models, i.e., catastrophic forgetting and update
conflict. While the foundation models struggle with parameter-efficient
continual learning, the hippocampus in the human brain has evolved highly
efficient mechanisms for memory formation and consolidation. Inspired by the
rapid Binding and pattern separation mechanisms in the hippocampus, in this
work, we propose Bisecle for video-language continual learning, where a
multi-directional supervision module is used to capture more cross-modal
relationships and a contrastive prompt learning scheme is designed to isolate
task-specific knowledge to facilitate efficient memory storage. Binding and
separation processes further strengthen the ability of VLMs to retain complex
experiences, enabling robust and efficient continual learning in video
understanding tasks. We perform a thorough evaluation of the proposed Bisecle,
demonstrating its ability to mitigate forgetting and enhance cross-task
generalization on several VideoQA benchmarks.

</details>


### [43] [ARIG: Autoregressive Interactive Head Generation for Real-time Conversations](https://arxiv.org/abs/2507.00472)
*Ying Guo,Xi Liu,Cheng Zhen,Pengfei Yan,Xiaoming Wei*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Face-to-face communication, as a common human activity, motivates the
research on interactive head generation. A virtual agent can generate motion
responses with both listening and speaking capabilities based on the audio or
motion signals of the other user and itself. However, previous clip-wise
generation paradigm or explicit listener/speaker generator-switching methods
have limitations in future signal acquisition, contextual behavioral
understanding, and switching smoothness, making it challenging to be real-time
and realistic. In this paper, we propose an autoregressive (AR) based
frame-wise framework called ARIG to realize the real-time generation with
better interaction realism. To achieve real-time generation, we model motion
prediction as a non-vector-quantized AR process. Unlike discrete codebook-index
prediction, we represent motion distribution using diffusion procedure,
achieving more accurate predictions in continuous space. To improve interaction
realism, we emphasize interactive behavior understanding (IBU) and detailed
conversational state understanding (CSU). In IBU, based on dual-track
dual-modal signals, we summarize short-range behaviors through
bidirectional-integrated learning and perform contextual understanding over
long ranges. In CSU, we use voice activity signals and context features of IBU
to understand the various states (interruption, feedback, pause, etc.) that
exist in actual conversations. These serve as conditions for the final
progressive motion prediction. Extensive experiments have verified the
effectiveness of our model.

</details>


### [44] [ADAptation: Reconstruction-based Unsupervised Active Learning for Breast Ultrasound Diagnosis](https://arxiv.org/abs/2507.00474)
*Yaofei Duan,Yuhao Huang,Xin Yang,Luyi Han,Xinyu Xie,Zhiyuan Zhu,Ping He,Ka-Hou Chan,Ligang Cui,Sio-Kei Im,Dong Ni,Tao Tan*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Deep learning-based diagnostic models often suffer performance drops due to
distribution shifts between training (source) and test (target) domains.
Collecting and labeling sufficient target domain data for model retraining
represents an optimal solution, yet is limited by time and scarce resources.
Active learning (AL) offers an efficient approach to reduce annotation costs
while maintaining performance, but struggles to handle the challenge posed by
distribution variations across different datasets. In this study, we propose a
novel unsupervised Active learning framework for Domain Adaptation, named
ADAptation, which efficiently selects informative samples from multi-domain
data pools under limited annotation budget. As a fundamental step, our method
first utilizes the distribution homogenization capabilities of diffusion models
to bridge cross-dataset gaps by translating target images into source-domain
style. We then introduce two key innovations: (a) a hypersphere-constrained
contrastive learning network for compact feature clustering, and (b) a
dual-scoring mechanism that quantifies and balances sample uncertainty and
representativeness. Extensive experiments on four breast ultrasound datasets
(three public and one in-house/multi-center) across five common deep
classifiers demonstrate that our method surpasses existing strong AL-based
competitors, validating its effectiveness and generalization for clinical
domain adaptation. The code is available at the anonymized link:
https://github.com/miccai25-966/ADAptation.

</details>


### [45] [Just Noticeable Difference for Large Multimodal Models](https://arxiv.org/abs/2507.00490)
*Zijian Chen,Yuan Tian,Yuze Sun,Wei Sun,Zicheng Zhang,Weisi Lin,Guangtao Zhai,Wenjun Zhang*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Just noticeable difference (JND), the minimum change that the human visual
system (HVS) can perceive, has been studied for decades. Although recent work
has extended this line of research into machine vision, there has been a
scarcity of studies systematically exploring its perceptual boundaries across
multiple tasks and stimulus types, particularly in the current era of rapidly
advancing large multimodal models (LMMs), where studying the multifaceted
capabilities of models has become a mainstream focus. Moreover, the perceptual
defects of LMMs are not investigated thoroughly, resulting in potential
security issues and suboptimal response efficiency. In this paper, we take an
initial attempt and demonstrate that there exist significant visual blind spots
in current LMMs. To systemically quantify this characteristic, we propose a new
concept, {\bf LMM-JND}, together with its determination pipeline. Targeting
uncovering the behavior commonalities in HVS-aligned visual perception tasks,
we delve into several LMM families and construct a large-scale dataset, named
VPA-JND, which contains 21.5k reference images with over 489k stimuli across 12
distortion types, to facilitate LMM-JND studies. VPA-JND exposes areas where
state-of-the-art LMMs, including GPT-4o and the InternVL2.5 series, struggle
with basic comparison queries and fall significantly short of human-level
visual performance. We further explore the effects of vision and language
backbones and find a notable correlation between their design philosophy that
may instruct the future refinement of LMMs for their visual acuity. Together,
our research underscores the significance of LMM-JND as a unique perspective
for studying LMMs, and predictable LMM-JND is crucial for security concerns.
This work will be available at https://github.com/zijianchen98/LMM-JND.

</details>


### [46] [Visual Anagrams Reveal Hidden Differences in Holistic Shape Processing Across Vision Models](https://arxiv.org/abs/2507.00493)
*Fenil R. Doshi,Thomas Fel,Talia Konkle,George Alvarez*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Humans are able to recognize objects based on both local texture cues and the
configuration of object parts, yet contemporary vision models primarily harvest
local texture cues, yielding brittle, non-compositional features. Work on
shape-vs-texture bias has pitted shape and texture representations in
opposition, measuring shape relative to texture, ignoring the possibility that
models (and humans) can simultaneously rely on both types of cues, and
obscuring the absolute quality of both types of representation. We therefore
recast shape evaluation as a matter of absolute configural competence,
operationalized by the Configural Shape Score (CSS), which (i) measures the
ability to recognize both images in Object-Anagram pairs that preserve local
texture while permuting global part arrangement to depict different object
categories. Across 86 convolutional, transformer, and hybrid models, CSS (ii)
uncovers a broad spectrum of configural sensitivity with fully self-supervised
and language-aligned transformers -- exemplified by DINOv2, SigLIP2 and
EVA-CLIP -- occupying the top end of the CSS spectrum. Mechanistic probes
reveal that (iii) high-CSS networks depend on long-range interactions:
radius-controlled attention masks abolish performance showing a distinctive
U-shaped integration profile, and representational-similarity analyses expose a
mid-depth transition from local to global coding. A BagNet control remains at
chance (iv), ruling out "border-hacking" strategies. Finally, (v) we show that
configural shape score also predicts other shape-dependent evals. Overall, we
propose that the path toward truly robust, generalizable, and human-like vision
systems may not lie in forcing an artificial choice between shape and texture,
but rather in architectural and learning frameworks that seamlessly integrate
both local-texture and global configural shape.

</details>


### [47] [Laplace-Mamba: Laplace Frequency Prior-Guided Mamba-CNN Fusion Network for Image Dehazing](https://arxiv.org/abs/2507.00501)
*Yongzhen Wang,Liangliang Chen,Bingwen Hu,Heng Liu,Xiao-Ping Zhang,Mingqiang Wei*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Recent progress in image restoration has underscored Spatial State Models
(SSMs) as powerful tools for modeling long-range dependencies, owing to their
appealing linear complexity and computational efficiency. However, SSM-based
approaches exhibit limitations in reconstructing localized structures and tend
to be less effective when handling high-dimensional data, frequently resulting
in suboptimal recovery of fine image features. To tackle these challenges, we
introduce Laplace-Mamba, a novel framework that integrates Laplace frequency
prior with a hybrid Mamba-CNN architecture for efficient image dehazing.
Leveraging the Laplace decomposition, the image is disentangled into
low-frequency components capturing global texture and high-frequency components
representing edges and fine details. This decomposition enables specialized
processing via dual parallel pathways: the low-frequency branch employs SSMs
for global context modeling, while the high-frequency branch utilizes CNNs to
refine local structural details, effectively addressing diverse haze scenarios.
Notably, the Laplace transformation facilitates information-preserving
downsampling of low-frequency components in accordance with the Nyquist theory,
thereby significantly improving computational efficiency. Extensive evaluations
across multiple benchmarks demonstrate that our method outperforms
state-of-the-art approaches in both restoration quality and efficiency. The
source code and pretrained models are available at
https://github.com/yz-wang/Laplace-Mamba.

</details>


### [48] [ExPaMoE: An Expandable Parallel Mixture of Experts for Continual Test-Time Adaptation](https://arxiv.org/abs/2507.00502)
*JianChao Zhao,Songlin Dong*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Continual Test-Time Adaptation (CTTA) aims to enable models to adapt
on-the-fly to a stream of unlabeled data under evolving distribution shifts.
However, existing CTTA methods typically rely on shared model parameters across
all domains, making them vulnerable to feature entanglement and catastrophic
forgetting in the presence of large or non-stationary domain shifts. To address
this limitation, we propose \textbf{ExPaMoE}, a novel framework based on an
\emph{Expandable Parallel Mixture-of-Experts} architecture. ExPaMoE decouples
domain-general and domain-specific knowledge via a dual-branch expert design
with token-guided feature separation, and dynamically expands its expert pool
based on a \emph{Spectral-Aware Online Domain Discriminator} (SODD) that
detects distribution changes in real-time using frequency-domain cues.
Extensive experiments demonstrate the superiority of ExPaMoE across diverse
CTTA scenarios. We evaluate our method on standard benchmarks including
CIFAR-10C, CIFAR-100C, ImageNet-C, and Cityscapes-to-ACDC for semantic
segmentation. Additionally, we introduce \textbf{ImageNet++}, a large-scale and
realistic CTTA benchmark built from multiple ImageNet-derived datasets, to
better reflect long-term adaptation under complex domain evolution. ExPaMoE
consistently outperforms prior arts, showing strong robustness, scalability,
and resistance to forgetting.

</details>


### [49] [LLaVA-SP: Enhancing Visual Representation with Visual Spatial Tokens for MLLMs](https://arxiv.org/abs/2507.00505)
*Haoran Lou,Chunxiao Fan,Ziyan Liu,Yuexin Wu,Xinxiang Wang*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The architecture of multimodal large language models (MLLMs) commonly
connects a vision encoder, often based on CLIP-ViT, to a large language model.
While CLIP-ViT works well for capturing global image features, it struggles to
model local relationships between adjacent patches, leading to weaker visual
representation, which in turn affects the detailed understanding ability of
MLLMs. To solve this, we propose LLaVA-SP, which \textbf{ only adds six spatial
visual tokens} to the original visual tokens to enhance the visual
representation. Our approach offers three key advantages: 1)We propose a novel
Projector, which uses convolutional kernels to derive visual spatial tokens
from ViT patch features, simulating two visual spatial ordering approaches:
``from central region to global" and ``from abstract to specific". Then, a
cross-attention mechanism is applied to fuse fine-grained visual information,
enriching the overall visual representation. 2) We present two model variants:
LLaVA-SP-Cropping, which focuses on detail features through progressive
cropping, and LLaVA-SP-Pooling, which captures global semantics through
adaptive pooling, enabling the model to handle diverse visual understanding
tasks. 3) Extensive experiments show that LLaVA-SP, fine-tuned with LoRA,
achieves significant performance improvements across various multimodal
benchmarks, outperforming the state-of-the-art LLaVA-1.5 model in multiple
tasks with nearly identical inference latency. The code and models are
available at
\href{https://github.com/CnFaker/LLaVA-SP}{\texttt{https://github.com/CnFaker/LLaVA-SP}}.

</details>


### [50] [SCING:Towards More Efficient and Robust Person Re-Identification through Selective Cross-modal Prompt Tuning](https://arxiv.org/abs/2507.00506)
*Yunfei Xie,Yuxuan Cheng,Juncheng Wu,Haoyu Zhang,Yuyin Zhou,Shoudong Han*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Recent advancements in adapting vision-language pre-training models like CLIP
for person re-identification (ReID) tasks often rely on complex adapter design
or modality-specific tuning while neglecting cross-modal interaction, leading
to high computational costs or suboptimal alignment. To address these
limitations, we propose a simple yet effective framework named Selective
Cross-modal Prompt Tuning (SCING) that enhances cross-modal alignment and
robustness against real-world perturbations. Our method introduces two key
innovations: Firstly, we proposed Selective Visual Prompt Fusion (SVIP), a
lightweight module that dynamically injects discriminative visual features into
text prompts via a cross-modal gating mechanism. Moreover, the proposed
Perturbation-Driven Consistency Alignment (PDCA) is a dual-path training
strategy that enforces invariant feature alignment under random image
perturbations by regularizing consistency between original and augmented
cross-modal embeddings. Extensive experiments are conducted on several popular
benchmarks covering Market1501, DukeMTMC-ReID, Occluded-Duke, Occluded-REID,
and P-DukeMTMC, which demonstrate the impressive performance of the proposed
method. Notably, our framework eliminates heavy adapters while maintaining
efficient inference, achieving an optimal trade-off between performance and
computational overhead. The code will be released upon acceptance.

</details>


### [51] [Topology-Constrained Learning for Efficient Laparoscopic Liver Landmark Detection](https://arxiv.org/abs/2507.00519)
*Ruize Cui,Jiaan Zhang,Jialun Pei,Kai Wang,Pheng-Ann Heng,Jing Qin*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Liver landmarks provide crucial anatomical guidance to the surgeon during
laparoscopic liver surgery to minimize surgical risk. However, the tubular
structural properties of landmarks and dynamic intraoperative deformations pose
significant challenges for automatic landmark detection. In this study, we
introduce TopoNet, a novel topology-constrained learning framework for
laparoscopic liver landmark detection. Our framework adopts a snake-CNN
dual-path encoder to simultaneously capture detailed RGB texture information
and depth-informed topological structures. Meanwhile, we propose a
boundary-aware topology fusion (BTF) module, which adaptively merges RGB-D
features to enhance edge perception while preserving global topology.
Additionally, a topological constraint loss function is embedded, which
contains a center-line constraint loss and a topological persistence loss to
ensure homotopy equivalence between predictions and labels. Extensive
experiments on L3D and P2ILF datasets demonstrate that TopoNet achieves
outstanding accuracy and computational complexity, highlighting the potential
for clinical applications in laparoscopic liver surgery. Our code will be
available at https://github.com/cuiruize/TopoNet.

</details>


### [52] [Box-QAymo: Box-Referring VQA Dataset for Autonomous Driving](https://arxiv.org/abs/2507.00525)
*Djamahl Etchegaray,Yuxia Fu,Zi Huang,Yadan Luo*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Interpretable communication is essential for safe and trustworthy autonomous
driving, yet current vision-language models (VLMs) often operate under
idealized assumptions and struggle to capture user intent in real-world
scenarios. Existing driving-oriented VQA datasets are limited to full-scene
descriptions or waypoint prediction, preventing the assessment of whether VLMs
can respond to localized user-driven queries. We introduce Box-QAymo, a
box-referring dataset and benchmark designed to both evaluate and finetune VLMs
on spatial and temporal reasoning over user-specified objects. Users express
intent by drawing bounding boxes, offering a fast and intuitive interface for
focused queries in complex scenes. Specifically, we propose a hierarchical
evaluation protocol that begins with binary sanity-check questions to assess
basic model capacities, and progresses to (1) attribute prediction for
box-referred objects, (2) motion understanding of target instances, and (3)
spatiotemporal motion reasoning over inter-object dynamics across frames. To
support this, we crowd-sourced fine-grained object classes and visual
attributes that reflect the complexity drivers encounter, and extract object
trajectories to construct temporally grounded QA pairs. Rigorous quality
control through negative sampling, temporal consistency checks, and
difficulty-aware balancing guarantee dataset robustness and diversity. Our
comprehensive evaluation reveals significant limitations in current VLMs when
queried about perception questions, highlighting the gap in achieving
real-world performance. This work provides a foundation for developing more
robust and interpretable autonomous driving systems that can communicate
effectively with users under real-world conditions. Project page and dataset
are available at https://djamahl99.github.io/qaymo-pages/.

</details>


### [53] [Not All Attention Heads Are What You Need: Refining CLIP's Image Representation with Attention Ablation](https://arxiv.org/abs/2507.00537)
*Feng Lin,Marco Chen,Haokui Zhang,Xiaotian Yu,Guangming Lu,Rong Xiao*

Main category: cs.CV

TL;DR: This paper introduces the Attention Ablation Technique (AAT) to improve CLIP's performance by suppressing negative attention heads.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance CLIP's representations by identifying and removing detrimental attention heads.

Method: AAT manipulates attention weights to suppress specific heads, using two strategies for different scenarios.

Result: AAT improves downstream task performance, increasing recall by up to 11.1% in cross-modal retrieval.

Conclusion: AAT can refine vision-language models without extra inference cost, showing great potential.

Abstract: This paper studies the role of attention heads in CLIP's image encoder. While
CLIP has exhibited robust performance across diverse applications, we
hypothesize that certain attention heads negatively affect final
representations and that ablating them can improve performance in downstream
tasks. To capitalize on this insight, we propose a simple yet effective method,
called Attention Ablation Technique (AAT), to suppress the contribution of
specific heads by manipulating attention weights. By integrating two
alternative strategies tailored for different application scenarios, AAT
systematically identifies and ablates detrimental attention heads to enhance
representation quality. Experiments demonstrate that AAT consistently improves
downstream task performance across various domains, boosting recall rate by up
to 11.1% on CLIP-family models for cross-modal retrieval. The results highlight
the potential of AAT to effectively refine large-scale vision-language models
with virtually no increase in inference cost.

</details>


### [54] [LOD-GS: Level-of-Detail-Sensitive 3D Gaussian Splatting for Detail Conserved Anti-Aliasing](https://arxiv.org/abs/2507.00554)
*Zhenya Yang,Bingchen Gong,Kai Chen,Qi Dou*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Despite the advancements in quality and efficiency achieved by 3D Gaussian
Splatting (3DGS) in 3D scene rendering, aliasing artifacts remain a persistent
challenge. Existing approaches primarily rely on low-pass filtering to mitigate
aliasing. However, these methods are not sensitive to the sampling rate, often
resulting in under-filtering and over-smoothing renderings. To address this
limitation, we propose LOD-GS, a Level-of-Detail-sensitive filtering framework
for Gaussian Splatting, which dynamically predicts the optimal filtering
strength for each 3D Gaussian primitive. Specifically, we introduce a set of
basis functions to each Gaussian, which take the sampling rate as input to
model appearance variations, enabling sampling-rate-sensitive filtering. These
basis function parameters are jointly optimized with the 3D Gaussian in an
end-to-end manner. The sampling rate is influenced by both focal length and
camera distance. However, existing methods and datasets rely solely on
down-sampling to simulate focal length changes for anti-aliasing evaluation,
overlooking the impact of camera distance. To enable a more comprehensive
assessment, we introduce a new synthetic dataset featuring objects rendered at
varying camera distances. Extensive experiments on both public datasets and our
newly collected dataset demonstrate that our method achieves SOTA rendering
quality while effectively eliminating aliasing. The code and dataset have been
open-sourced.

</details>


### [55] [Zero-shot Skeleton-based Action Recognition with Prototype-guided Feature Alignment](https://arxiv.org/abs/2507.00566)
*Kai Zhou,Shuhai Zhang,Zeng You,Jinwu Hu,Mingkui Tan,Fei Liu*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Zero-shot skeleton-based action recognition aims to classify unseen
skeleton-based human actions without prior exposure to such categories during
training. This task is extremely challenging due to the difficulty in
generalizing from known to unknown actions. Previous studies typically use
two-stage training: pre-training skeleton encoders on seen action categories
using cross-entropy loss and then aligning pre-extracted skeleton and text
features, enabling knowledge transfer to unseen classes through skeleton-text
alignment and language models' generalization. However, their efficacy is
hindered by 1) insufficient discrimination for skeleton features, as the fixed
skeleton encoder fails to capture necessary alignment information for effective
skeleton-text alignment; 2) the neglect of alignment bias between skeleton and
unseen text features during testing. To this end, we propose a prototype-guided
feature alignment paradigm for zero-shot skeleton-based action recognition,
termed PGFA. Specifically, we develop an end-to-end cross-modal contrastive
training framework to improve skeleton-text alignment, ensuring sufficient
discrimination for skeleton features. Additionally, we introduce a
prototype-guided text feature alignment strategy to mitigate the adverse impact
of the distribution discrepancy during testing. We provide a theoretical
analysis to support our prototype-guided text feature alignment strategy and
empirically evaluate our overall PGFA on three well-known datasets. Compared
with the top competitor SMIE method, our PGFA achieves absolute accuracy
improvements of 22.96%, 12.53%, and 18.54% on the NTU-60, NTU-120, and PKU-MMD
datasets, respectively.

</details>


### [56] [Out-of-distribution detection in 3D applications: a review](https://arxiv.org/abs/2507.00570)
*Zizhao Li,Xueyang Kang,Joseph West,Kourosh Khoshelham*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The ability to detect objects that are not prevalent in the training set is a
critical capability in many 3D applications, including autonomous driving.
Machine learning methods for object recognition often assume that all object
categories encountered during inference belong to a closed set of classes
present in the training data. This assumption limits generalization to the real
world, as objects not seen during training may be misclassified or entirely
ignored. As part of reliable AI, OOD detection identifies inputs that deviate
significantly from the training distribution. This paper provides a
comprehensive overview of OOD detection within the broader scope of trustworthy
and uncertain AI. We begin with key use cases across diverse domains, introduce
benchmark datasets spanning multiple modalities, and discuss evaluation
metrics. Next, we present a comparative analysis of OOD detection
methodologies, exploring model structures, uncertainty indicators, and
distributional distance taxonomies, alongside uncertainty calibration
techniques. Finally, we highlight promising research directions, including
adversarially robust OOD detection and failure identification, particularly
relevant to 3D applications. The paper offers both theoretical and practical
insights into OOD detection, showcasing emerging research opportunities such as
3D vision integration. These insights help new researchers navigate the field
more effectively, contributing to the development of reliable, safe, and robust
AI systems.

</details>


### [57] [AI-Generated Video Detection via Perceptual Straightening](https://arxiv.org/abs/2507.00583)
*Christian Internò,Robert Geirhos,Markus Olhofer,Sunny Liu,Barbara Hammer,David Klindt*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The rapid advancement of generative AI enables highly realistic synthetic
videos, posing significant challenges for content authentication and raising
urgent concerns about misuse. Existing detection methods often struggle with
generalization and capturing subtle temporal inconsistencies. We propose
ReStraV(Representation Straightening Video), a novel approach to distinguish
natural from AI-generated videos. Inspired by the "perceptual straightening"
hypothesis -- which suggests real-world video trajectories become more straight
in neural representation domain -- we analyze deviations from this expected
geometric property. Using a pre-trained self-supervised vision transformer
(DINOv2), we quantify the temporal curvature and stepwise distance in the
model's representation domain. We aggregate statistics of these measures for
each video and train a classifier. Our analysis shows that AI-generated videos
exhibit significantly different curvature and distance patterns compared to
real videos. A lightweight classifier achieves state-of-the-art detection
performance (e.g., 97.17% accuracy and 98.63% AUROC on the VidProM benchmark),
substantially outperforming existing image- and video-based methods. ReStraV is
computationally efficient, it is offering a low-cost and effective detection
solution. This work provides new insights into using neural representation
geometry for AI-generated video detection.

</details>


### [58] [Similarity Memory Prior is All You Need for Medical Image Segmentation](https://arxiv.org/abs/2507.00585)
*Tang Hao,Guo ZhiQing,Wang LieJun,Liu Chao*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In recent years, it has been found that "grandmother cells" in the primary
visual cortex (V1) of macaques can directly recognize visual input with complex
shapes. This inspires us to examine the value of these cells in promoting the
research of medical image segmentation. In this paper, we design a Similarity
Memory Prior Network (Sim-MPNet) for medical image segmentation. Specifically,
we propose a Dynamic Memory Weights-Loss Attention (DMW-LA), which matches and
remembers the category features of specific lesions or organs in medical images
through the similarity memory prior in the prototype memory bank, thus helping
the network to learn subtle texture changes between categories. DMW-LA also
dynamically updates the similarity memory prior in reverse through Weight-Loss
Dynamic (W-LD) update strategy, effectively assisting the network directly
extract category features. In addition, we propose the Double-Similarity Global
Internal Enhancement Module (DS-GIM) to deeply explore the internal differences
in the feature distribution of input data through cosine similarity and
euclidean distance. Extensive experiments on four public datasets show that
Sim-MPNet has better segmentation performance than other state-of-the-art
methods. Our code is available on https://github.com/vpsg-research/Sim-MPNet.

</details>


### [59] [Context-Aware Academic Emotion Dataset and Benchmark](https://arxiv.org/abs/2507.00586)
*Luming Zhao,Jingwen Xuan,Jiamin Lou,Yonghui Yu,Wenwu Yang*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Academic emotion analysis plays a crucial role in evaluating students'
engagement and cognitive states during the learning process. This paper
addresses the challenge of automatically recognizing academic emotions through
facial expressions in real-world learning environments. While significant
progress has been made in facial expression recognition for basic emotions,
academic emotion recognition remains underexplored, largely due to the scarcity
of publicly available datasets. To bridge this gap, we introduce RAER, a novel
dataset comprising approximately 2,700 video clips collected from around 140
students in diverse, natural learning contexts such as classrooms, libraries,
laboratories, and dormitories, covering both classroom sessions and individual
study. Each clip was annotated independently by approximately ten annotators
using two distinct sets of academic emotion labels with varying granularity,
enhancing annotation consistency and reliability. To our knowledge, RAER is the
first dataset capturing diverse natural learning scenarios. Observing that
annotators naturally consider context cues-such as whether a student is looking
at a phone or reading a book-alongside facial expressions, we propose CLIP-CAER
(CLIP-based Context-aware Academic Emotion Recognition). Our method utilizes
learnable text prompts within the vision-language model CLIP to effectively
integrate facial expression and context cues from videos. Experimental results
demonstrate that CLIP-CAER substantially outperforms state-of-the-art
video-based facial expression recognition methods, which are primarily designed
for basic emotions, emphasizing the crucial role of context in accurately
recognizing academic emotions. Project page: https://zgsfer.github.io/CAER

</details>


### [60] [Overtake Detection in Trucks Using CAN Bus Signals: A Comparative Study of Machine Learning Methods](https://arxiv.org/abs/2507.00593)
*Fernando Alonso-Fernandez,Talha Hanif Butt,Prayag Tiwari*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Safe overtaking manoeuvres in trucks are vital for preventing accidents and
ensuring efficient traffic flow. Accurate prediction of such manoeuvres is
essential for Advanced Driver Assistance Systems (ADAS) to make timely and
informed decisions. In this study, we focus on overtake detection using
Controller Area Network (CAN) bus data collected from five in-service trucks
provided by the Volvo Group. We evaluate three common classifiers for vehicle
manoeuvre detection, Artificial Neural Networks (ANN), Random Forest (RF), and
Support Vector Machines (SVM), and analyse how different preprocessing
configurations affect performance. We find that variability in traffic
conditions strongly influences the signal patterns, particularly in the
no-overtake class, affecting classification performance if training data lacks
adequate diversity. Since the data were collected under unconstrained,
real-world conditions, class diversity cannot be guaranteed a priori. However,
training with data from multiple vehicles improves generalisation and reduces
condition-specific bias. Our pertruck analysis also reveals that classification
accuracy, especially for overtakes, depends on the amount of training data per
vehicle. To address this, we apply a score-level fusion strategy, which yields
the best per-truck performance across most cases. Overall, we achieve an
accuracy via fusion of TNR=93% (True Negative Rate) and TPR=86.5% (True
Positive Rate). This research has been part of the BIG FUN project, which
explores how Artificial Intelligence can be applied to logged vehicle data to
understand and predict driver behaviour, particularly in relation to Camera
Monitor Systems (CMS), being introduced as digital replacements for traditional
exterior mirrors.

</details>


### [61] [World4Drive: End-to-End Autonomous Driving via Intention-aware Physical Latent World Model](https://arxiv.org/abs/2507.00603)
*Yupeng Zheng,Pengxuan Yang,Zebin Xing,Qichao Zhang,Yuhang Zheng,Yinfeng Gao,Pengfei Li,Teng Zhang,Zhongpu Xia,Peng Jia,Dongbin Zhao*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: End-to-end autonomous driving directly generates planning trajectories from
raw sensor data, yet it typically relies on costly perception supervision to
extract scene information. A critical research challenge arises: constructing
an informative driving world model to enable perception annotation-free,
end-to-end planning via self-supervised learning. In this paper, we present
World4Drive, an end-to-end autonomous driving framework that employs vision
foundation models to build latent world models for generating and evaluating
multi-modal planning trajectories. Specifically, World4Drive first extracts
scene features, including driving intention and world latent representations
enriched with spatial-semantic priors provided by vision foundation models. It
then generates multi-modal planning trajectories based on current scene
features and driving intentions and predicts multiple intention-driven future
states within the latent space. Finally, it introduces a world model selector
module to evaluate and select the best trajectory. We achieve perception
annotation-free, end-to-end planning through self-supervised alignment between
actual future observations and predicted observations reconstructed from the
latent space. World4Drive achieves state-of-the-art performance without manual
perception annotations on both the open-loop nuScenes and closed-loop NavSim
benchmarks, demonstrating an 18.1\% relative reduction in L2 error, 46.7% lower
collision rate, and 3.75 faster training convergence. Codes will be accessed at
https://github.com/ucaszyp/World4Drive.

</details>


### [62] [De-Simplifying Pseudo Labels to Enhancing Domain Adaptive Object Detection](https://arxiv.org/abs/2507.00608)
*Zehua Fu,Chenguang Liu,Yuyu Chen,Jiaqi Zhou,Qingjie Liu,Yunhong Wang*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Despite its significant success, object detection in traffic and
transportation scenarios requires time-consuming and laborious efforts in
acquiring high-quality labeled data. Therefore, Unsupervised Domain Adaptation
(UDA) for object detection has recently gained increasing research attention.
UDA for object detection has been dominated by domain alignment methods, which
achieve top performance. Recently, self-labeling methods have gained popularity
due to their simplicity and efficiency. In this paper, we investigate the
limitations that prevent self-labeling detectors from achieving commensurate
performance with domain alignment methods. Specifically, we identify the high
proportion of simple samples during training, i.e., the simple-label bias, as
the central cause. We propose a novel approach called De-Simplifying Pseudo
Labels (DeSimPL) to mitigate the issue. DeSimPL utilizes an instance-level
memory bank to implement an innovative pseudo label updating strategy. Then,
adversarial samples are introduced during training to enhance the proportion.
Furthermore, we propose an adaptive weighted loss to avoid the model suffering
from an abundance of false positive pseudo labels in the late training period.
Experimental results demonstrate that DeSimPL effectively reduces the
proportion of simple samples during training, leading to a significant
performance improvement for self-labeling detectors. Extensive experiments
conducted on four benchmarks validate our analysis and conclusions.

</details>


### [63] [UMDATrack: Unified Multi-Domain Adaptive Tracking Under Adverse Weather Conditions](https://arxiv.org/abs/2507.00648)
*Siyuan Yao,Rui Zhu,Ziqi Wang,Wenqi Ren,Yanyang Yan,Xiaochun Cao*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Visual object tracking has gained promising progress in past decades. Most of
the existing approaches focus on learning target representation in
well-conditioned daytime data, while for the unconstrained real-world scenarios
with adverse weather conditions, e.g. nighttime or foggy environment, the
tremendous domain shift leads to significant performance degradation. In this
paper, we propose UMDATrack, which is capable of maintaining high-quality
target state prediction under various adverse weather conditions within a
unified domain adaptation framework. Specifically, we first use a controllable
scenario generator to synthesize a small amount of unlabeled videos (less than
2% frames in source daytime datasets) in multiple weather conditions under the
guidance of different text prompts. Afterwards, we design a simple yet
effective domain-customized adapter (DCA), allowing the target objects'
representation to rapidly adapt to various weather conditions without redundant
model updating. Furthermore, to enhance the localization consistency between
source and target domains, we propose a target-aware confidence alignment
module (TCA) following optimal transport theorem. Extensive experiments
demonstrate that UMDATrack can surpass existing advanced visual trackers and
lead new state-of-the-art performance by a significant margin. Our code is
available at https://github.com/Z-Z188/UMDATrack.

</details>


### [64] [LoD-Loc v2: Aerial Visual Localization over Low Level-of-Detail City Models using Explicit Silhouette Alignment](https://arxiv.org/abs/2507.00659)
*Juelin Zhu,Shuaibang Peng,Long Wang,Hanlin Tan,Yu Liu,Maojun Zhang,Shen Yan*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We propose a novel method for aerial visual localization over low
Level-of-Detail (LoD) city models. Previous wireframe-alignment-based method
LoD-Loc has shown promising localization results leveraging LoD models.
However, LoD-Loc mainly relies on high-LoD (LoD3 or LoD2) city models, but the
majority of available models and those many countries plan to construct
nationwide are low-LoD (LoD1). Consequently, enabling localization on low-LoD
city models could unlock drones' potential for global urban localization. To
address these issues, we introduce LoD-Loc v2, which employs a coarse-to-fine
strategy using explicit silhouette alignment to achieve accurate localization
over low-LoD city models in the air. Specifically, given a query image, LoD-Loc
v2 first applies a building segmentation network to shape building silhouettes.
Then, in the coarse pose selection stage, we construct a pose cost volume by
uniformly sampling pose hypotheses around a prior pose to represent the pose
probability distribution. Each cost of the volume measures the degree of
alignment between the projected and predicted silhouettes. We select the pose
with maximum value as the coarse pose. In the fine pose estimation stage, a
particle filtering method incorporating a multi-beam tracking approach is used
to efficiently explore the hypothesis space and obtain the final pose
estimation. To further facilitate research in this field, we release two
datasets with LoD1 city models covering 10.7 km , along with real RGB queries
and ground-truth pose annotations. Experimental results show that LoD-Loc v2
improves estimation accuracy with high-LoD models and enables localization with
low-LoD models for the first time. Moreover, it outperforms state-of-the-art
baselines by large margins, even surpassing texture-model-based methods, and
broadens the convergence basin to accommodate larger prior errors.

</details>


### [65] [A Unified Transformer-Based Framework with Pretraining For Whole Body Grasping Motion Generation](https://arxiv.org/abs/2507.00676)
*Edward Effendy,Kuan-Wei Tseng,Rei Kawakami*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Accepted in the ICIP 2025
  We present a novel transformer-based framework for whole-body grasping that
addresses both pose generation and motion infilling, enabling realistic and
stable object interactions. Our pipeline comprises three stages: Grasp Pose
Generation for full-body grasp generation, Temporal Infilling for smooth motion
continuity, and a LiftUp Transformer that refines downsampled joints back to
high-resolution markers. To overcome the scarcity of hand-object interaction
data, we introduce a data-efficient Generalized Pretraining stage on large,
diverse motion datasets, yielding robust spatio-temporal representations
transferable to grasping tasks. Experiments on the GRAB dataset show that our
method outperforms state-of-the-art baselines in terms of coherence, stability,
and visual realism. The modular design also supports easy adaptation to other
human-motion applications.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [66] [DiMo-GUI: Advancing Test-time Scaling in GUI Grounding via Modality-Aware Visual Reasoning](https://arxiv.org/abs/2507.00008)
*Hang Wu,Hongkai Chen,Yujun Cai,Chang Liu,Qingwen Ye,Ming-Hsuan Yang,Yiwei Wang*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Grounding natural language queries in graphical user interfaces (GUIs) poses
unique challenges due to the diversity of visual elements, spatial clutter, and
the ambiguity of language. In this paper, we introduce DiMo-GUI, a
training-free framework for GUI grounding that leverages two core strategies:
dynamic visual grounding and modality-aware optimization. Instead of treating
the GUI as a monolithic image, our method splits the input into textual
elements and iconic elements, allowing the model to reason over each modality
independently using general-purpose vision-language models. When predictions
are ambiguous or incorrect, DiMo-GUI dynamically focuses attention by
generating candidate focal regions centered on the model's initial predictions
and incrementally zooms into subregions to refine the grounding result. This
hierarchical refinement process helps disambiguate visually crowded layouts
without the need for additional training or annotations. We evaluate our
approach on standard GUI grounding benchmarks and demonstrate consistent
improvements over baseline inference pipelines, highlighting the effectiveness
of combining modality separation with region-focused reasoning.

</details>


### [67] [TalentMine: LLM-Based Extraction and Question-Answering from Multimodal Talent Tables](https://arxiv.org/abs/2507.00041)
*Varun Mannam,Fang Wang,Chaochun Liu,Xin Chen*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In talent management systems, critical information often resides in complex
tabular formats, presenting significant retrieval challenges for conventional
language models. These challenges are pronounced when processing Talent
documentation that requires precise interpretation of tabular relationships for
accurate information retrieval and downstream decision-making. Current table
extraction methods struggle with semantic understanding, resulting in poor
performance when integrated into retrieval-augmented chat applications. This
paper identifies a key bottleneck - while structural table information can be
extracted, the semantic relationships between tabular elements are lost,
causing downstream query failures. To address this, we introduce TalentMine, a
novel LLM-enhanced framework that transforms extracted tables into semantically
enriched representations. Unlike conventional approaches relying on CSV or text
linearization, our method employs specialized multimodal reasoning to preserve
both structural and semantic dimensions of tabular data. Experimental
evaluation across employee benefits document collections demonstrates
TalentMine's superior performance, achieving 100% accuracy in query answering
tasks compared to 0% for standard AWS Textract extraction and 40% for AWS
Textract Visual Q&A capabilities. Our comparative analysis also reveals that
the Claude v3 Haiku model achieves optimal performance for talent management
applications. The key contributions of this work include (1) a systematic
analysis of semantic information loss in current table extraction pipelines,
(2) a novel LLM-based method for semantically enriched table representation,
(3) an efficient integration framework for retrieval-augmented systems as
end-to-end systems, and (4) comprehensive benchmarks on talent analytics tasks
showing substantial improvements across multiple categories.

</details>


### [68] [A collaborative digital twin built on FAIR data and compute infrastructure](https://arxiv.org/abs/2507.00048)
*Thomas M. Deucher,Juan C. Verduzco,Michael Titus,Alejandro Strachan*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The integration of machine learning with automated experimentation in
self-driving laboratories (SDL) offers a powerful approach to accelerate
discovery and optimization tasks in science and engineering applications. When
supported by findable, accessible, interoperable, and reusable (FAIR) data
infrastructure, SDLs with overlapping interests can collaborate more
effectively. This work presents a distributed SDL implementation built on
nanoHUB services for online simulation and FAIR data management. In this
framework, geographically dispersed collaborators conducting independent
optimization tasks contribute raw experimental data to a shared central
database. These researchers can then benefit from analysis tools and machine
learning models that automatically update as additional data become available.
New data points are submitted through a simple web interface and automatically
processed using a nanoHUB Sim2L, which extracts derived quantities and indexes
all inputs and outputs in a FAIR data repository called ResultsDB. A separate
nanoHUB workflow enables sequential optimization using active learning, where
researchers define the optimization objective, and machine learning models are
trained on-the-fly with all existing data, guiding the selection of future
experiments. Inspired by the concept of ``frugal twin", the optimization task
seeks to find the optimal recipe to combine food dyes to achieve the desired
target color. With easily accessible and inexpensive materials, researchers and
students can set up their own experiments, share data with collaborators, and
explore the combination of FAIR data, predictive ML models, and sequential
optimization. The tools introduced are generally applicable and can easily be
extended to other optimization problems.

</details>


### [69] [SEZ-HARN: Self-Explainable Zero-shot Human Activity Recognition Network](https://arxiv.org/abs/2507.00050)
*Devin Y. De Silva,Sandareka Wickramanayake,Dulani Meedeniya,Sanka Rasnayaka*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Human Activity Recognition (HAR), which uses data from Inertial Measurement
Unit (IMU) sensors, has many practical applications in healthcare and assisted
living environments. However, its use in real-world scenarios has been limited
by the lack of comprehensive IMU-based HAR datasets that cover a wide range of
activities and the lack of transparency in existing HAR models. Zero-shot HAR
(ZS-HAR) overcomes the data limitations, but current models struggle to explain
their decisions, making them less transparent. This paper introduces a novel
IMU-based ZS-HAR model called the Self-Explainable Zero-shot Human Activity
Recognition Network (SEZ-HARN). It can recognize activities not encountered
during training and provide skeleton videos to explain its decision-making
process. We evaluate the effectiveness of the proposed SEZ-HARN on four
benchmark datasets PAMAP2, DaLiAc, HTD-MHAD and MHealth and compare its
performance against three state-of-the-art black-box ZS-HAR models. The
experiment results demonstrate that SEZ-HARN produces realistic and
understandable explanations while achieving competitive Zero-shot recognition
accuracy. SEZ-HARN achieves a Zero-shot prediction accuracy within 3\% of the
best-performing black-box model on PAMAP2 while maintaining comparable
performance on the other three datasets.

</details>


### [70] [Enhancing Reasoning Capabilities in SLMs with Reward Guided Dataset Distillation](https://arxiv.org/abs/2507.00054)
*Shreyansh Padarha*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The push to compress and impart the proficiency of Large Language Models
(LLMs) into more deployable and efficient Small Language Models (SLMs) has
benefited from improvements in knowledge distillation (KD) techniques. These
techniques allow a smaller student model to learn from a more capable and
larger teacher model's responses. However, distillation often revolves around
the student model merely copying the teacher's in-distribution responses,
limiting its generalisability. This limitation is amplified on reasoning tasks
and can be computationally expensive. In this study, we propose AdvDistill, a
reward-guided dataset distillation framework. We utilise multiple generations
(responses) from a teacher for each prompt and assign rewards based on
rule-based verifiers. These varying and normally distributed rewards serve as
weights when training student models. Our methods and their subsequent
behavioural analysis demonstrate a significant improvement in student model
performance for mathematical and complex reasoning tasks, showcasing the
efficacy and benefits of incorporating a rewarding mechanism in dataset
distillation processes.

</details>


### [71] [VoyagerVision: Investigating the Role of Multi-modal Information for Open-ended Learning Systems](https://arxiv.org/abs/2507.00079)
*Ethan Smyth,Alessandro Suglia*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Open-endedness is an active field of research in the pursuit of capable
Artificial General Intelligence (AGI), allowing models to pursue tasks of their
own choosing. Simultaneously, recent advancements in Large Language Models
(LLMs) such as GPT-4o [9] have allowed such models to be capable of
interpreting image inputs. Implementations such as OMNI-EPIC [4] have made use
of such features, providing an LLM with pixel data of an agent's POV to parse
the environment and allow it to solve tasks. This paper proposes that providing
these visual inputs to a model gives it greater ability to interpret spatial
environments, and as such, can increase the number of tasks it can successfully
perform, extending its open-ended potential. To this aim, this paper proposes
VoyagerVision -- a multi-modal model capable of creating structures within
Minecraft using screenshots as a form of visual feedback, building on the
foundation of Voyager. VoyagerVision was capable of creating an average of 2.75
unique structures within fifty iterations of the system, as Voyager was
incapable of this, it is an extension in an entirely new direction.
Additionally, in a set of building unit tests VoyagerVision was successful in
half of all attempts in flat worlds, with most failures arising in more complex
structures. Project website is available at
https://esmyth-dev.github.io/VoyagerVision.github.io/

</details>


### [72] [Thinking About Thinking: SAGE-nano's Inverse Reasoning for Self-Aware Language Models](https://arxiv.org/abs/2507.00092)
*Basab Jha,Firoj Paudel,Ujjwal Puri,Zhang Yuting,Choi Donghyuk,Wang Junhao*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities at
solving complex reasoning tasks with Chain-of-Thought (CoT) prompting, but
their decision-making processes remain somewhat blackbox. We introduce
textbfinverse reasoning, a novel paradigm enabling LLMs to decompose and
explain their own reasoning chains post-hoc. Our approach, used in SAGE-nano, a
4-billion-parameter reasoning model, employs a metacognitive structure that
reflects back via attention processes to identify major decision points and
generate explanations of reasoning choices. While typical CoT approaches are
directed towards forward reasoning generation, inverse reasoning provides
insight into why specific reasoning chains were selected over others. Through
thorough testing of logical reasoning puzzles, math problems and ethical
dilemmas from AQUA-RAT, CommonsenseQA, and customized benchmarks, we
demonstrate that SAGE-nano is at the cutting edge both on reasoning accuracy
(74.6% on AQUA-RAT) and explanation quality (92.1% human preference score) for
its task, and offers performance almost on par with models like Claude-3.5
Sonnet or GPT-4o. Our contributions are: (i) the first rigorous framework for
LLM self-reflection via inverse reasoning, (ii) a novel metalearning framework
to reverse the attention flow, (iii) comprehensive evaluation frameworks for
reasoning transparency, and (iv) evidence that increasing reasoning using
inverse reasoning improves interpretability along with reasoning performance.
Our work creates new avenues for transparent AI systems and closes significant
gaps in AI safety, education, and scientific discovery.

</details>


### [73] [BlackBoxToBlueprint: Extracting Interpretable Logic from Legacy Systems using Reinforcement Learning and Counterfactual Analysis](https://arxiv.org/abs/2507.00180)
*Vidhi Rathore*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Modernizing legacy software systems is a critical but challenging task, often
hampered by a lack of documentation and understanding of the original system's
intricate decision logic. Traditional approaches like behavioral cloning merely
replicate input-output behavior without capturing the underlying intent. This
paper proposes a novel pipeline to automatically extract interpretable decision
logic from legacy systems treated as black boxes. The approach uses a
Reinforcement Learning (RL) agent to explore the input space and identify
critical decision boundaries by rewarding actions that cause meaningful changes
in the system's output. These counterfactual state transitions, where the
output changes, are collected and clustered using K-Means. Decision trees are
then trained on these clusters to extract human-readable rules that approximate
the system's decision logic near the identified boundaries. I demonstrated the
pipeline's effectiveness on three dummy legacy systems with varying complexity,
including threshold-based, combined-conditional, and non-linear range logic.
Results show that the RL agent successfully focuses exploration on relevant
boundary regions, and the extracted rules accurately reflect the core logic of
the underlying dummy systems, providing a promising foundation for generating
specifications and test cases during legacy migration.

</details>


### [74] [ChatGPT produces more "lazy" thinkers: Evidence of cognitive engagement decline](https://arxiv.org/abs/2507.00181)
*Georgios P. Georgiou*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Despite the increasing use of large language models (LLMs) in education,
concerns have emerged about their potential to reduce deep thinking and active
learning. This study investigates the impact of generative artificial
intelligence (AI) tools, specifically ChatGPT, on the cognitive engagement of
students during academic writing tasks. The study employed an experimental
design with participants randomly assigned to either an AI-assisted (ChatGPT)
or a non-assisted (control) condition. Participants completed a structured
argumentative writing task followed by a cognitive engagement scale (CES), the
CES-AI, developed to assess mental effort, attention, deep processing, and
strategic thinking. The results revealed significantly lower cognitive
engagement scores in the ChatGPT group compared to the control group. These
findings suggest that AI assistance may lead to cognitive offloading. The study
contributes to the growing body of literature on the psychological implications
of AI in education and raises important questions about the integration of such
tools into academic practice. It calls for pedagogical strategies that promote
active, reflective engagement with AI-generated content to avoid compromising
self-regulated learning and deep cognitive involvement of students.

</details>


### [75] [Holistic Artificial Intelligence in Medicine; improved performance and explainability](https://arxiv.org/abs/2507.00205)
*Periklis Petridis,Georgios Margaritis,Vasiliki Stoumpou,Dimitris Bertsimas*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: With the increasing interest in deploying Artificial Intelligence in
medicine, we previously introduced HAIM (Holistic AI in Medicine), a framework
that fuses multimodal data to solve downstream clinical tasks. However, HAIM
uses data in a task-agnostic manner and lacks explainability. To address these
limitations, we introduce xHAIM (Explainable HAIM), a novel framework
leveraging Generative AI to enhance both prediction and explainability through
four structured steps: (1) automatically identifying task-relevant patient data
across modalities, (2) generating comprehensive patient summaries, (3) using
these summaries for improved predictive modeling, and (4) providing clinical
explanations by linking predictions to patient-specific medical knowledge.
Evaluated on the HAIM-MIMIC-MM dataset, xHAIM improves average AUC from 79.9%
to 90.3% across chest pathology and operative tasks. Importantly, xHAIM
transforms AI from a black-box predictor into an explainable decision support
system, enabling clinicians to interactively trace predictions back to relevant
patient data, bridging AI advancements with clinical utility.

</details>


### [76] [Learning for routing: A guided review of recent developments and future directions](https://arxiv.org/abs/2507.00218)
*Fangting Zhou,Attila Lischka,Balazs Kulcsar,Jiaming Wu,Morteza Haghir Chehreghani,Gilbert Laporte*

Main category: cs.AI

TL;DR: This paper reviews how machine learning is used to solve NP-hard routing problems like TSP and VRP, proposing a taxonomy of construction and improvement methods to combine traditional OR techniques with ML for future research.


<details>
  <summary>Details</summary>
Motivation: NP-hard routing problems like TSP and VRP are challenging due to their complexity, making exact algorithms slow and heuristics unreliable. ML offers potential to enhance solutions.

Method: The paper proposes a taxonomy categorizing ML-based routing methods into construction-based and improvement-based approaches, integrating traditional OR methods with ML techniques.', 

Result: The review provides a structured framework for combining traditional OR methods with ML, aiding future research and addressing new VRP variants.

Conclusion: ML techniques can effectively enhance solving NP-hard routing problems when integrated with traditional OR methods, suggesting a promising direction for future research.

Abstract: This paper reviews the current progress in applying machine learning (ML)
tools to solve NP-hard combinatorial optimization problems, with a focus on
routing problems such as the traveling salesman problem (TSP) and the vehicle
routing problem (VRP). Due to the inherent complexity of these problems, exact
algorithms often require excessive computational time to find optimal
solutions, while heuristics can only provide approximate solutions without
guaranteeing optimality. With the recent success of machine learning models,
there is a growing trend in proposing and implementing diverse ML techniques to
enhance the resolution of these challenging routing problems. We propose a
taxonomy categorizing ML-based routing methods into construction-based and
improvement-based approaches, highlighting their applicability to various
problem characteristics. This review aims to integrate traditional OR methods
with state-of-the-art ML techniques, providing a structured framework to guide
future research and address emerging VRP variants.

</details>


### [77] [ASTRO: Teaching Language Models to Reason by Reflecting and Backtracking In-Context](https://arxiv.org/abs/2507.00417)
*Joongwon Kim,Anirudh Goyal,Liang Tan,Hannaneh Hajishirzi,Srinivasan Iyer,Tianlu Wang*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We introduce ASTRO, the "Autoregressive Search-Taught Reasoner", a framework
for training language models to reason like search algorithms, explicitly
leveraging self-reflection, backtracking, and exploration in their outputs.
Recently, training large language models (LLMs) via reinforcement learning (RL)
has led to the advent of reasoning models with greatly enhanced reasoning
capabilities. Open-source replications of reasoning models, while successful,
build upon models that already exhibit strong reasoning capabilities along with
search behavior observed even before RL. As a result, it is yet unclear how to
boost the reasoning capabilities of other non-reasoner models including Llama
3. ASTRO teaches such models to internalize structured search behavior through
a synthetic dataset derived from Monte Carlo Tree Search (MCTS) over
mathematical problem-solving trajectories. By converting search traces into
natural language chain-of-thoughts that capture both successes and recoveries
from failure, ASTRO bootstraps models with a rich prior for exploration during
RL. We finetune our models on these search-derived traces and further improve
performance via RL with verifiable rewards. We apply ASTRO to the Llama 3
family of models and achieve absolute performance gains of 16.0% on MATH-500,
26.9% on AMC 2023, and 20.0% on AIME 2024, especially improving upon
challenging problems that require iterative correction. Our results demonstrate
that search-inspired training offers a principled way to instill robust
reasoning capabilities into open LLMs.

</details>


### [78] [Does Math Reasoning Improve General LLM Capabilities? Understanding Transferability of LLM Reasoning](https://arxiv.org/abs/2507.00432)
*Maggie Huan,Yuetai Li,Tuney Zheng,Xiaoyu Xu,Seungone Kim,Minxin Du,Radha Poovendran,Graham Neubig,Xiang Yue*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Math reasoning has become the poster child of progress in large language
models (LLMs), with new models rapidly surpassing human-level performance on
benchmarks like MATH and AIME. But as math leaderboards improve week by week,
it is worth asking: do these gains reflect broader problem-solving ability or
just narrow overfitting? To answer this question, we evaluate over 20
open-weight reasoning-tuned models across a broad suite of tasks, including
math, scientific QA, agent planning, coding, and standard
instruction-following. We surprisingly find that most models that succeed in
math fail to transfer their gains to other domains. To rigorously study this
phenomenon, we conduct controlled experiments on Qwen3-14B models using
math-only data but different tuning methods. We find that reinforcement
learning (RL)-tuned models generalize well across domains, while supervised
fine-tuning (SFT)-tuned models often forget general capabilities. Latent-space
representation and token-space distribution shift analyses reveal that SFT
induces substantial representation and output drift, while RL preserves
general-domain structure. Our results suggest a need to rethink standard
post-training recipes, particularly the reliance on SFT-distilled data for
advancing reasoning models.

</details>


### [79] [Advancing Local Search in SMT-NRA with MCSAT Integration](https://arxiv.org/abs/2507.00557)
*Tianyi Ding,Haokun Li,Xinpeng Ni,Bican Xia,Tianqi Zhao*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this paper, we advance local search for Satisfiability Modulo the Theory
of Nonlinear Real Arithmetic (SMT-NRA for short). First, we introduce a
two-dimensional cell-jump move, called \emph{$2d$-cell-jump}, generalizing the
key operation, cell-jump, of the local search method for SMT-NRA. Then, we
propose an extended local search framework, named \emph{$2d$-LS} (following the
local search framework, LS, for SMT-NRA), integrating the model constructing
satisfiability calculus (MCSAT) framework to improve search efficiency. To
further improve the efficiency of MCSAT, we implement a recently proposed
technique called \emph{sample-cell projection operator} for MCSAT, which is
well suited for CDCL-style search in the real domain and helps guide the search
away from conflicting states. Finally, we design a hybrid framework for SMT-NRA
combining MCSAT, $2d$-LS and OpenCAD, to improve search efficiency through
information exchange. The experimental results demonstrate improvements in
local search performance, highlighting the effectiveness of the proposed
methods.

</details>


### [80] [Can Large Language Models Develop Strategic Reasoning? Post-training Insights from Learning Chess](https://arxiv.org/abs/2507.00726)
*Dongyoon Hwang,Hojoon Lee,Jaegul Choo,Dongmin Park,Jongho Park*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: While reinforcement learning (RL) for large language models (LLMs) has shown
promise in mathematical reasoning, strategic reasoning for LLMs using RL
remains largely unexplored. We investigate whether LLMs can develop strategic
reasoning capabilities through RL in chess. To this end, we leverage a
chess-pretrained action-value network to provide dense reward on the LLM's
output move quality, which can be seen as a form of knowledge distillation. Our
experiments show that our distillation-based dense rewards often outperform
sparse binary rewards. However, surprisingly, all models plateau far below
expert levels. We provide SFT and RL ablations on chess reasoning training and
find evidence that this limitation stems from a deficit in the pretrained
models' internal understanding of chess--a deficit which RL alone may not be
able to fully overcome.

</details>


### [81] [A Robust Algorithm for Non-IID Machine Learning Problems with Convergence Analysis](https://arxiv.org/abs/2507.00810)
*Qing Xu,Xiaohua Xuan*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this paper, we propose an improved numerical algorithm for solving minimax
problems based on nonsmooth optimization, quadratic programming and iterative
process. We also provide a rigorous proof of convergence for our algorithm
under some mild assumptions, such as gradient continuity and boundedness. Such
an algorithm can be widely applied in various fields such as robust
optimization, imbalanced learning, etc.

</details>


### [82] [SafeMobile: Chain-level Jailbreak Detection and Automated Evaluation for Multimodal Mobile Agents](https://arxiv.org/abs/2507.00841)
*Siyuan Liang,Tianmeng Fang,Zhe Liu,Aishan Liu,Yan Xiao,Jinyuan He,Ee-Chien Chang,Xiaochun Cao*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: With the wide application of multimodal foundation models in intelligent
agent systems, scenarios such as mobile device control, intelligent assistant
interaction, and multimodal task execution are gradually relying on such large
model-driven agents. However, the related systems are also increasingly exposed
to potential jailbreak risks. Attackers may induce the agents to bypass the
original behavioral constraints through specific inputs, and then trigger
certain risky and sensitive operations, such as modifying settings, executing
unauthorized commands, or impersonating user identities, which brings new
challenges to system security. Existing security measures for intelligent
agents still have limitations when facing complex interactions, especially in
detecting potentially risky behaviors across multiple rounds of conversations
or sequences of tasks. In addition, an efficient and consistent automated
methodology to assist in assessing and determining the impact of such risks is
currently lacking. This work explores the security issues surrounding mobile
multimodal agents, attempts to construct a risk discrimination mechanism by
incorporating behavioral sequence information, and designs an automated
assisted assessment scheme based on a large language model. Through preliminary
validation in several representative high-risk tasks, the results show that the
method can improve the recognition of risky behaviors to some extent and assist
in reducing the probability of agents being jailbroken. We hope that this study
can provide some valuable references for the security risk modeling and
protection of multimodal intelligent agent systems.

</details>


### [83] [Thinking Beyond Tokens: From Brain-Inspired Intelligence to Cognitive Foundations for Artificial General Intelligence and its Societal Impact](https://arxiv.org/abs/2507.00951)
*Rizwan Qureshi,Ranjan Sapkota,Abbas Shah,Amgad Muneer,Anas Zafar,Ashmal Vayani,Maged Shoman,Abdelrahman B. M. Eldaly,Kai Zhang,Ferhat Sadak,Shaina Raza,Xinqi Fan,Ravid Shwartz-Ziv,Hong Yan,Vinjia Jain,Aman Chadha,Manoj Karkee,Jia Wu,Philip Torr,Seyedali Mirjalili*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Can machines truly think, reason and act in domains like humans? This
enduring question continues to shape the pursuit of Artificial General
Intelligence (AGI). Despite the growing capabilities of models such as GPT-4.5,
DeepSeek, Claude 3.5 Sonnet, Phi-4, and Grok 3, which exhibit multimodal
fluency and partial reasoning, these systems remain fundamentally limited by
their reliance on token-level prediction and lack of grounded agency. This
paper offers a cross-disciplinary synthesis of AGI development, spanning
artificial intelligence, cognitive neuroscience, psychology, generative models,
and agent-based systems. We analyze the architectural and cognitive foundations
of general intelligence, highlighting the role of modular reasoning, persistent
memory, and multi-agent coordination. In particular, we emphasize the rise of
Agentic RAG frameworks that combine retrieval, planning, and dynamic tool use
to enable more adaptive behavior. We discuss generalization strategies,
including information compression, test-time adaptation, and training-free
methods, as critical pathways toward flexible, domain-agnostic intelligence.
Vision-Language Models (VLMs) are reexamined not just as perception modules but
as evolving interfaces for embodied understanding and collaborative task
completion. We also argue that true intelligence arises not from scale alone
but from the integration of memory and reasoning: an orchestration of modular,
interactive, and self-improving components where compression enables adaptive
behavior. Drawing on advances in neurosymbolic systems, reinforcement learning,
and cognitive scaffolding, we explore how recent architectures begin to bridge
the gap between statistical learning and goal-directed cognition. Finally, we
identify key scientific, technical, and ethical challenges on the path to AGI.

</details>


### [84] [Enhancing LLM Agent Safety via Causal Influence Prompting](https://arxiv.org/abs/2507.00979)
*Dongyoon Hahm,Woogyeol Jin,June Suk Choi,Sungsoo Ahn,Kimin Lee*

Main category: cs.AI

TL;DR: This paper introduces CIP, a technique using causal influence diagrams (CIDs) to enhance the safety of autonomous agents powered by large language models (LLMs) in tasks like code execution and mobile device control.


<details>
  <summary>Details</summary>
Motivation: Ensuring safe and reliable behavior of autonomous agents powered by LLMs is crucial to prevent unintended consequences, especially as they are applied to assistive tasks.

Method: The method involves three steps: initializing a CID based on task specs, guiding agent interactions with the CID, and iteratively refining the CID based on observed behaviors and outcomes.

Result: The method effectively enhances safety in code execution and mobile device control tasks.

Conclusion: CIP, utilizing CIDs, is a promising approach for identifying and mitigating risks in agent decision-making, leading to safer outcomes.

Abstract: As autonomous agents powered by large language models (LLMs) continue to
demonstrate potential across various assistive tasks, ensuring their safe and
reliable behavior is crucial for preventing unintended consequences. In this
work, we introduce CIP, a novel technique that leverages causal influence
diagrams (CIDs) to identify and mitigate risks arising from agent
decision-making. CIDs provide a structured representation of cause-and-effect
relationships, enabling agents to anticipate harmful outcomes and make safer
decisions. Our approach consists of three key steps: (1) initializing a CID
based on task specifications to outline the decision-making process, (2)
guiding agent interactions with the environment using the CID, and (3)
iteratively refining the CID based on observed behaviors and outcomes.
Experimental results demonstrate that our method effectively enhances safety in
both code execution and mobile device control tasks.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [85] [Table Understanding and (Multimodal) LLMs: A Cross-Domain Case Study on Scientific vs. Non-Scientific Data](https://arxiv.org/abs/2507.00152)
*Ekaterina Borisova,Fabio Barth,Nils Feldhus,Raia Abu Ahmad,Malte Ostendorff,Pedro Ortiz Suarez,Georg Rehm,Sebastian Möller*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Tables are among the most widely used tools for representing structured data
in research, business, medicine, and education. Although LLMs demonstrate
strong performance in downstream tasks, their efficiency in processing tabular
data remains underexplored. In this paper, we investigate the effectiveness of
both text-based and multimodal LLMs on table understanding tasks through a
cross-domain and cross-modality evaluation. Specifically, we compare their
performance on tables from scientific vs. non-scientific contexts and examine
their robustness on tables represented as images vs. text. Additionally, we
conduct an interpretability analysis to measure context usage and input
relevance. We also introduce the TableEval benchmark, comprising 3017 tables
from scholarly publications, Wikipedia, and financial reports, where each table
is provided in five different formats: Image, Dictionary, HTML, XML, and LaTeX.
Our findings indicate that while LLMs maintain robustness across table
modalities, they face significant challenges when processing scientific tables.

</details>


### [86] [Prompting as Scientific Inquiry](https://arxiv.org/abs/2507.00163)
*Ari Holtzman,Chenhao Tan*

Main category: cs.CL

TL;DR: Prompting is essential for studying and controlling LLMs, and should be regarded as a science, not alchemy.


<details>
  <summary>Details</summary>
Motivation: The paper aims to reframe prompting as a scientific method for understanding LLMs, challenging the perception of it as mere alchemy.

Method: The paper uses argumentation and analogy to discuss the role of prompting in LLM research.

Result: The paper concludes that prompting is a vital part of LLM science, comparable to mechanistic interpretability.

Conclusion: Prompting should be treated as a legitimate scientific approach rather than a workaround.

Abstract: Prompting is the primary method by which we study and control large language
models. It is also one of the most powerful: nearly every major capability
attributed to LLMs-few-shot learning, chain-of-thought, constitutional AI-was
first unlocked through prompting. Yet prompting is rarely treated as science
and is frequently frowned upon as alchemy. We argue that this is a category
error. If we treat LLMs as a new kind of complex and opaque organism that is
trained rather than programmed, then prompting is not a workaround: it is
behavioral science. Mechanistic interpretability peers into the neural
substrate, prompting probes the model in its native interface: language. We
contend that prompting is not inferior, but rather a key component in the
science of LLMs.

</details>


### [87] [LineRetriever: Planning-Aware Observation Reduction for Web Agents](https://arxiv.org/abs/2507.00210)
*Imene Kerboua,Sahar Omidi Shayegan,Megh Thakkar,Xing Han Lù,Massimo Caccia,Véronique Eglin,Alexandre Aussem,Jérémy Espinas,Alexandre Lacoste*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: While large language models have demonstrated impressive capabilities in web
navigation tasks, the extensive context of web pages, often represented as DOM
or Accessibility Tree (AxTree) structures, frequently exceeds model context
limits. Current approaches like bottom-up truncation or embedding-based
retrieval lose critical information about page state and action history. This
is particularly problematic for adaptive planning in web agents, where
understanding the current state is essential for determining future actions. We
hypothesize that embedding models lack sufficient capacity to capture
plan-relevant information, especially when retrieving content that supports
future action prediction. This raises a fundamental question: how can retrieval
methods be optimized for adaptive planning in web navigation tasks? In
response, we introduce \textit{LineRetriever}, a novel approach that leverages
a language model to identify and retrieve observation lines most relevant to
future navigation steps. Unlike traditional retrieval methods that focus solely
on semantic similarity, \textit{LineRetriever} explicitly considers the
planning horizon, prioritizing elements that contribute to action prediction.
Our experiments demonstrate that \textit{LineRetriever} can reduce the size of
the observation at each step for the web agent while maintaining consistent
performance within the context limitations.

</details>


### [88] [Two-Stage Reasoning-Infused Learning: Improving Classification with LLM-Generated Reasoning](https://arxiv.org/abs/2507.00214)
*Mads Henrichsen,Rasmus Krebs*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Standard classification models often map inputs directly to labels without
explicit reasoning, potentially limiting their performance, robustness, and
interpretability. This paper introduces a novel two-stage approach to enhance
text classification by leveraging Large Language Model (LLM)-generated
reasonings. In the first stage, we fine-tune a Llama-3.2-1B-Instruct model
(henceforth Llama-R-Gen) on a general-purpose reasoning dataset
(syvai/reasoning-gen) to generate textual reasoning (R) given a question and
its answer. In the second stage, this generally trained Llama-R-Gen is used
offline to create an augmented training dataset for a downstream generative
model. This downstream model, based on Llama-3.2-1B-Instruct, takes only the
input text (Q) and is trained to output the generated reasoning (R) immediately
followed by the predicted emotion (A). We demonstrate this methodology on the
dair-ai/emotion dataset for emotion classification. Our experiments show that
the generative model trained to output reasoning and the emotion (Classifier
Q->RA) achieves a significant improvement of 8.7 percentage points in accuracy
(for emotion prediction) compared to a baseline generative model trained solely
to output the emotion (Classifier Q->A), highlighting the strong generalization
capabilities of the reasoning generation and the benefit of explicit reasoning
training. This work underscores the potential of LLM-generated reasonings for
creating richer training datasets, thereby improving the performance of diverse
downstream NLP tasks and providing explicit explanations.

</details>


### [89] [Towards Style Alignment in Cross-Cultural Translation](https://arxiv.org/abs/2507.00216)
*Shreya Havaldar,Adam Stein,Eric Wong,Lyle Ungar*

Main category: cs.CL

TL;DR: The paper discusses how LLMs struggle with translating style due to cultural differences, leading to loss of politeness, and introduces RASTA to improve stylistic alignment.


<details>
  <summary>Details</summary>
Motivation: To address the issue of LLMs failing to translate style accurately, especially in non-Western languages, and to align the speaker's intended style with the listener's interpretation.

Method: RASTA (Retrieval-Augmented STylistic Alignment) uses learned stylistic concepts to guide LLM translations towards appropriate cultural communication norms.

Result: RASTA mitigates the failure of LLMs in translating style by improving alignment with cultural norms and reducing bias towards neutrality.

Conclusion: The study concludes that RASTA is effective in enhancing stylistic alignment in LLM translations, particularly for non-Western languages.

Abstract: Successful communication depends on the speaker's intended style (i.e., what
the speaker is trying to convey) aligning with the listener's interpreted style
(i.e., what the listener perceives). However, cultural differences often lead
to misalignment between the two; for example, politeness is often lost in
translation. We characterize the ways that LLMs fail to translate style -
biasing translations towards neutrality and performing worse in non-Western
languages. We mitigate these failures with RASTA (Retrieval-Augmented STylistic
Alignment), a method that leverages learned stylistic concepts to encourage LLM
translation to appropriately convey cultural communication norms and align
style.

</details>


### [90] [Linearly Decoding Refused Knowledge in Aligned Language Models](https://arxiv.org/abs/2507.00239)
*Aryan Shrivastava,Ari Holtzman*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Most commonly used language models (LMs) are instruction-tuned and aligned
using a combination of fine-tuning and reinforcement learning, causing them to
refuse users requests deemed harmful by the model. However, jailbreak prompts
can often bypass these refusal mechanisms and elicit harmful responses. In this
work, we study the extent to which information accessed via jailbreak prompts
is decodable using linear probes trained on LM hidden states. We show that a
great deal of initially refused information is linearly decodable. For example,
across models, the response of a jailbroken LM for the average IQ of a country
can be predicted by a linear probe with Pearson correlations exceeding $0.8$.
Surprisingly, we find that probes trained on base models (which do not refuse)
sometimes transfer to their instruction-tuned versions and are capable of
revealing information that jailbreaks decode generatively, suggesting that the
internal representations of many refused properties persist from base LMs
through instruction-tuning. Importantly, we show that this information is not
merely "leftover" in instruction-tuned models, but is actively used by them: we
find that probe-predicted values correlate with LM generated pairwise
comparisons, indicating that the information decoded by our probes align with
suppressed generative behavior that may be expressed more subtly in other
downstream tasks. Overall, our results suggest that instruction-tuning does not
wholly eliminate or even relocate harmful information in representation
space-they merely suppress its direct expression, leaving it both linearly
accessible and indirectly influential in downstream behavior.

</details>


### [91] [The Algebraic Structure of Morphosyntax](https://arxiv.org/abs/2507.00244)
*Isabella Senturia,Matilde Marcolli*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Within the context of the mathematical formulation of Merge and the Strong
Minimalist Thesis, we present a mathematical model of the morphology-syntax
interface. In this setting, morphology has compositional properties responsible
for word formation, organized into a magma of morphological trees. However,
unlike syntax, we do not have movement within morphology. A coproduct
decomposition exists, but it requires extending the set of morphological trees
beyond those which are generated solely by the magma, to a larger set of
possible morphological inputs to syntactic trees. These participate in the
formation of morphosyntactic trees as an algebra over an operad, and a
correspondence between algebras over an operad. The process of structure
formation for morphosyntactic trees can then be described in terms of this
operadic correspondence that pairs syntactic and morphological data and the
morphology coproduct. We reinterpret in this setting certain operations of
Distributed Morphology as transformation that allow for flexibility in moving
the boundary between syntax and morphology within the morphosyntactic objects.

</details>


### [92] [EfficientXLang: Towards Improving Token Efficiency Through Cross-Lingual Reasoning](https://arxiv.org/abs/2507.00246)
*Sanchit Ahuja,Praneetha Vaddamanu,Barun Patra*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Despite recent advances in Language Reasoning Models (LRMs), most research
focuses solely on English, even though many models are pretrained on
multilingual data. In this work, we investigate: Is English the most
token-efficient language for reasoning? We evaluate three open-source RLMs:
DeepSeek R1, Qwen 2.5 and Qwen 3, across four math datasets and seven
typologically diverse languages. We find that reasoning in non-English
languages not only reduces token usage, but also preserves accuracy. These
gains persist even after translating the reasoning traces into English,
suggesting genuine shifts in reasoning behavior rather than surface-level
linguistic effects. The extent of improvement, however, depends on the models
multilingual strength. Our findings motivate a broader view of reasoning in
language models, highlighting the potential of multilingual reasoning and the
importance of strong multilingual foundations. The code for our work can be
found: https://github.com/microsoft/EfficientXLang.

</details>


### [93] [Impact of Fine-Tuning Methods on Memorization in Large Language Models](https://arxiv.org/abs/2507.00258)
*Jie Hou,Chuxiong Wu,Lannan Luo,Qiang Zeng*

Main category: cs.CL

TL;DR: Prompt-based fine-tuning is more privacy-preserving than parameter-based fine-tuning.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the lack of attention on privacy risks from memorization during fine-tuning of LLMs.

Method: Categorizing fine-tuning approaches and evaluating their impact on memorization using membership inference attacks.

Result: Prompt-based fine-tuning shows lower vulnerability to MIAs and maintains low memorization across model scales.

Conclusion: Parameter-based fine-tuning is more likely to leak private information, making prompt-based methods a better privacy-preserving choice.

Abstract: As the capabilities of pre-trained large language models (LLMs) continue to
advance, the "pre-train and fine-tune" paradigm has become increasingly
mainstream, leading to the development of various fine-tuning methods. However,
the privacy risks arising from memorization during fine-tuning have received
relatively little attention. To address this gap, we categorize popular
fine-tuning approaches and assess their impact on memorization through the lens
of membership inference attacks (MIAs). Our results show that, compared to
parameter-based fine-tuning, prompt-based fine-tuning achieves competitive
performance while exhibiting lower vulnerability to MIAs. Furthermore,
prompt-based methods maintain low memorization regardless of model scale. These
findings suggest that parameter-based fine-tuning is more prone to leaking
private information, whereas prompt-based fine-tuning serves as a more
privacy-preserving option.

</details>


### [94] [Natural language processing for African languages](https://arxiv.org/abs/2507.00297)
*David Ifeoluwa Adelani*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Recent advances in word embeddings and language models use large-scale,
unlabelled data and self-supervised learning to boost NLP performance.
Multilingual models, often trained on web-sourced data like Wikipedia, face
challenges: few low-resource languages are included, their data is often noisy,
and lack of labeled datasets makes it hard to evaluate performance outside
high-resource languages like English. In this dissertation, we focus on
languages spoken in Sub-Saharan Africa where all the indigenous languages in
this region can be regarded as low-resourced in terms of the availability of
labelled data for NLP tasks and unlabelled data found on the web. We analyse
the noise in the publicly available corpora, and curate a high-quality corpus,
demonstrating that the quality of semantic representations learned in word
embeddings does not only depend on the amount of data but on the quality of
pre-training data. We demonstrate empirically the limitations of word
embeddings, and the opportunities the multilingual pre-trained language model
(PLM) offers especially for languages unseen during pre-training and
low-resource scenarios. We further study how to adapt and specialize
multilingual PLMs to unseen African languages using a small amount of
monolingual texts. To address the under-representation of the African languages
in NLP research, we developed large scale human-annotated labelled datasets for
21 African languages in two impactful NLP tasks: named entity recognition and
machine translation. We conduct an extensive empirical evaluation using
state-of-the-art methods across supervised, weakly-supervised, and transfer
learning settings.

</details>


### [95] [Failure by Interference: Language Models Make Balanced Parentheses Errors When Faulty Mechanisms Overshadow Sound Ones](https://arxiv.org/abs/2507.00322)
*Daking Rai,Samuel Miller,Kevin Moran,Ziyu Yao*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Despite remarkable advances in coding capabilities, language models (LMs)
still struggle with simple syntactic tasks such as generating balanced
parentheses. In this study, we investigate the underlying mechanisms behind the
persistence of these errors across LMs of varying sizes (124M-7B) to both
understand and mitigate the errors. Our study reveals that LMs rely on a number
of components (attention heads and FF neurons) that independently make their
own predictions. While some components reliably promote correct answers across
a generalized range of inputs (i.e., implementing "sound mechanisms''), others
are less reliable and introduce noise by promoting incorrect tokens (i.e.,
implementing "faulty mechanisms''). Errors occur when the faulty mechanisms
overshadow the sound ones and dominantly affect the predictions. Motivated by
this insight, we introduce RASteer, a steering method to systematically
identify and increase the contribution of reliable components for improving
model performance. RASteer substantially improves performance on balanced
parentheses tasks, boosting accuracy of some models from $0$% to around $100$%
without impairing the models' general coding ability. We further demonstrate
its broader applicability in arithmetic reasoning tasks, achieving performance
gains of up to around $20$%.

</details>


### [96] [Modeling Data Diversity for Joint Instance and Verbalizer Selection in Cold-Start Scenarios](https://arxiv.org/abs/2507.00330)
*Mohna Chakraborty,Adithya Kulkarni,Qi Li*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Prompt-based methods leverage the knowledge of pre-trained language models
(PLMs) trained with a masked language modeling (MLM) objective; however, these
methods are sensitive to template, verbalizer, and few-shot instance selection,
particularly in cold-start settings with no labeled data. Existing studies
overlook the dependency between instances and verbalizers, where instance-label
probabilities depend on verbalizer token proximity in the embedding space. To
address this, we propose COLDSELECT, a joint verbalizer and instance selection
approach that models data diversity. COLDSELECT maps PLM vocabulary and
$h_{[MASK]}$ embeddings into a shared space, applying dimensionality reduction
and clustering to ensure efficient and diverse selection. By optimizing for
minimal uncertainty and maximal diversity, COLDSELECT captures data
relationships effectively. Experiments on eight benchmarks demonstrate
COLDSELECT's superiority in reducing uncertainty and enhancing generalization,
outperforming baselines in verbalizer and few-shot instance selection for
cold-start scenarios.

</details>


### [97] [Question Decomposition for Retrieval-Augmented Generation](https://arxiv.org/abs/2507.00355)
*Paul J. L. Ammann,Jonas Golde,Alan Akbik*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Grounding large language models (LLMs) in verifiable external sources is a
well-established strategy for generating reliable answers. Retrieval-augmented
generation (RAG) is one such approach, particularly effective for tasks like
question answering: it retrieves passages that are semantically related to the
question and then conditions the model on this evidence. However, multi-hop
questions, such as "Which company among NVIDIA, Apple, and Google made the
biggest profit in 2023?," challenge RAG because relevant facts are often
distributed across multiple documents rather than co-occurring in one source,
making it difficult for standard RAG to retrieve sufficient information. To
address this, we propose a RAG pipeline that incorporates question
decomposition: (i) an LLM decomposes the original query into sub-questions,
(ii) passages are retrieved for each sub-question, and (iii) the merged
candidate pool is reranked to improve the coverage and precision of the
retrieved evidence. We show that question decomposition effectively assembles
complementary documents, while reranking reduces noise and promotes the most
relevant passages before answer generation. Although reranking itself is
standard, we show that pairing an off-the-shelf cross-encoder reranker with
LLM-driven question decomposition bridges the retrieval gap on multi-hop
questions and provides a practical, drop-in enhancement, without any extra
training or specialized indexing. We evaluate our approach on the MultiHop-RAG
and HotpotQA, showing gains in retrieval (MRR@10: +36.7%) and answer accuracy
(F1: +11.6%) over standard RAG baselines.

</details>


### [98] [Gregorian melody, modality, and memory: Segmenting chant with Bayesian nonparametrics](https://arxiv.org/abs/2507.00380)
*Vojtěch Lanz,Jan Hajič jr*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The idea that Gregorian melodies are constructed from some vocabulary of
segments has long been a part of chant scholarship. This so-called
"centonisation" theory has received much musicological criticism, but frequent
re-use of certain melodic segments has been observed in chant melodies, and the
intractable number of possible segmentations allowed the option that some
undiscovered segmentation exists that will yet prove the value of
centonisation, and recent empirical results have shown that segmentations can
outperform music-theoretical features in mode classification. Inspired by the
fact that Gregorian chant was memorised, we search for an optimal unsupervised
segmentation of chant melody using nested hierarchical Pitman-Yor language
models. The segmentation we find achieves state-of-the-art performance in mode
classification. Modeling a monk memorising the melodies from one liturgical
manuscript, we then find empirical evidence for the link between mode
classification and memory efficiency, and observe more formulaic areas at the
beginnings and ends of melodies corresponding to the practical role of modality
in performance. However, the resulting segmentations themselves indicate that
even such a memory-optimal segmentation is not what is understood as
centonisation.

</details>


### [99] [Causal Prompting for Implicit Sentiment Analysis with Large Language Models](https://arxiv.org/abs/2507.00389)
*Jing Ren,Wenhao Zhou,Bowen Li,Mujie Liu,Nguyen Linh Dan Le,Jiade Cen,Liping Chen,Ziqi Xu,Xiwei Xu,Xiaodong Li*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Implicit Sentiment Analysis (ISA) aims to infer sentiment that is implied
rather than explicitly stated, requiring models to perform deeper reasoning
over subtle contextual cues. While recent prompting-based methods using Large
Language Models (LLMs) have shown promise in ISA, they often rely on majority
voting over chain-of-thought (CoT) reasoning paths without evaluating their
causal validity, making them susceptible to internal biases and spurious
correlations. To address this challenge, we propose CAPITAL, a causal prompting
framework that incorporates front-door adjustment into CoT reasoning. CAPITAL
decomposes the overall causal effect into two components: the influence of the
input prompt on the reasoning chains, and the impact of those chains on the
final output. These components are estimated using encoder-based clustering and
the NWGM approximation, with a contrastive learning objective used to better
align the encoder's representation with the LLM's reasoning space. Experiments
on benchmark ISA datasets with three LLMs demonstrate that CAPITAL consistently
outperforms strong prompting baselines in both accuracy and robustness,
particularly under adversarial conditions. This work offers a principled
approach to integrating causal inference into LLM prompting and highlights its
benefits for bias-aware sentiment reasoning. The source code and case study are
available at: https://github.com/whZ62/CAPITAL.

</details>


### [100] [Beyond Sociodemographic Prompting: Using Supervision to Align LLMs with Human Response Distributions](https://arxiv.org/abs/2507.00439)
*Gauri Kambhatla,Sanjana Gautam,Angela Zhang,Alex Liu,Ravi Srinivasan,Junyi Jessy Li,Matthew Lease*

Main category: cs.CL

TL;DR: This paper shows that simple supervision can enhance language model alignment with diverse population groups, using three datasets. It also highlights how alignment differs across groups and offers a benchmark for future research.


<details>
  <summary>Details</summary>
Motivation: Accurately predicting how different population groups answer subjective questions is valuable for improving language model alignment and understanding group-specific responses.

Method: The study uses simple supervision and evaluates alignment across multiple datasets and LLMs, testing various prompting strategies.

Result: Simple supervision significantly improves alignment with diverse groups, and alignment varies across specific populations as measured by the datasets.

Conclusion: The approach is simple and general, making it easy to adopt, and the findings offer practical guidance for using the method effectively.

Abstract: The ability to accurately predict how different population groups would
answer subjective questions would have great value. In this work, we show that
use of relatively simple supervision can greatly improve language model
alignment with diverse population groups, as measured over three datasets
spanning various topics. Beyond evaluating average performance, we also report
how alignment varies across specific groups. The simplicity and generality of
our approach promotes easy adoption, while our broad findings provide useful
guidance for when to use or not use our approach in practice. By conducting
evaluation over many LLMs and prompting strategies, along with open-sourcing
our work, we provide a useful benchmark to stimulate future research.

</details>


### [101] [Pitfalls of Evaluating Language Models with Open Benchmarks](https://arxiv.org/abs/2507.00460)
*Md. Najib Hasan,Mohammad Fakhruddin Babar,Souvika Sarkar,Monowar Hasan,Santu Karmaker*

Main category: cs.CL

TL;DR: This study shows that small models fine-tuned on public test sets can achieve high scores on open benchmarks like HELM, but these scores don't necessarily mean they're effective in real-world scenarios. The paper suggests that private or dynamic benchmarks are needed to maintain integrity and that current benchmarking practices should be reevaluated.


<details>
  <summary>Details</summary>
Motivation: The motivation is to highlight the potential flaws in open LLM benchmarks, which are widely used for fair comparison and reproducibility but may have critical issues that have not been well explored.

Method: The method involves creating 'cheating' models by fine-tuning smaller versions of BART, T5, and GPT-2 directly on public test sets to evaluate their performance on open benchmarks like HELM.

Result: The result shows that these smaller 'cheating' models achieve top rankings on HELM despite having poor generalization and limited practical utility.

Conclusion: The conclusion emphasizes that high leaderboard performance in open benchmarks may not reflect real-world effectiveness, suggesting the need for private/dynamic benchmarks and a reevaluation of current benchmarking practices to ensure reliable LM assessments.

Abstract: Open Large Language Model (LLM) benchmarks, such as HELM and BIG-bench, offer
standardized, transparent protocols that facilitate the fair comparison,
reproducibility, and iterative advancement of Language Models (LMs). However,
their openness also introduces critical and underexplored pitfalls. This study
exposes these weaknesses by systematically constructing ``cheating'' models --
smaller variants of BART, T5, and GPT-2 fine-tuned directly on public test sets
-- which achieve top rankings on a prominent open, holistic benchmark (HELM)
despite poor generalization and limited practical utility. Our findings
underscore three key insights: \ca high leaderboard performance on open
benchmarks may not always reflect real-world effectiveness; \cb private or
dynamic benchmarks must complement open evaluations to safeguard integrity; and
\cc a fundamental reevaluation of current benchmarking practices is essential
to ensure robust and trustworthy LM assessments.

</details>


### [102] [TeamCMU at Touché: Adversarial Co-Evolution for Advertisement Integration and Detection in Conversational Search](https://arxiv.org/abs/2507.00509)
*To Eun Kim,João Coelho,Gbemileke Onilude,Jai Singh*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: As conversational search engines increasingly adopt generation-based
paradigms powered by Large Language Models (LLMs) and Retrieval-Augmented
Generation (RAG), the integration of advertisements into generated responses
presents both commercial opportunities and challenges for user experience.
Unlike traditional search, where advertisements are clearly delineated,
generative systems blur the boundary between informational content and
promotional material, raising concerns around transparency and trust. In this
work, we propose a modular pipeline for advertisement management in RAG-based
conversational systems, consisting of an ad-rewriter for seamless ad
integration and a robust ad-classifier for detection. We leverage synthetic
data to train high-performing classifiers, which are then used to guide two
complementary ad-integration strategies: supervised fine-tuning of the
ad-rewriter and a best-of-N sampling approach that selects the least detectable
ad-integrated response among multiple candidates. Our evaluation focuses on two
core questions: the effectiveness of ad classifiers in detecting diverse ad
integration strategies, and the training methods that best support coherent,
minimally intrusive ad insertion. Experimental results show that our
ad-classifier, trained on synthetic advertisement data inspired by marketing
strategies and enhanced through curriculum learning, achieves robust detection
performance. Additionally, we demonstrate that classifier-guided optimization,
through both fine-tuning and best-of-N sampling, significantly improves ad
stealth, enabling more seamless integration. These findings contribute an
adversarial co-evolution framework for developing more sophisticated ad-aware
generative search systems and robust ad classifiers.

</details>


### [103] [NIRANTAR: Continual Learning with New Languages and Domains on Real-world Speech Data](https://arxiv.org/abs/2507.00534)
*Tahir Javed,Kaushal Bhogale,Mitesh M. Khapra*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We introduce Nirantar, a comprehensive framework for evaluating continual
learning (CL) in multilingual and multi-domain ASR. Designed to reflect
real-world CL challenges, Nirantar leverages data collected incrementally
across 22 languages and 208 districts in India through natural episodes. This
enables evaluation across Language-Incremental (LIL), Domain-Incremental (DIL),
and the novel Language-Incremental Domain-Incremental Learning (LIDIL)
scenarios. Unlike prior work that relies on simulated episodes, Nirantar
presents dynamic, non-uniform language and domain shifts, making it an ideal
testbed for CL research. With 3250 hours of human-transcribed speech, including
1720 hours newly introduced in this work, our framework enables systematic
benchmarking of CL methods. We evaluate existing approaches and demonstrate
that no single method performs consistently well, underscoring the need for
more robust CL strategies.

</details>


### [104] [Capsule Network-Based Semantic Intent Modeling for Human-Computer Interaction](https://arxiv.org/abs/2507.00540)
*Shixiao Wang,Yifan Zhuang,Runsheng Zhang,Zhijun Song*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper proposes a user semantic intent modeling algorithm based on
Capsule Networks to address the problem of insufficient accuracy in intent
recognition for human-computer interaction. The method represents semantic
features in input text through a vectorized capsule structure. It uses a
dynamic routing mechanism to transfer information across multiple capsule
layers. This helps capture hierarchical relationships and part-whole structures
between semantic entities more effectively. The model uses a convolutional
feature extraction module as the low-level encoder. After generating initial
semantic capsules, it forms high-level abstract intent representations through
an iterative routing process. To further enhance performance, a margin-based
mechanism is introduced into the loss function. This improves the model's
ability to distinguish between intent classes. Experiments are conducted using
a public natural language understanding dataset. Multiple mainstream models are
used for comparison. Results show that the proposed model outperforms
traditional methods and other deep learning structures in terms of accuracy,
F1-score, and intent detection rate. The study also analyzes the effect of the
number of dynamic routing iterations on model performance. A convergence curve
of the loss function during training is provided. These results verify the
stability and effectiveness of the proposed method in semantic modeling.
Overall, this study presents a new structured modeling approach to improve
intent recognition under complex semantic conditions.

</details>


### [105] [Methodological Rigour in Algorithm Application: An Illustration of Topic Modelling Algorithm](https://arxiv.org/abs/2507.00547)
*Malmi Amadoru*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The rise of advanced computational algorithms has opened new avenues for
computationally intensive research approaches to theory development. However,
the opacity of these algorithms and lack of transparency and rigour in their
application pose methodological challenges, potentially undermining trust in
research. The discourse on methodological rigour in this new genre of research
is still emerging. Against this backdrop, I attempt to offer guidance on
methodological rigour, particularly in the context of topic modelling
algorithms. By illustrating the application of the structural topic modelling
algorithm and presenting a set of guidelines, I discuss how to ensure rigour in
topic modelling studies. Although the guidelines are for the application of
topic modelling algorithms, they can be applied to other algorithms with
context-specific adjustments. The guidelines are helpful, especially for novice
researchers applying topic modelling, and editors and reviewers handling topic
modelling manuscripts. I contribute to the literature on topic modelling and
join the emerging dialogue on methodological rigour in computationally
intensive theory construction research.

</details>


### [106] [TUM-MiKaNi at SemEval-2025 Task 3: Towards Multilingual and Knowledge-Aware Non-factual Hallucination Identification](https://arxiv.org/abs/2507.00579)
*Miriam Anschütz,Ekaterina Gikalo,Niklas Herbster,Georg Groh*

Main category: cs.CL

TL;DR: This paper addresses multilingual hallucinations in LLMs by proposing a two-part pipeline combining retrieval-based fact verification with a BERT-based system, achieving competitive results across multiple languages.


<details>
  <summary>Details</summary>
Motivation: Hallucinations in LLMs hinder their trustworthiness and deployment, but most research focuses on English data, neglecting multilingual aspects.

Method: A two-part pipeline integrating retrieval-based fact verification against Wikipedia and a BERT-based system trained to detect common hallucination patterns.

Result: The system achieved top-10 results in eight languages, including English, and supports multiple additional languages beyond the shared task's scope.

Conclusion: The proposed multilingual hallucination identifier can enhance LLM outputs and their future applicability.

Abstract: Hallucinations are one of the major problems of LLMs, hindering their
trustworthiness and deployment to wider use cases. However, most of the
research on hallucinations focuses on English data, neglecting the multilingual
nature of LLMs. This paper describes our submission to the SemEval-2025 Task-3
- Mu-SHROOM, the Multilingual Shared-task on Hallucinations and Related
Observable Overgeneration Mistakes. We propose a two-part pipeline that
combines retrieval-based fact verification against Wikipedia with a BERT-based
system fine-tuned to identify common hallucination patterns. Our system
achieves competitive results across all languages, reaching top-10 results in
eight languages, including English. Moreover, it supports multiple languages
beyond the fourteen covered by the shared task. This multilingual hallucination
identifier can help to improve LLM outputs and their usefulness in the future.

</details>


### [107] [Transferable Modeling Strategies for Low-Resource LLM Tasks: A Prompt and Alignment-Based](https://arxiv.org/abs/2507.00601)
*Shuangquan Lyu,Yingnan Deng,Guiran Liu,Zhen Qi,Ruotong Wang*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper addresses the limited transfer and adaptation capabilities of
large language models in low-resource language scenarios. It proposes a unified
framework that combines a knowledge transfer module with parameter-efficient
fine-tuning strategies. The method introduces knowledge alignment loss and soft
prompt tuning to guide the model in effectively absorbing the structural
features of target languages or tasks under minimal annotation. This enhances
both generalization performance and training stability. The framework includes
lightweight adaptation modules to reduce computational costs. During training,
it integrates freezing strategies and prompt injection to preserve the model's
original knowledge while enabling quick adaptation to new tasks. The study also
conducts stability analysis experiments and synthetic pseudo-data transfer
experiments to systematically evaluate the method's applicability and
robustness across different low-resource tasks. Experimental results show that
compared with existing multilingual pre-trained models and mainstream transfer
methods, the proposed approach achieves higher performance and stability on
cross-lingual tasks such as MLQA, XQuAD, and PAWS-X. It demonstrates
particularly strong advantages under extremely data-scarce conditions. The
proposed method offers strong generality and scalability. It enhances
task-specific adaptability while preserving the general capabilities of large
language models. This makes it well-suited for complex semantic modeling and
multilingual processing tasks.

</details>


### [108] [Mixture of Reasonings: Teach Large Language Models to Reason with Adaptive Strategies](https://arxiv.org/abs/2507.00606)
*Tao Xiong,Xavier Hu,Wenyan Fan,Shengyu Zhang*

Main category: cs.CL

TL;DR: The paper proposes MoR, a training framework that enhances LLMs' ability to perform complex tasks with diverse reasoning strategies, improving performance without task-specific prompts.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the adaptability and efficiency of large language models by reducing their reliance on manually crafted, task-specific prompts for complex tasks.

Method: The method involves two phases: Thought Generation, where reasoning chain templates are created using models like GPT-4o, and SFT Dataset Construction, where these templates are paired with benchmark datasets for supervised fine-tuning.

Result: The results show that MoR significantly enhances performance, with MoR150 achieving 0.730 (a 2.2% improvement) using CoT prompting and 0.734 (a 13.5% improvement) compared to baselines.

Conclusion: The conclusion is that MoR eliminates the need for task-specific prompts, providing a generalizable solution for robust reasoning across diverse tasks.

Abstract: Large language models (LLMs) excel in complex tasks through advanced
prompting techniques like Chain-of-Thought (CoT) and Tree-of-Thought (ToT), but
their reliance on manually crafted, task-specific prompts limits adaptability
and efficiency. We introduce Mixture of Reasoning (MoR), a training framework
that embeds diverse reasoning strategies into LLMs for autonomous,
task-adaptive reasoning without external prompt engineering. MoR has two
phases: Thought Generation, creating reasoning chain templates with models like
GPT-4o, and SFT Dataset Construction, pairing templates with benchmark datasets
for supervised fine-tuning.Our experiments show that MoR significantly enhances
performance, with MoR150 achieving 0.730 (2.2% improvement) using CoT prompting
and 0.734 (13.5% improvement) compared to baselines. MoR eliminates the need
for task-specific prompts, offering a generalizable solution for robust
reasoning across diverse tasks.

</details>


### [109] [SAFER: Probing Safety in Reward Models with Sparse Autoencoder](https://arxiv.org/abs/2507.00665)
*Sihang Li,Wei Shi,Ziyuan Xie,Tao Liang,Guojun Ma,Xiang Wang*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Reinforcement learning from human feedback (RLHF) is a key paradigm for
aligning large language models (LLMs) with human values, yet the reward models
at its core remain largely opaque. In this work, we present sparse Autoencoder
For Enhanced Reward model (\textbf{SAFER}), a novel framework for interpreting
and improving reward models through mechanistic analysis. Leveraging Sparse
Autoencoders (SAEs), we uncover human-interpretable features in reward model
activations, enabling insight into safety-relevant decision-making. We apply
SAFER to safety-oriented preference datasets and quantify the salience of
individual features by activation differences between chosen and rejected
responses. Using these feature-level signals, we design targeted data poisoning
and denoising strategies. Experiments show that SAFER can precisely degrade or
enhance safety alignment with minimal data modification, without sacrificing
general chat performance. Our approach contributes to interpreting, auditing
and refining reward models in high-stakes LLM alignment tasks. Our codes are
available at https://github.com/xzy-101/SAFER-code. \textit{This paper
discusses topics related to large language model safety and may include
discussions or examples that highlight potential risks or unsafe outcomes.}

</details>


### [110] [Contrasting Cognitive Styles in Vision-Language Models: Holistic Attention in Japanese Versus Analytical Focus in English](https://arxiv.org/abs/2507.00700)
*Ahmed Sabir,Azinovič Gasper,Mengsay Loem,Rajesh Sharma*

Main category: cs.CL

TL;DR: This study explores if VLMs trained on different languages (Japanese vs. English) show cultural differences in processing visual info, similar to humans.


<details>
  <summary>Details</summary>
Motivation: Understanding how cultural backgrounds influence VLMs' perception and cognition, as humans do.

Method: Comparative analysis of image descriptions from VLMs trained on Japanese and English data.

Result: VLMs exhibit cultural attention patterns, reflecting holistic vs. analytic tendencies based on training languages.

Conclusion: Cultural cognition is embedded in VLMs through training data, affecting their outputs.

Abstract: Cross-cultural research in perception and cognition has shown that
individuals from different cultural backgrounds process visual information in
distinct ways. East Asians, for example, tend to adopt a holistic perspective,
attending to contextual relationships, whereas Westerners often employ an
analytical approach, focusing on individual objects and their attributes. In
this study, we investigate whether Vision-Language Models (VLMs) trained
predominantly on different languages, specifically Japanese and English,
exhibit similar culturally grounded attentional patterns. Using comparative
analysis of image descriptions, we examine whether these models reflect
differences in holistic versus analytic tendencies. Our findings suggest that
VLMs not only internalize the structural properties of language but also
reproduce cultural behaviors embedded in the training data, indicating that
cultural cognition may implicitly shape model outputs.

</details>


### [111] [AI Analyst: Framework and Comprehensive Evaluation of Large Language Models for Financial Time Series Report Generation](https://arxiv.org/abs/2507.00718)
*Elizabeth Fons,Elena Kochkina,Rachneet Kaur,Zhen Zeng,Berowne Hlavaty,Charese Smiley,Svitlana Vyetrenko,Manuela Veloso*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper explores the potential of large language models (LLMs) to generate
financial reports from time series data. We propose a framework encompassing
prompt engineering, model selection, and evaluation. We introduce an automated
highlighting system to categorize information within the generated reports,
differentiating between insights derived directly from time series data,
stemming from financial reasoning, and those reliant on external knowledge.
This approach aids in evaluating the factual grounding and reasoning
capabilities of the models. Our experiments, utilizing both data from the real
stock market indices and synthetic time series, demonstrate the capability of
LLMs to produce coherent and informative financial reports.

</details>


### [112] [LitBench: A Benchmark and Dataset for Reliable Evaluation of Creative Writing](https://arxiv.org/abs/2507.00769)
*Daniel Fein,Sebastian Russo,Violet Xiang,Kabir Jolly,Rafael Rafailov,Nick Haber*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Evaluating creative writing generated by large language models (LLMs) remains
challenging because open-ended narratives lack ground truths. Without
performant automated evaluation methods, off-the-shelf (OTS) language models
are employed as zero-shot judges, yet their reliability is unclear in this
context. In pursuit of robust evaluation for creative writing, we introduce
LitBench, the first standardized benchmark and paired dataset for creative
writing verification, comprising a held-out test set of 2,480 debiased,
human-labeled story comparisons drawn from Reddit and a 43,827-pair training
corpus of human preference labels. Using LitBench, we (i) benchmark zero-shot
LLM judges, (ii) train Bradley Terry and generative reward models, and (iii)
conduct an online human study to validate reward model rankings on newly
LLM-generated stories. Our benchmark identifies Claude-3.7-Sonnet as the
strongest off-the-shelf judge, reaching 73% agreement with human preferences;
among trained reward models, Bradley-Terry and Generative reward models both
attain an accuracy of 78%, outperforming all off-the-shelf judges. An online
human study further confirms that our trained reward models consistently align
with human preferences in novel LLM-generated stories. We release LitBench and
reward models at
https://huggingface.co/collections/SAA-Lab/litbench-68267b5da3aafe58f9e43461,
providing a vetted resource for reliable, automated evaluation and optimization
of creative writing systems.

</details>


### [113] [A Diagrammatic Calculus for a Functional Model of Natural Language Semantics](https://arxiv.org/abs/2507.00782)
*Matthieu Pierre Boyer*

Main category: cs.CL

TL;DR: The paper explores a functional programming approach to natural language semantics, enhancing expressivity beyond traditional denotational methods through a category-based type and effect system and a diagrammatic calculus for parsing and effect management.


<details>
  <summary>Details</summary>
Motivation: To increase the expressivity of traditional denotational semantics in natural language processing by leveraging functional programming techniques.

Method: Formalizing a category-based type and effect system and constructing a diagrammatic calculus for parsing and handling effects.

Result: Efficient computation of denotations for sentences using the diagrammatic calculus.

Conclusion: The functional programming approach with category-based systems and diagrammatic calculi offers a more expressive and efficient method for modeling natural language semantics.

Abstract: In this paper, we study a functional programming approach to natural language
semantics, allowing us to increase the expressivity of a more traditional
denotation style. We will formalize a category based type and effect system,
and construct a diagrammatic calculus to model parsing and handling of effects,
and use it to efficiently compute the denotations for sentences.

</details>


### [114] [Generative AI and the future of scientometrics: current topics and future questions](https://arxiv.org/abs/2507.00783)
*Benedetto Lepori,Jens Peter Andersen,Karsten Donnay*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The aim of this paper is to review the use of GenAI in scientometrics, and to
begin a debate on the broader implications for the field. First, we provide an
introduction on GenAI's generative and probabilistic nature as rooted in
distributional linguistics. And we relate this to the debate on the extent to
which GenAI might be able to mimic human 'reasoning'. Second, we leverage this
distinction for a critical engagement with recent experiments using GenAI in
scientometrics, including topic labelling, the analysis of citation contexts,
predictive applications, scholars' profiling, and research assessment. GenAI
shows promise in tasks where language generation dominates, such as labelling,
but faces limitations in tasks that require stable semantics, pragmatic
reasoning, or structured domain knowledge. However, these results might become
quickly outdated. Our recommendation is, therefore, to always strive to
systematically compare the performance of different GenAI models for specific
tasks. Third, we inquire whether, by generating large amounts of scientific
language, GenAI might have a fundamental impact on our field by affecting
textual characteristics used to measure science, such as authors, words, and
references. We argue that careful empirical work and theoretical reflection
will be essential to remain capable of interpreting the evolving patterns of
knowledge production.

</details>


### [115] [Many LLMs Are More Utilitarian Than One](https://arxiv.org/abs/2507.00814)
*Anita Keshmirian,Razan Baltaji,Babak Hemmatian,Hadi Asghari,Lav R. Varshney*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Moral judgment is integral to large language model (LLM) alignment and social
reasoning. As multi-agent systems gain prominence, it becomes crucial to
understand how LLMs function collectively during collaboration, compared to
individual agents. In human moral judgment, group deliberation leads to a
utilitarian boost: a tendency to endorse norm violations that maximize benefits
for the greatest number of people despite harms. We study whether a similar
dynamic emerges in multi-agent LLM systems. We tested six models on
well-established sets of moral dilemmas across two conditions: (1) Solo, where
models reasoned independently, and (2) Group, where they engaged in multi-turn
discussions in pairs or triads. In personal moral dilemmas, where agents must
decide to directly harm one individual to maximize the utility for others, all
models found moral violations to be more acceptable when part of a group than
individually, similar to human experiments. Some models endorsed actions that
maximized overall well-being, even if they benefited strangers over familiar
individuals. Others became more willing to violate moral norms in groups.
However, while human groups show a similar action bias, the mechanism for their
utilitarian boost differs from LLMs. Whereas the human shift comes from
heightened sensitivity to decision outcomes, LLM groups show either reduced
norm sensitivity or enhanced impartiality. This suggests that while the surface
behavior of LLM collectives mimics human group reasoning, the underlying
drivers differ. We discuss the implications for AI alignment, multi-agent
design, and artificial moral reasoning.

</details>


### [116] [ProxAnn: Use-Oriented Evaluations of Topic Models and Document Clustering](https://arxiv.org/abs/2507.00828)
*Alexander Hoyle,Lorena Calvo-Bartolomé,Jordan Boyd-Graber,Philip Resnik*

Main category: cs.CL

TL;DR: Paper introduces a scalable human evaluation protocol and LLM-based proxy for topic models and document clustering, showing LLMs can effectively replace human annotators in automated evaluations.


<details>
  <summary>Details</summary>
Motivation: Current evaluations for topic models and clustering either rely on automated metrics that don't match human preferences or expert labels that are hard to scale. The need for a practical, scalable evaluation method is critical for real-world applications.

Method: Design a human evaluation protocol where annotators or LLMs assign categories to text items in topics/clusters and apply them to other documents. Collect annotations from crowdworkers on two datasets and validate LLM proxies against human annotations.

Result: The best LLM-based proxies perform as well as human annotators in the evaluation protocol, demonstrating their effectiveness as substitutes for human labels in automated evaluations.

Conclusion: LLM-based proxies can reliably replace human annotators for evaluating topic models and clustering, offering a scalable alternative for practitioners.

Abstract: Topic model and document-clustering evaluations either use automated metrics
that align poorly with human preferences or require expert labels that are
intractable to scale. We design a scalable human evaluation protocol and a
corresponding automated approximation that reflect practitioners' real-world
usage of models. Annotators -- or an LLM-based proxy -- review text items
assigned to a topic or cluster, infer a category for the group, then apply that
category to other documents. Using this protocol, we collect extensive
crowdworker annotations of outputs from a diverse set of topic models on two
datasets. We then use these annotations to validate automated proxies, finding
that the best LLM proxies are statistically indistinguishable from a human
annotator and can therefore serve as a reasonable substitute in automated
evaluations. Package, web interface, and data are at
https://github.com/ahoho/proxann

</details>


### [117] [Stylometry recognizes human and LLM-generated texts in short samples](https://arxiv.org/abs/2507.00838)
*Karol Przystalski,Jan K. Argasiński,Iwona Grabska-Gradzińska,Jeremi K. Ochab*

Main category: cs.CL

TL;DR: The paper investigates using stylometry to detect LLM-generated texts vs. human writings, creating a benchmark dataset with various text types and employing tree-based models for classification, achieving high accuracy in binary and multiclass scenarios.


<details>
  <summary>Details</summary>
Motivation: To address model attribution, intellectual property, and ethical AI use by distinguishing between human and LLM-generated texts.

Method: Created a benchmark dataset from Wikipedia with human summaries, LLM-generated texts, and summarized/rewritten texts. Used tree-based models (decision trees, LightGBM) with stylometric features (StyloMetrix, n-gram-based pipeline) to classify texts.

Result: Achieved up to 0.87 Matthews correlation coefficient in multiclass classification (7 classes) and 0.79-1.0 accuracy in binary classification, with 0.98 accuracy for Wikipedia vs. GPT-4. Shapley explanations identified key features like encyclopedic style, overused words, and grammatical standardization in LLM texts.

Conclusion: Despite increasingly sophisticated LLMs, stylometry can effectively distinguish machine-generated from human texts for well-defined types like encyclopedic content.

Abstract: The paper explores stylometry as a method to distinguish between texts
created by Large Language Models (LLMs) and humans, addressing issues of model
attribution, intellectual property, and ethical AI use. Stylometry has been
used extensively to characterise the style and attribute authorship of texts.
By applying it to LLM-generated texts, we identify their emergent writing
patterns. The paper involves creating a benchmark dataset based on Wikipedia,
with (a) human-written term summaries, (b) texts generated purely by LLMs
(GPT-3.5/4, LLaMa 2/3, Orca, and Falcon), (c) processed through multiple text
summarisation methods (T5, BART, Gensim, and Sumy), and (d) rephrasing methods
(Dipper, T5). The 10-sentence long texts were classified by tree-based models
(decision trees and LightGBM) using human-designed (StyloMetrix) and
n-gram-based (our own pipeline) stylometric features that encode lexical,
grammatical, syntactic, and punctuation patterns. The cross-validated results
reached a performance of up to .87 Matthews correlation coefficient in the
multiclass scenario with 7 classes, and accuracy between .79 and 1. in binary
classification, with the particular example of Wikipedia and GPT-4 reaching up
to .98 accuracy on a balanced dataset. Shapley Additive Explanations pinpointed
features characteristic of the encyclopaedic text type, individual overused
words, as well as a greater grammatical standardisation of LLMs with respect to
human-written texts. These results show -- crucially, in the context of the
increasingly sophisticated LLMs -- that it is possible to distinguish machine-
from human-generated texts at least for a well-defined text type.

</details>


### [118] [TransLaw: Benchmarking Large Language Models in Multi-Agent Simulation of the Collaborative Translation](https://arxiv.org/abs/2507.00875)
*Xi Xuan,King-kui Sin,Yufei Zhou,Chunyu Kit*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Multi-agent systems empowered by large language models (LLMs) have
demonstrated remarkable capabilities in a wide range of downstream
applications, including machine translation. However, the potential of LLMs in
translating Hong Kong legal judgments remains uncertain due to challenges such
as intricate legal terminology, culturally embedded nuances, and strict
linguistic structures. In this work, we introduce TransLaw, a novel multi-agent
framework implemented for real-world Hong Kong case law translation. It employs
three specialized agents, namely, Translator, Annotator, and Proofreader, to
collaboratively produce translations for high accuracy in legal meaning,
appropriateness in style, and adequate coherence and cohesion in structure.
This framework supports customizable LLM configurations and achieves tremendous
cost reduction compared to professional human translation services. We
evaluated its performance using 13 open-source and commercial LLMs as agents
and obtained interesting findings, including that it surpasses GPT-4o in legal
semantic accuracy, structural coherence, and stylistic fidelity, yet trails
human experts in contextualizing complex terminology and stylistic naturalness.
Our platform website is available at CityUHK, and our bilingual judgment corpus
used for the evaluation is available at Hugging Face.

</details>


### [119] [Mathematics Isn't Culture-Free: Probing Cultural Gaps via Entity and Scenario Perturbations](https://arxiv.org/abs/2507.00883)
*Aditya Tomar,Nihar Ranjan Sahoo,Ashish Mittal,Rudra Murthy,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: This paper creates culturally adapted math problem sets for five regions and finds that LLMs perform worse on them, but models with better reasoning are more resilient.


<details>
  <summary>Details</summary>
Motivation: To address the cultural bias in existing math benchmarks like GSM8K, which are mainly based on Western norms, and to evaluate how well LLMs can handle cultural variations in math problem presentation.

Method: Creating culturally adapted variants of GSM8K for five regions through prompt-based transformations and manual verification, then evaluating six LLMs with different prompting strategies.

Result: There is a consistent performance gap where models perform best on the original US-centric dataset and worse on culturally adapted versions. However, models with stronger reasoning abilities are more resilient to these cultural shifts.

Conclusion: Cultural adaptation of math problems affects LLM performance, but reasoning capabilities can help mitigate the impact of cultural presentation gaps.

Abstract: Although mathematics is often considered culturally neutral, the way
mathematical problems are presented can carry implicit cultural context.
Existing benchmarks like GSM8K are predominantly rooted in Western norms,
including names, currencies, and everyday scenarios. In this work, we create
culturally adapted variants of the GSM8K test set for five regions Africa,
India, China, Korea, and Japan using prompt-based transformations followed by
manual verification. We evaluate six large language models (LLMs), ranging from
8B to 72B parameters, across five prompting strategies to assess their
robustness to cultural variation in math problem presentation. Our findings
reveal a consistent performance gap: models perform best on the original
US-centric dataset and comparatively worse on culturally adapted versions.
However, models with reasoning capabilities are more resilient to these shifts,
suggesting that deeper reasoning helps bridge cultural presentation gaps in
mathematical tasks

</details>


### [120] [Scaling Laws Are Unreliable for Downstream Tasks: A Reality Check](https://arxiv.org/abs/2507.00885)
*Nicholas Lourie,Michael Y. Hu,Kyunghyun Cho*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Downstream scaling laws aim to predict task performance at larger scales from
pretraining losses at smaller scales. Whether this prediction should be
possible is unclear: some works demonstrate that task performance follows clear
linear scaling trends under transformation, whereas others point out
fundamental challenges to downstream scaling laws, such as emergence and
inverse scaling. In this work, we conduct a meta-analysis of existing data on
downstream scaling laws, finding that close fit to linear scaling laws only
occurs in a minority of cases: 39% of the time. Furthermore, seemingly benign
changes to the experimental setting can completely change the scaling trend.
Our analysis underscores the need to understand the conditions under which
scaling laws succeed. To fully model the relationship between pretraining loss
and downstream task performance, we must embrace the cases in which scaling
behavior deviates from linear trends.

</details>


### [121] [MemeCMD: An Automatically Generated Chinese Multi-turn Dialogue Dataset with Contextually Retrieved Memes](https://arxiv.org/abs/2507.00891)
*Yuheng Wang,Xianhe Tang,Pufeng Huang*

Main category: cs.CL

TL;DR: Introduce MemeCMD, a Chinese multi-turn dialogue dataset with contextually retrieved memes, combining MLLM-annotated memes and auto-generated dialogues by dual agents.


<details>
  <summary>Details</summary>
Motivation: Existing dialogue datasets lack expressiveness and contextual nuance by focusing on manually annotated or pure-text conversations, while memes offer a more vivid and intuitive way to express emotions and intentions in online social interactions.

Method: Combine a large-scale MLLM-annotated meme library with dialogues auto-generated by dual agents across diverse scenarios, using a retrieval framework and adaptive threshold for contextually relevant meme usage.

Result: Effectiveness of the approach in generating contextually appropriate and diverse meme-incorporated dialogues, providing a scalable and privacy-preserving resource for multimodal conversational AI.

Conclusion: MemeCMD dataset addresses limitations of existing dialogue datasets by incorporating memes, enhancing expressiveness and contextual nuance in multimodal conversations.

Abstract: Memes are widely used in online social interactions, providing vivid,
intuitive, and often humorous means to express intentions and emotions.
Existing dialogue datasets are predominantly limited to either manually
annotated or pure-text conversations, lacking the expressiveness and contextual
nuance that multimodal interactions provide.To address these challenges, we
introduce MemeCMD, an automatically generated Chinese Multi-turn Dialogue
dataset with contextually retrieved memes. Our dataset combines a large-scale,
MLLM-annotated meme library with dialogues auto-generated by dual agents across
diverse scenarios. We introduce a retrieval framework and adaptive threshold to
ensure contextually relevant, naturally spaced meme usage. Experiments
demonstrate the effectiveness of our approach in generating contextually
appropriate and diverse meme-incorporated dialogues, offering a scalable and
privacy-preserving resource for advancing multimodal conversational AI.

</details>


### [122] [The Cognate Data Bottleneck in Language Phylogenetics](https://arxiv.org/abs/2507.00911)
*Luise Häuser,Alexandros Stamatakis*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: To fully exploit the potential of computational phylogenetic methods for
cognate data one needs to leverage specific (complex) models an machine
learning-based techniques. However, both approaches require datasets that are
substantially larger than the manually collected cognate data currently
available. To the best of our knowledge, there exists no feasible approach to
automatically generate larger cognate datasets. We substantiate this claim by
automatically extracting datasets from BabelNet, a large multilingual
encyclopedic dictionary. We demonstrate that phylogenetic inferences on the
respective character matrices yield trees that are largely inconsistent with
the established gold standard ground truth trees. We also discuss why we
consider it as being unlikely to be able to extract more suitable character
matrices from other multilingual resources. Phylogenetic data analysis
approaches that require larger datasets can therefore not be applied to cognate
data. Thus, it remains an open question how, and if these computational
approaches can be applied in historical linguistics.

</details>


### [123] [Discourse Heuristics For Paradoxically Moral Self-Correction](https://arxiv.org/abs/2507.00985)
*Guangliang Liu,Zimo Qi,Xitong Zhang,Kristen Marie Johnson*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Moral self-correction has emerged as a promising approach for aligning the
output of Large Language Models (LLMs) with human moral values. However, moral
self-correction techniques are subject to two primary paradoxes. First, despite
empirical and theoretical evidence to support the effectiveness of
self-correction, this LLM capability only operates at a superficial level.
Second, while LLMs possess the capability of self-diagnosing immoral aspects of
their output, they struggle to identify the cause of this moral inconsistency
during their self-correction process. To better understand and address these
paradoxes, we analyze the discourse constructions in fine-tuning corpora
designed to enhance moral self-correction, uncovering the existence of the
heuristics underlying effective constructions. We demonstrate that moral
self-correction relies on discourse constructions that reflect heuristic
shortcuts, and that the presence of these heuristic shortcuts during
self-correction leads to inconsistency when attempting to enhance both
self-correction and self-diagnosis capabilities jointly. Based on our findings,
we propose a solution to improve moral self-correction by leveraging the
heuristics of curated datasets. We also highlight the generalization challenges
of this capability, particularly in terms of learning from situated context and
model scales.

</details>


### [124] [Should We Still Pretrain Encoders with Masked Language Modeling?](https://arxiv.org/abs/2507.00994)
*Hippolyte Gisserot-Boukhlef,Nicolas Boizard,Manuel Faysse,Duarte M. Alves,Emmanuel Malherbe,André F. T. Martins,Céline Hudelot,Pierre Colombo*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Learning high-quality text representations is fundamental to a wide range of
NLP tasks. While encoder pretraining has traditionally relied on Masked
Language Modeling (MLM), recent evidence suggests that decoder models
pretrained with Causal Language Modeling (CLM) can be effectively repurposed as
encoders, often surpassing traditional encoders on text representation
benchmarks. However, it remains unclear whether these gains reflect an inherent
advantage of the CLM objective or arise from confounding factors such as model
and data scale. In this paper, we address this question through a series of
large-scale, carefully controlled pretraining ablations, training a total of 30
models ranging from 210 million to 1 billion parameters, and conducting over
15,000 fine-tuning and evaluation runs. We find that while training with MLM
generally yields better performance across text representation tasks,
CLM-trained models are more data-efficient and demonstrate improved fine-tuning
stability. Building on these findings, we experimentally show that a biphasic
training strategy that sequentially applies CLM and then MLM, achieves optimal
performance under a fixed computational training budget. Moreover, we
demonstrate that this strategy becomes more appealing when initializing from
readily available pretrained CLM models (from the existing LLM ecosystem),
reducing the computational burden needed to train best-in-class encoder models.
We release all project artifacts at https://hf.co/MLMvsCLM to foster further
research.

</details>


### [125] [La Leaderboard: A Large Language Model Leaderboard for Spanish Varieties and Languages of Spain and Latin America](https://arxiv.org/abs/2507.00999)
*María Grandury,Javier Aula-Blasco,Júlia Falcão,Clémentine Fourrier,Miguel González,Gonzalo Martínez,Gonzalo Santamaría,Rodrigo Agerri,Nuria Aldama,Luis Chiruzzo,Javier Conde,Helena Gómez,Marta Guerrero,Guido Ivetta,Natalia López,Flor Miriam Plaza-del-Arco,María Teresa Martín-Valdivia,Helena Montoro,Carmen Muñoz,Pedro Reviriego,Leire Rosado,Alejandro Vaca,María Estrella Vallecillo-Rodríguez,Jorge Vallego,Irune Zubiaga*

Main category: cs.CL

TL;DR: The paper introduces La Leaderboard, the first open-source leaderboard for evaluating generative LLMs in Spanish-speaking languages and dialects, aiming to promote linguistic and cultural diversity.


<details>
  <summary>Details</summary>
Motivation: To motivate the development of LLMs that represent the linguistic and cultural diversity of the Spanish-speaking community, there is a need for an evaluation standard that focuses on languages and language varieties of Spain and Latin America.

Method: La Leaderboard is a community-driven project that combines 66 datasets across Basque, Catalan, Galician, and various Spanish varieties to evaluate 50 models. The methodology includes guidance on selecting evaluation setups for downstream tasks and using fewer few-shot examples to reduce environmental impact and improve reproducibility.

Result: The initial version of La Leaderboard includes 66 datasets and evaluates 50 models, demonstrating the current state of LLMs in Spanish-speaking languages and dialects.

Conclusion: La Leaderboard establishes a community-driven evaluation standard for LLMs in Spanish-speaking languages, encouraging broader participation and sustainable development by reducing environmental impact through fewer few-shot examples.

Abstract: Leaderboards showcase the current capabilities and limitations of Large
Language Models (LLMs). To motivate the development of LLMs that represent the
linguistic and cultural diversity of the Spanish-speaking community, we present
La Leaderboard, the first open-source leaderboard to evaluate generative LLMs
in languages and language varieties of Spain and Latin America. La Leaderboard
is a community-driven project that aims to establish an evaluation standard for
everyone interested in developing LLMs for the Spanish-speaking community. This
initial version combines 66 datasets in Basque, Catalan, Galician, and
different Spanish varieties, showcasing the evaluation results of 50 models. To
encourage community-driven development of leaderboards in other languages, we
explain our methodology, including guidance on selecting the most suitable
evaluation setup for each downstream task. In particular, we provide a
rationale for using fewer few-shot examples than typically found in the
literature, aiming to reduce environmental impact and facilitate access to
reproducible results for a broader research community.

</details>


### [126] [SciArena: An Open Evaluation Platform for Foundation Models in Scientific Literature Tasks](https://arxiv.org/abs/2507.01001)
*Yilun Zhao,Kaiyan Zhang,Tiansheng Hu,Sihong Wu,Ronan Le Bras,Taira Anderson,Jonathan Bragg,Joseph Chee Chang,Jesse Dodge,Matt Latzke,Yixin Liu,Charles McGrady,Xiangru Tang,Zihang Wang,Chen Zhao,Hannaneh Hajishirzi,Doug Downey,Arman Cohan*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We present SciArena, an open and collaborative platform for evaluating
foundation models on scientific literature tasks. Unlike traditional benchmarks
for scientific literature understanding and synthesis, SciArena engages the
research community directly, following the Chatbot Arena evaluation approach of
community voting on model comparisons. By leveraging collective intelligence,
SciArena offers a community-driven evaluation of model performance on
open-ended scientific tasks that demand literature-grounded, long-form
responses. The platform currently supports 23 open-source and proprietary
foundation models and has collected over 13,000 votes from trusted researchers
across diverse scientific domains. We analyze the data collected so far and
confirm that the submitted questions are diverse, aligned with real-world
literature needs, and that participating researchers demonstrate strong
self-consistency and inter-annotator agreement in their evaluations. We discuss
the results and insights based on the model ranking leaderboard. To further
promote research in building model-based automated evaluation systems for
literature tasks, we release SciArena-Eval, a meta-evaluation benchmark based
on our collected preference data. The benchmark measures the accuracy of models
in judging answer quality by comparing their pairwise assessments with human
votes. Our experiments highlight the benchmark's challenges and emphasize the
need for more reliable automated evaluation methods.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [127] [State and Memory is All You Need for Robust and Reliable AI Agents](https://arxiv.org/abs/2507.00081)
*Matthew Muhoberac,Atharva Parikh,Nirvi Vakharia,Saniya Virani,Aco Radujevic,Savannah Wood,Meghav Verma,Dimitri Metaxotos,Jeyaraman Soundararajan,Thierry Masquelin,Alexander G. Godfrey,Sean Gardner,Dobrila Rudnicki,Sam Michael,Gaurav Chopra*

Main category: cs.MA

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Large language models (LLMs) have enabled powerful advances in natural
language understanding and generation. Yet their application to complex,
real-world scientific workflows remain limited by challenges in memory,
planning, and tool integration. Here, we introduce SciBORG (Scientific Bespoke
Artificial Intelligence Agents Optimized for Research Goals), a modular agentic
framework that allows LLM-based agents to autonomously plan, reason, and
achieve robust and reliable domain-specific task execution. Agents are
constructed dynamically from source code documentation and augmented with
finite-state automata (FSA) memory, enabling persistent state tracking and
context-aware decision-making. This approach eliminates the need for manual
prompt engineering and allows for robust, scalable deployment across diverse
applications via maintaining context across extended workflows and to recover
from tool or execution failures. We validate SciBORG through integration with
both physical and virtual hardware, such as microwave synthesizers for
executing user-specified reactions, with context-aware decision making and
demonstrate its use in autonomous multi-step bioassay retrieval from the
PubChem database utilizing multi-step planning, reasoning, agent-to-agent
communication and coordination for execution of exploratory tasks. Systematic
benchmarking shows that SciBORG agents achieve reliable execution, adaptive
planning, and interpretable state transitions. Our results show that memory and
state awareness are critical enablers of agentic planning and reliability,
offering a generalizable foundation for deploying AI agents in complex
environments.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [128] [Beat and Downbeat Tracking in Performance MIDI Using an End-to-End Transformer Architecture](https://arxiv.org/abs/2507.00466)
*Sebastian Murgul,Michael Heizmann*

Main category: cs.SD

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Beat tracking in musical performance MIDI is a challenging and important task
for notation-level music transcription and rhythmical analysis, yet existing
methods primarily focus on audio-based approaches. This paper proposes an
end-to-end transformer-based model for beat and downbeat tracking in
performance MIDI, leveraging an encoder-decoder architecture for
sequence-to-sequence translation of MIDI input to beat annotations. Our
approach introduces novel data preprocessing techniques, including dynamic
augmentation and optimized tokenization strategies, to improve accuracy and
generalizability across different datasets. We conduct extensive experiments
using the A-MAPS, ASAP, GuitarSet, and Leduc datasets, comparing our model
against state-of-the-art hidden Markov models (HMMs) and deep learning-based
beat tracking methods. The results demonstrate that our model outperforms
existing symbolic music beat tracking approaches, achieving competitive
F1-scores across various musical styles and instruments. Our findings highlight
the potential of transformer architectures for symbolic beat tracking and
suggest future integration with automatic music transcription systems for
enhanced music analysis and score generation.

</details>


### [129] [Leveraging Large Language Models for Spontaneous Speech-Based Suicide Risk Detection](https://arxiv.org/abs/2507.00693)
*Yifan Gao,Jiao Fu,Long Guo,Hong Liu*

Main category: cs.SD

TL;DR: This paper presents a method using large language models and conventional features to identify suicide risk in adolescents through speech, achieving 74% accuracy and first place in the SW1 challenge.


<details>
  <summary>Details</summary>
Motivation: Early identification of suicide risk is crucial for preventing suicidal behaviors, leading to the focus on finding patterns and markers for suicide risk in current research.

Method: The approach uses large language models (LLM) for feature extraction along with traditional acoustic and semantic features.

Result: The proposed method achieved 74% accuracy on the test set, ranking first in the SW1 challenge.

Conclusion: The findings show the potential of LLM-based methods for analyzing speech in suicide risk assessment.

Abstract: Early identification of suicide risk is crucial for preventing suicidal
behaviors. As a result, the identification and study of patterns and markers
related to suicide risk have become a key focus of current research. In this
paper, we present the results of our work in the 1st SpeechWellness Challenge
(SW1), which aims to explore speech as a non-invasive and easily accessible
mental health indicator for identifying adolescents at risk of suicide.Our
approach leverages large language model (LLM) as the primary tool for feature
extraction, alongside conventional acoustic and semantic features. The proposed
method achieves an accuracy of 74\% on the test set, ranking first in the SW1
challenge. These findings demonstrate the potential of LLM-based methods for
analyzing speech in the context of suicide risk assessment.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [130] [MassTool: A Multi-Task Search-Based Tool Retrieval Framework for Large Language Models](https://arxiv.org/abs/2507.00487)
*Jianghao Lin,Xinyuan Wang,Xinyi Dai,Menghui Zhu,Bo Chen,Ruiming Tang,Yong Yu,Weinan Zhang*

Main category: cs.IR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Tool retrieval is a critical component in enabling large language models
(LLMs) to interact effectively with external tools. It aims to precisely filter
the massive tools into a small set of candidates for the downstream
tool-augmented LLMs. However, most existing approaches primarily focus on
optimizing tool representations, often neglecting the importance of precise
query comprehension. To address this gap, we introduce MassTool, a multi-task
search-based framework designed to enhance both query representation and tool
retrieval accuracy. MassTool employs a two-tower architecture: a tool usage
detection tower that predicts the need for function calls, and a tool retrieval
tower that leverages a query-centric graph convolution network (QC-GCN) for
effective query-tool matching. It also incorporates search-based user intent
modeling (SUIM) to handle diverse and out-of-distribution queries, alongside an
adaptive knowledge transfer (AdaKT) module for efficient multi-task learning.
By jointly optimizing tool usage detection loss, list-wise retrieval loss, and
contrastive regularization loss, MassTool establishes a robust dual-step
sequential decision-making pipeline for precise query understanding. Extensive
experiments demonstrate its effectiveness in improving retrieval accuracy. Our
code is available at https://github.com/wxydada/MassTool.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [131] [Efficient Conformance Checking of Rich Data-Aware Declare Specifications (Extended)](https://arxiv.org/abs/2507.00094)
*Jacobo Casas-Ramos,Sarah Winkler,Alessandro Gianola,Marco Montali,Manuel Mucientes,Manuel Lama*

Main category: cs.DB

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Despite growing interest in process analysis and mining for data-aware
specifications, alignment-based conformance checking for declarative process
models has focused on pure control-flow specifications, or mild data-aware
extensions limited to numerical data and variable-to-constant comparisons. This
is not surprising: finding alignments is computationally hard, even more so in
the presence of data dependencies. In this paper, we challenge this problem in
the case where the reference model is captured using data-aware Declare with
general data types and data conditions. We show that, unexpectedly, it is
possible to compute data-aware optimal alignments in this rich setting,
enjoying at once efficiency and expressiveness. This is achieved by carefully
combining the two best-known approaches to deal with control flow and data
dependencies when computing alignments, namely A* search and SMT solving.
Specifically, we introduce a novel algorithmic technique that efficiently
explores the search space, generating descendant states through the application
of repair actions aiming at incrementally resolving constraint violations. We
prove the correctness of our algorithm and experimentally show its efficiency.
The evaluation witnesses that our approach matches or surpasses the performance
of the state of the art while also supporting significantly more expressive
data dependencies, showcasing its potential to support real-world applications.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [132] [Phase Transition in Nonparametric Minimax Rates for Covariate Shifts on Approximate Manifolds](https://arxiv.org/abs/2507.00889)
*Yuyao Wang,Nabarun Deb,Debarghya Mukherjee*

Main category: math.ST

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study nonparametric regression under covariate shift with structured data,
where a small amount of labeled target data is supplemented by a large labeled
source dataset. In many real-world settings, the covariates in the target
domain lie near a low-dimensional manifold within the support of the source,
e.g., personalized handwritten digits (target) within a large, high-dimensional
image repository (source). Since density ratios may not exist in these
settings, standard transfer learning techniques often fail to leverage such
structure. This necessitates the development of methods that exploit both the
size of the source dataset and the structured nature of the target.
  Motivated by this, we establish new minimax rates under covariate shift for
estimating a regression function in a general H\"older class, assuming the
target distribution lies near -- but not exactly on -- a smooth submanifold of
the source. General smoothness helps reduce the curse of dimensionality when
the target function is highly regular, while approximate manifolds capture
realistic, noisy data. We identify a phase transition in the minimax rate of
estimation governed by the distance to the manifold, source and target sample
sizes, function smoothness, and intrinsic versus ambient dimensions. We propose
a local polynomial regression estimator that achieves optimal rates on either
side of the phase transition boundary. Additionally, we construct a fully
adaptive procedure that adjusts to unknown smoothness and intrinsic dimension,
and attains nearly optimal rates. Our results unify and extend key threads in
covariate shift, manifold learning, and adaptive nonparametric inference.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [133] [Estimating Correctness Without Oracles in LLM-Based Code Generation](https://arxiv.org/abs/2507.00057)
*Thomas Valentin,Ardi Madadi,Gaetano Sapia,Marcel Böhme*

Main category: cs.PL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Generating code from natural language specifications is one of the most
successful applications of Large Language Models (LLMs). Yet, they hallucinate:
LLMs produce outputs that may be grammatically correct but are factually
incorrect. Without an existing, correct implementation (i.e., an oracle), can
we quantify how likely the generated program is correct?
  In this paper, we propose a measure of incorrectness, called incoherence,
that can be estimated efficiently in the absence of an oracle and provides a
lower bound on the error, i.e., the probability that the LLM-generated program
for that specification is incorrect. Our experiments demonstrate an
extraordinary effectiveness. For the average code generation task, our
incoherence-based methodology can automatically identify about two-thirds of
incorrect programs without reports of false positives. In fact, an oracle-based
evaluation of LLMs can be reliably replaced by an incoherence-based evaluation.
In particular, we find a very strong agreement between the ranking of LLMs by
the number of programs deemed correct via an oracle (pass@1) and the ranking of
LLMs by the number of programs deemed correct via our incoherence.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [134] [InSight-R: A Framework for Risk-informed Human Failure Event Identification and Interface-Induced Risk Assessment Driven by AutoGraph](https://arxiv.org/abs/2507.00066)
*Xingyu Xiao,Jiejuan Tong,Peng Chen,Jun Sun,Zhe Sui,Jingang Liang,Hongru Zhao,Jun Zhao,Haitao Wang*

Main category: cs.HC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Human reliability remains a critical concern in safety-critical domains such
as nuclear power, where operational failures are often linked to human error.
While conventional human reliability analysis (HRA) methods have been widely
adopted, they rely heavily on expert judgment for identifying human failure
events (HFEs) and assigning performance influencing factors (PIFs). This
reliance introduces challenges related to reproducibility, subjectivity, and
limited integration of interface-level data. In particular, current approaches
lack the capacity to rigorously assess how human-machine interface design
contributes to operator performance variability and error susceptibility. To
address these limitations, this study proposes a framework for risk-informed
human failure event identification and interface-induced risk assessment driven
by AutoGraph (InSight-R). By linking empirical behavioral data to the
interface-embedded knowledge graph (IE-KG) constructed by the automated
graph-based execution framework (AutoGraph), the InSight-R framework enables
automated HFE identification based on both error-prone and time-deviated
operational paths. Furthermore, we discuss the relationship between
designer-user conflicts and human error. The results demonstrate that InSight-R
not only enhances the objectivity and interpretability of HFE identification
but also provides a scalable pathway toward dynamic, real-time human
reliability assessment in digitalized control environments. This framework
offers actionable insights for interface design optimization and contributes to
the advancement of mechanism-driven HRA methodologies.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [135] [Hypertokens: Holographic Associative Memory in Tokenized LLMs](https://arxiv.org/abs/2507.00002)
*Christopher James Augeri*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Large Language Models (LLMs) exhibit remarkable capabilities but suffer from
apparent precision loss, reframed here as information spreading. This reframing
shifts the problem from computational precision to an information-theoretic
communication issue. We address the K:V and V:K memory problem in LLMs by
introducing HDRAM (Holographically Defined Random Access Memory), a symbolic
memory framework treating transformer latent space as a spread-spectrum
channel. Built upon hypertokens, structured symbolic codes integrating
classical error-correcting codes (ECC), holographic computing, and
quantum-inspired search, HDRAM recovers distributed information through
principled despreading. These phase-coherent memory addresses enable efficient
key-value operations and Grover-style search in latent space. By combining ECC
grammar with compressed sensing and Krylov subspace alignment, HDRAM
significantly improves associative retrieval without architectural changes,
demonstrating how Classical-Holographic-Quantum-inspired (CHQ) principles can
fortify transformer architectures.

</details>


### [136] [Deciding When Not to Decide: Indeterminacy-Aware Intrusion Detection with NeutroSENSE](https://arxiv.org/abs/2507.00003)
*Eyhab Al-Masri*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper presents NeutroSENSE, a neutrosophic-enhanced ensemble framework
for interpretable intrusion detection in IoT environments. By integrating
Random Forest, XGBoost, and Logistic Regression with neutrosophic logic, the
system decomposes prediction confidence into truth (T), falsity (F), and
indeterminacy (I) components, enabling uncertainty quantification and
abstention. Predictions with high indeterminacy are flagged for review using
both global and adaptive, class-specific thresholds. Evaluated on the IoT-CAD
dataset, NeutroSENSE achieved 97% accuracy, while demonstrating that
misclassified samples exhibit significantly higher indeterminacy (I = 0.62)
than correct ones (I = 0.24). The use of indeterminacy as a proxy for
uncertainty enables informed abstention and targeted review-particularly
valuable in edge deployments. Figures and tables validate the correlation
between I-scores and error likelihood, supporting more trustworthy,
human-in-the-loop AI decisions. This work shows that neutrosophic logic
enhances both accuracy and explainability, providing a practical foundation for
trust-aware AI in edge and fog-based IoT security systems.

</details>


### [137] [A Theory of Inference Compute Scaling: Reasoning through Directed Stochastic Skill Search](https://arxiv.org/abs/2507.00004)
*Austin R. Ellis-Mohr,Anuj K. Nayak,Lav R. Varshney*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Large language models (LLMs) demand considerable computational, energy, and
financial resources during both training and deployment. While scaling laws for
training have guided much of the field's recent progress, inference costs now
represent a significant and growing component of the overall resource burden,
particularly for reasoning-focused models. Existing characterizations of
compute-optimality that consider model size, dataset size, and inference tokens
in isolation or in fixed combinations risk overlooking more efficient operating
points. We introduce directed stochastic skill search (DS3), a general
framework that represents inference as stochastic traversal over a learned
skill graph. From a simplified yet expressive instantiation, we derive
closed-form expressions for task success and compute cost across a wide range
of inference strategies -- including chain-of-thought (CoT) and tree-of-thought
(ToT) -- enabling comparative analysis as a function of task difficulty and
model capability. To that end, we extend a prior first-principles tripartite
graph framework of LLM training to incorporate inference, and separately bridge
DS3 with empirical methods that characterize LLM scaling behavior. We
theoretically recover empirically observed patterns, including: linear accuracy
scaling with logarithmic compute; variation in preferred inference strategies
as a function of task difficulty and model capability; emergent behavior
elicited by reasoning even when performance plateaus under parameter scaling;
and both best-of-N (BoN) and majority voting behavior captured within a unified
analytical framework. By explicitly characterizing training-inference
interdependencies, our framework deepens theoretical understanding and supports
principled algorithmic design and resource allocation.

</details>


### [138] [Novel RL approach for efficient Elevator Group Control Systems](https://arxiv.org/abs/2507.00011)
*Nathan Vaartjes,Vincent Francois-Lavet*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Efficient elevator traffic management in large buildings is critical for
minimizing passenger travel times and energy consumption. Because heuristic- or
pattern-detection-based controllers struggle with the stochastic and
combinatorial nature of dispatching, we model the six-elevator, fifteen-floor
system at Vrije Universiteit Amsterdam as a Markov Decision Process and train
an end-to-end Reinforcement Learning (RL) Elevator Group Control System (EGCS).
Key innovations include a novel action space encoding to handle the
combinatorial complexity of elevator dispatching, the introduction of
infra-steps to model continuous passenger arrivals, and a tailored reward
signal to improve learning efficiency. In addition, we explore various ways to
adapt the discounting factor to the infra-step formulation. We investigate RL
architectures based on Dueling Double Deep Q-learning, showing that the
proposed RL-based EGCS adapts to fluctuating traffic patterns, learns from a
highly stochastic environment, and thereby outperforms a traditional rule-based
algorithm.

</details>


### [139] [Towards Undistillable Models by Minimizing Conditional Mutual Information](https://arxiv.org/abs/2507.00012)
*Linfeng Ye,Shayan Mohajer Hamidi,En-hui Yang*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: A deep neural network (DNN) is said to be undistillable if, when used as a
black-box input-output teacher, it cannot be distilled through knowledge
distillation (KD). In this case, the distilled student (referred to as the
knockoff student) does not outperform a student trained independently with
label smoothing (LS student) in terms of prediction accuracy. To protect
intellectual property of DNNs, it is desirable to build undistillable DNNs. To
this end, it is first observed that an undistillable DNN may have the trait
that each cluster of its output probability distributions in response to all
sample instances with the same label should be highly concentrated to the
extent that each cluster corresponding to each label should ideally collapse
into one probability distribution. Based on this observation and by measuring
the concentration of each cluster in terms of conditional mutual information
(CMI), a new training method called CMI minimized (CMIM) method is proposed,
which trains a DNN by jointly minimizing the conventional cross entropy (CE)
loss and the CMI values of all temperature scaled clusters across the entire
temperature spectrum. The resulting CMIM model is shown, by extensive
experiments, to be undistillable by all tested KD methods existing in the
literature. That is, the knockoff students distilled by these KD methods from
the CMIM model underperform the respective LS students. In addition, the CMIM
model is also shown to performs better than the model trained with the CE loss
alone in terms of their own prediction accuracy.

</details>


### [140] [ST-MTM: Masked Time Series Modeling with Seasonal-Trend Decomposition for Time Series Forecasting](https://arxiv.org/abs/2507.00013)
*Hyunwoo Seo,Chiehyeon Lim*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Forecasting complex time series is an important yet challenging problem that
involves various industrial applications. Recently, masked time-series modeling
has been proposed to effectively model temporal dependencies for forecasting by
reconstructing masked segments from unmasked ones. However, since the semantic
information in time series is involved in intricate temporal variations
generated by multiple time series components, simply masking a raw time series
ignores the inherent semantic structure, which may cause MTM to learn spurious
temporal patterns present in the raw data. To capture distinct temporal
semantics, we show that masked modeling techniques should address entangled
patterns through a decomposition approach. Specifically, we propose ST-MTM, a
masked time-series modeling framework with seasonal-trend decomposition, which
includes a novel masking method for the seasonal-trend components that
incorporates different temporal variations from each component. ST-MTM uses a
period masking strategy for seasonal components to produce multiple masked
seasonal series based on inherent multi-periodicity and a sub-series masking
strategy for trend components to mask temporal regions that share similar
variations. The proposed masking method presents an effective pre-training task
for learning intricate temporal variations and dependencies. Additionally,
ST-MTM introduces a contrastive learning task to support masked modeling by
enhancing contextual consistency among multiple masked seasonal
representations. Experimental results show that our proposed ST-MTM achieves
consistently superior forecasting performance compared to existing masked
modeling, contrastive learning, and supervised forecasting methods.

</details>


### [141] [SWE-Bench-CL: Continual Learning for Coding Agents](https://arxiv.org/abs/2507.00014)
*Thomas Joshi,Shayan Chowdhury,Fatih Uysal*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Large Language Models (LLMs) have achieved impressive results on static
code-generation benchmarks, but real-world software development unfolds as a
continuous stream of evolving issues, fixes, and feature requests. We introduce
SWE-Bench-CL, a novel continual learning benchmark built on the human-verified
SWE-Bench Verified dataset introduced by OpenAI and Princeton-NLP in 2024. By
organizing GitHub issues into chronologically ordered sequences that reflect
natural repository evolution, SWE-Bench-CL enables direct evaluation of an
agent's ability to accumulate experience, transfer knowledge across tasks, and
resist catastrophic forgetting. We complement the dataset with (i) a
preliminary analysis of inter-task structural similarity and contextual
sensitivity, (ii) an interactive LangGraph-based evaluation framework augmented
with a FAISS-backed semantic memory module, and (iii) a suite of specialized
continual learning metrics -- including average accuracy, forgetting,
forward/backward transfer, tool-use efficiency, and a generalized Composite
Continual Learning Score and CL-F-beta score -- to capture the
stability-plasticity trade-off. We outline a rigorous experimental protocol
comparing memory-enabled and memory-disabled agents across diverse Python
repositories. All code and data are publicly available at
https://github.com/thomasjoshi/agents-never-forget, providing the community
with a reproducible platform for developing more adaptive and robust AI agents
in software engineering.

</details>


### [142] [Vision Transformer with Adversarial Indicator Token against Adversarial Attacks in Radio Signal Classifications](https://arxiv.org/abs/2507.00015)
*Lu Zhang,Sangarapillai Lambotharan,Gan Zheng,Guisheng Liao,Xuekang Liu,Fabio Roli,Carsten Maple*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The remarkable success of transformers across various fields such as natural
language processing and computer vision has paved the way for their
applications in automatic modulation classification, a critical component in
the communication systems of Internet of Things (IoT) devices. However, it has
been observed that transformer-based classification of radio signals is
susceptible to subtle yet sophisticated adversarial attacks. To address this
issue, we have developed a defensive strategy for transformer-based modulation
classification systems to counter such adversarial attacks. In this paper, we
propose a novel vision transformer (ViT) architecture by introducing a new
concept known as adversarial indicator (AdvI) token to detect adversarial
attacks. To the best of our knowledge, this is the first work to propose an
AdvI token in ViT to defend against adversarial attacks. Integrating an
adversarial training method with a detection mechanism using AdvI token, we
combine a training time defense and running time defense in a unified neural
network model, which reduces architectural complexity of the system compared to
detecting adversarial perturbations using separate models. We investigate into
the operational principles of our method by examining the attention mechanism.
We show the proposed AdvI token acts as a crucial element within the ViT,
influencing attention weights and thereby highlighting regions or features in
the input data that are potentially suspicious or anomalous. Through
experimental results, we demonstrate that our approach surpasses several
competitive methods in handling white-box attack scenarios, including those
utilizing the fast gradient method, projected gradient descent attacks and
basic iterative method.

</details>


### [143] [Gradient-based Fine-Tuning through Pre-trained Model Regularization](https://arxiv.org/abs/2507.00016)
*Xuanbo Liu,Liu Liu,Fuxiang Wu,Fusheng Hao,Xianglong Liu*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Large pre-trained models have demonstrated extensive applications across
various fields. However, fine-tuning these models for specific downstream tasks
demands significant computational resources and storage. One fine-tuning
method, gradient-based parameter selection (GPS), focuses on fine-tuning only
the parameters with high gradients in each neuron, thereby reducing the number
of training parameters. Nevertheless, this approach increases computational
resource requirements and storage demands. In this paper, we propose an
efficient gradient-based and regularized fine-tuning method (GRFT) that updates
the rows or columns of the weight matrix. We theoretically demonstrate that the
rows or columns with the highest sum of squared gradients are optimal for
updating. This strategy effectively reduces storage overhead and improves the
efficiency of parameter selection. Additionally, we incorporate regularization
to enhance knowledge transfer from the pre-trained model. GRFT achieves
state-of-the-art performance, surpassing existing methods such as GPS, Adapter
Tuning, and LoRA. Notably, GRFT requires updating only 1.22% and 0.30% of the
total parameters on FGVC and VTAB datasets, respectively, demonstrating its
high efficiency and effectiveness. The source code will be released soon.

</details>


### [144] [Implicit Reward as the Bridge: A Unified View of SFT and DPO Connections](https://arxiv.org/abs/2507.00018)
*Bo Wang,Qinyuan Cheng,Runyu Peng,Rong Bao,Peiji Li,Qipeng Guo,Linyang Li,Zhiyuan Zeng,Yunhua Zhou,Xipeng Qiu*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Post-training processes are essential phases in grounding pre-trained
language models to real-world tasks, with learning from demonstrations or
preference signals playing a crucial role in this adaptation. We present a
unified theoretical framework bridging Supervised Fine-Tuning (SFT) and
preference learning in Large Language Model (LLM) post-training. Through
rigorous mathematical derivation, we demonstrate that both SFT and preference
learning methods like Direct Preference Optimization (DPO) operate within the
same optimal policy-reward subspace, with SFT representing a special case of
implicit reward learning. Our analysis reveals a critical limitation in
conventional SFT: the KL divergence term in distribution matching becomes
constant with respect to the policy during optimization, failing to constrain
model updates. To address this, we propose a simple yet effective learning rate
reduction approach that yields significant performance improvements (up to
\textbf{25\%} relative gain and \textbf{6\%} absolute win rate increase in
instruction following tasks. Additionally, we derive alternative SFT objectives
from various f-divergence functions that preserve the KL term during
optimization, further enhancing post-DPO model performance. Finally, we extend
the theoretical relationship between LLM logits and Q-functions from preference
learning to the SFT context, providing mathematical derivations and
experimental validation.

</details>


### [145] [Quantum Inspired Encoding Strategies for Machine Learning Models: Proposing and Evaluating Instance Level, Global Discrete, and Class Conditional Representations](https://arxiv.org/abs/2507.00019)
*Minati Rath,Hema Date*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this study, we propose, evaluate and compare three quantum inspired data
encoding strategies, Instance Level Strategy (ILS), Global Discrete Strategy
(GDS) and Class Conditional Value Strategy (CCVS), for transforming classical
data into quantum data for use in pure classical machine learning models. The
primary objective is to reduce high encoding time while ensuring correct
encoding values and analyzing their impact on classification performance. The
Instance Level Strategy treats each row of dataset independently; mimics local
quantum states. Global Discrete Value Based encoding strategy maps all unique
feature values across the full dataset to quantum states uniformly. In
contrast, the Class conditional Value based encoding strategy encodes unique
values separately for each class, preserving class dependent information.
  We apply these encoding strategies to a classification task and assess their
impact on en-coding efficiency, correctness, model accuracy, and computational
cost. By analyzing the trade offs between encoding time, precision, and
predictive performance, this study provides insights into optimizing quantum
inspired data transformations for classical machine learning workflows.

</details>


### [146] [GLU Attention Improve Transformer](https://arxiv.org/abs/2507.00022)
*Zehao Wang*

Main category: cs.LG

TL;DR: The paper introduces GLU Attention, a novel attention mechanism that enhances neural network performance by adding nonlinearity to attention values, with no extra parameters and minimal computational cost, integrating well with existing technologies.


<details>
  <summary>Details</summary>
Motivation: To enhance neural network performance by introducing nonlinearity into the attention mechanism without increasing parameters or computational costs.

Method: Introduce GLU Attention, which modifies the attention mechanism to incorporate nonlinearity in the values.

Result: Experiments show GLU Attention improves model performance and convergence speed in both text and vision tasks with zero additional parameters and negligible computational overhead.

Conclusion: GLU Attention is a lightweight, effective attention mechanism that integrates seamlessly with existing technologies like Flash Attention, RoPE, and MHA variants.

Abstract: Gated Linear Units (GLU) have shown great potential in enhancing neural
network performance. In this paper, I introduce a novel attention mechanism
called GLU Attention, which introduces nonlinearity into the values of
Attention. My experiments demonstrate that GLU Attention improves both model
performance and convergence speed across text and vision modalities with zero
additional parameters and negligible computational costs. GLU Attention is
lightweight and can seamlessly integrate with other technologies, such as Flash
Attention, Rotary Position Embedding (RoPE), and various Multi-Head Attention
(MHA) variants such as Grouped-Query Attention (GQA). This project is
open-sourced at github.

</details>


### [147] [AIMatDesign: Knowledge-Augmented Reinforcement Learning for Inverse Materials Design under Data Scarcity](https://arxiv.org/abs/2507.00024)
*Yeyong Yu,Xilei Bian,Jie Xiong,Xing Wu,Quan Qian*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: With the growing demand for novel materials, machine learning-driven inverse
design methods face significant challenges in reconciling the high-dimensional
materials composition space with limited experimental data. Existing approaches
suffer from two major limitations: (I) machine learning models often lack
reliability in high-dimensional spaces, leading to prediction biases during the
design process; (II) these models fail to effectively incorporate domain expert
knowledge, limiting their capacity to support knowledge-guided inverse design.
To address these challenges, we introduce AIMatDesign, a reinforcement learning
framework that addresses these limitations by augmenting experimental data
using difference-based algorithms to build a trusted experience pool,
accelerating model convergence. To enhance model reliability, an automated
refinement strategy guided by large language models (LLMs) dynamically corrects
prediction inconsistencies, reinforcing alignment between reward signals and
state value functions. Additionally, a knowledge-based reward function
leverages expert domain rules to improve stability and efficiency during
training. Our experiments demonstrate that AIMatDesign significantly surpasses
traditional machine learning and reinforcement learning methods in discovery
efficiency, convergence speed, and success rates. Among the numerous candidates
proposed by AIMatDesign, experimental synthesis of representative Zr-based
alloys yielded a top-performing BMG with 1.7GPa yield strength and 10.2\%
elongation, closely matching predictions. Moreover, the framework accurately
captured the trend of yield strength variation with composition, demonstrating
its reliability and potential for closed-loop materials discovery.

</details>


### [148] [Generalizing to New Dynamical Systems via Frequency Domain Adaptation](https://arxiv.org/abs/2507.00025)
*Tiexin Qin,Hong Yan,Haoliang Li*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Learning the underlying dynamics from data with deep neural networks has
shown remarkable potential in modeling various complex physical dynamics.
However, current approaches are constrained in their ability to make reliable
predictions in a specific domain and struggle with generalizing to unseen
systems that are governed by the same general dynamics but differ in
environmental characteristics. In this work, we formulate a parameter-efficient
method, Fourier Neural Simulator for Dynamical Adaptation (FNSDA), that can
readily generalize to new dynamics via adaptation in the Fourier space.
Specifically, FNSDA identifies the shareable dynamics based on the known
environments using an automatic partition in Fourier modes and learns to adjust
the modes specific for each new environment by conditioning on low-dimensional
latent systematic parameters for efficient generalization. We evaluate our
approach on four representative families of dynamic systems, and the results
show that FNSDA can achieve superior or competitive generalization performance
compared to existing methods with a significantly reduced parameter cost. Our
code is available at https://github.com/WonderSeven/FNSDA.

</details>


### [149] [ROSE: Toward Reality-Oriented Safety Evaluation of Large Language Models](https://arxiv.org/abs/2507.00026)
*Jiale Ding,Xiang Zheng,Cong Wang,Wei-Bin Lee,Xingjun Ma,Yu-Gang Jiang*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: As Large Language Models (LLMs) are increasingly deployed as black-box
components in real-world applications, evaluating their safety-especially under
adversarial prompting-has become critical. Arguably, effective safety
evaluations should be adaptive, evolving with LLM capabilities, and also cover
a broad spectrum of harmful topics and real-world scenarios to fully expose
potential vulnerabilities. Existing manual safety benchmarks, built on
handcrafted adversarial prompts, are limited by their static nature and the
intensive labor required to update them, making it difficult to keep pace with
rapidly advancing LLMs. In contrast, automated adversarial prompt generation
offers a promising path toward adaptive evaluation. However, current methods
often suffer from insufficient adversarial topic coverage (topic-level
diversity) and weak alignment with real-world contexts. These shortcomings stem
from the exploration-exploitation dilemma in black-box optimization and a lack
of real-world contextualization, resulting in adversarial prompts that are both
topically narrow and scenario-repetitive. To address these issues, we propose
Reality-Oriented Safety Evaluation (ROSE), a novel framework that uses
multi-objective reinforcement learning to fine-tune an adversarial LLM for
generating topically diverse and contextually rich adversarial prompts.
Experiments show that ROSE outperforms existing methods in uncovering safety
vulnerabilities in state-of-the-art LLMs, with notable improvements in
integrated evaluation metrics. We hope ROSE represents a step toward more
practical and reality-oriented safety evaluation of LLMs. WARNING: This paper
contains examples of potentially harmful text.

</details>


### [150] [HiT-JEPA: A Hierarchical Self-supervised Trajectory Embedding Framework for Similarity Computation](https://arxiv.org/abs/2507.00028)
*Lihuan Li,Hao Xue,Shuang Ao,Yang Song,Flora Salim*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The representation of urban trajectory data plays a critical role in
effectively analyzing spatial movement patterns. Despite considerable progress,
the challenge of designing trajectory representations that can capture diverse
and complementary information remains an open research problem. Existing
methods struggle in incorporating trajectory fine-grained details and
high-level summary in a single model, limiting their ability to attend to both
long-term dependencies while preserving local nuances. To address this, we
propose HiT-JEPA (Hierarchical Interactions of Trajectory Semantics via a Joint
Embedding Predictive Architecture), a unified framework for learning
multi-scale urban trajectory representations across semantic abstraction
levels. HiT-JEPA adopts a three-layer hierarchy that progressively captures
point-level fine-grained details, intermediate patterns, and high-level
trajectory abstractions, enabling the model to integrate both local dynamics
and global semantics in one coherent structure. Extensive experiments on
multiple real-world datasets for trajectory similarity computation show that
HiT-JEPA's hierarchical design yields richer, multi-scale representations. Code
is available at: https://anonymous.4open.science/r/HiT-JEPA.

</details>


### [151] [LoRA-Mixer: Coordinate Modular LoRA Experts Through Serial Attention Routing](https://arxiv.org/abs/2507.00029)
*Wenbing Li,Zikai Song,Hang Zhou,Yunyao Zhang,Junqing Yu,Wei Yang*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Recent efforts to combine low-rank adaptation (LoRA) with mixture-of-experts
(MoE) for adapting large language models (LLMs) to multiple tasks still exhibit
prevailing limitations: they either swap entire attention/feed-forward layers
for switch experts or bolt on parallel expert branches, diluting parameter
efficiency and task fidelity. We propose the LoRA-Mixer, a modular and
lightweight MoE framework that integrates LoRA experts. Our core innovation
lies in replacing the projection matrices of the attention module's
input/output linear layers with dynamically routed, task-specific LoRA experts.
This design ensures seamless compatibility with diverse foundation models,
including transformers and state space models (SSMs), by leveraging their
inherent linear projection structures. The framework supports two operational
paradigms: (1) joint optimization of LoRA experts and routing mechanisms via a
novel hard-soft routing strategy, or (2) direct deployment of pre-trained,
frozen LoRA modules sourced from external repositories. To enable robust router
training with limited data while ensuring stable routing decisions and
maximizing expert reuse, we introduce an adaptive Specialization Balance Loss
(SBL) that jointly optimizes expert balance and task-specific alignment.
Extensive experiments on seven benchmark datasets, including MedQA, CoLA,
SST-2, GSM8K, ARC-E, ARC-C, and HumanEval, demonstrate the effectiveness of
LoRA-Mixer. On datasets such as GSM8K, HumanEval, and MedQA, LoRA-Mixer
achieves significant improvements of 7.61%, 4.88%, and 3.08% over the base
models, respectively. Compared with state-of-the-art methods, LoRA-Mixer
achieves additional improvements of 1.09%, 1.45%, and 1.68%, respectively,
using only 48% of the parameters, demonstrating its efficiency and strong
performance.

</details>


### [152] [Adaptive Action Duration with Contextual Bandits for Deep Reinforcement Learning in Dynamic Environments](https://arxiv.org/abs/2507.00030)
*Abhishek Verma,Nallarasan V,Balaraman Ravindran*

Main category: cs.LG

TL;DR: This paper introduces a method that combines contextual bandits with DRL to adaptively choose action durations, improving performance in games and robotics.


<details>
  <summary>Details</summary>
Motivation: DRL's temporal scale of action execution is underexplored, yet crucial for real-time applications.

Method: Augmenting DQN with a contextual bandit module to learn optimal action repetition rates.

Result: Experiments show significant performance improvements over static duration baselines on Atari games.

Conclusion: Adaptive temporal abstractions enhance DRL's flexibility and efficiency, offering scalable solutions for real-time tasks.

Abstract: Deep Reinforcement Learning (DRL) has achieved remarkable success in complex
sequential decision-making tasks, such as playing Atari 2600 games and
mastering board games. A critical yet underexplored aspect of DRL is the
temporal scale of action execution. We propose a novel paradigm that integrates
contextual bandits with DRL to adaptively select action durations, enhancing
policy flexibility and computational efficiency. Our approach augments a Deep
Q-Network (DQN) with a contextual bandit module that learns to choose optimal
action repetition rates based on state contexts. Experiments on Atari 2600
games demonstrate significant performance improvements over static duration
baselines, highlighting the efficacy of adaptive temporal abstractions in DRL.
This paradigm offers a scalable solution for real-time applications like gaming
and robotics, where dynamic action durations are critical.

</details>


### [153] [Model Fusion via Neuron Interpolation](https://arxiv.org/abs/2507.00037)
*Phoomraphee Luenam,Andreas Spanopoulos,Amit Sant,Thomas Hofmann,Sotiris Anagnostidis,Sidak Pal Singh*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Model fusion aims to combine the knowledge of multiple models by creating one
representative model that captures the strengths of all of its parents.
However, this process is non-trivial due to differences in internal
representations, which can stem from permutation invariance, random
initialization, or differently distributed training data. We present a novel,
neuron-centric family of model fusion algorithms designed to integrate multiple
trained neural networks into a single network effectively regardless of
training data distribution. Our algorithms group intermediate neurons of parent
models to create target representations that the fused model approximates with
its corresponding sub-network. Unlike prior approaches, our approach
incorporates neuron attribution scores into the fusion process. Furthermore,
our algorithms can generalize to arbitrary layer types. Experimental results on
various benchmark datasets demonstrate that our algorithms consistently
outperform previous fusion techniques, particularly in zero-shot and non-IID
fusion scenarios. The code is available at
https://github.com/AndrewSpano/neuron-interpolation-model-fusion.

</details>


### [154] [Quality over Quantity: An Effective Large-Scale Data Reduction Strategy Based on Pointwise V-Information](https://arxiv.org/abs/2507.00038)
*Fei Chen,Wenchi Zhou*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Data reduction plays a vital role in data-centric AI by identifying the most
informative instance within large-scale datasets to enhance model training
efficiency. The core challenge lies in how to select the optimal
instances-rather than the entire datasets-to improve data quality and training
efficiency. In this paper, we propose an effective data reduction strategy
based on Pointwise V-information(PVI). First, we quantify instance difficulty
using PVI and filter out low-difficulty instances enabling a static approach.
Experiments demonstrate that removing 10%-30% of the data preserves the
classifier performance with only a 0.0001% to 0.76% loss in accuracy.Second, we
use a progressive learning approach to training the classifiers on instances
sorted by ascending PVI, accelerating convergence and achieving a 0.8% accuracy
gain over conventional training. Our results suggest that with the effective
data reduction strategy, training a classifier on the selected optimal subset
could enhance the model performance and boost training efficiency. Moreover, we
have transferred the PVI framework, which previously applied only to English
datasets, to diverse Chinese NLP tasks and base models, leading to valuable
insights for cross-lingual data reduction and faster training. The codes are
released at https://github.com/zhouwenchi/DatasetReductionStrategy.

</details>


### [155] [Pattern-Based Graph Classification: Comparison of Quality Measures and Importance of Preprocessing](https://arxiv.org/abs/2507.00039)
*Lucas Potin,Rosa Figueiredo,Vincent Labatut,Christine Largeron*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Graph classification aims to categorize graphs based on their structural and
attribute features, with applications in diverse fields such as social network
analysis and bioinformatics. Among the methods proposed to solve this task,
those relying on patterns (i.e. subgraphs) provide good explainability, as the
patterns used for classification can be directly interpreted. To identify
meaningful patterns, a standard approach is to use a quality measure, i.e. a
function that evaluates the discriminative power of each pattern. However, the
literature provides tens of such measures, making it difficult to select the
most appropriate for a given application. Only a handful of surveys try to
provide some insight by comparing these measures, and none of them specifically
focuses on graphs. This typically results in the systematic use of the most
widespread measures, without thorough evaluation. To address this issue, we
present a comparative analysis of 38 quality measures from the literature. We
characterize them theoretically, based on four mathematical properties. We
leverage publicly available datasets to constitute a benchmark, and propose a
method to elaborate a gold standard ranking of the patterns. We exploit these
resources to perform an empirical comparison of the measures, both in terms of
pattern ranking and classification performance. Moreover, we propose a
clustering-based preprocessing step, which groups patterns appearing in the
same graphs to enhance classification performance. Our experimental results
demonstrate the effectiveness of this step, reducing the number of patterns to
be processed while achieving comparable performance. Additionally, we show that
some popular measures widely used in the literature are not associated with the
best results.

</details>


### [156] [Smooth-Distill: A Self-distillation Framework for Multitask Learning with Wearable Sensor Data](https://arxiv.org/abs/2507.00061)
*Hoang-Dieu Vu,Duc-Nghia Tran,Quang-Tu Pham,Hieu H. Pham,Nicolas Vuillerme,Duc-Tan Tran*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper introduces Smooth-Distill, a novel self-distillation framework
designed to simultaneously perform human activity recognition (HAR) and sensor
placement detection using wearable sensor data. The proposed approach utilizes
a unified CNN-based architecture, MTL-net, which processes accelerometer data
and branches into two outputs for each respective task. Unlike conventional
distillation methods that require separate teacher and student models, the
proposed framework utilizes a smoothed, historical version of the model itself
as the teacher, significantly reducing training computational overhead while
maintaining performance benefits. To support this research, we developed a
comprehensive accelerometer-based dataset capturing 12 distinct sleep postures
across three different wearing positions, complementing two existing public
datasets (MHealth and WISDM). Experimental results show that Smooth-Distill
consistently outperforms alternative approaches across different evaluation
scenarios, achieving notable improvements in both human activity recognition
and device placement detection tasks. This method demonstrates enhanced
stability in convergence patterns during training and exhibits reduced
overfitting compared to traditional multitask learning baselines. This
framework contributes to the practical implementation of knowledge distillation
in human activity recognition systems, offering an effective solution for
multitask learning with accelerometer data that balances accuracy and training
efficiency. More broadly, it reduces the computational cost of model training,
which is critical for scenarios requiring frequent model updates or training on
resource-constrained platforms. The code and model are available at
https://github.com/Kuan2vn/smooth\_distill.

</details>


### [157] [The language of time: a language model perspective on time-series foundation models](https://arxiv.org/abs/2507.00078)
*Yi Xie,Yun Xiong,Zejian Shi,Hao Niu,Zhengfu Liu*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: With the rise of large language models, the paradigm of training foundation
models with massive parameter counts on vast datasets has been adopted in
multiple domains to achieve remarkable success. Time series foundation models
represent a significant extension of this paradigm, demonstrating exceptional
expressive power, generalization, and cross-domain transferability. However,
this gives rise to a fundamental paradox: time series data reflect distinct
dynamical systems, making cross-domain transfer intuitively implausible, yet
this is contradicted by the models' empirical success. To resolve this paradox,
this paper investigates, from both theoretical and experimental perspectives,
the representation learning mechanisms and generalization capabilities of
patch-based time series foundation models. We argue that such models are not
merely applying a new architecture but are fundamentally generalizing the
representation paradigm of language models by extending deterministic
vector-based representations to latent probabilistic distributional forms. Our
theoretical analysis supports this framework by demonstrating that continuous
time-series patches can be faithfully quantized into a discrete vocabulary
whose key statistical properties are highly consistent with those of natural
language. This generalization allows time series models to inherit the robust
representation and transfer abilities of large language models, thereby
explaining their superior performance in temporal tasks. Ultimately, our work
provides a rigorous theoretical cornerstone for understanding, evaluating, and
improving the safety and reliability of large-scale time series foundation
models.

</details>


### [158] [Federated Learning-Enabled Hybrid Language Models for Communication-Efficient Token Transmission](https://arxiv.org/abs/2507.00082)
*Faranaksadat Solat,Joohyung Lee,Mohamed Seif,Dusit Niyato,H. Vincent Poor*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Hybrid Language Models (HLMs) combine the low-latency efficiency of Small
Language Models (SLMs) on edge devices with the high accuracy of Large Language
Models (LLMs) on centralized servers. Unlike traditional end-to-end LLM
inference, HLMs reduce latency and communication by invoking LLMs only when
local SLM predictions are uncertain, i.e., when token-level confidence is low
or entropy is high. However, ambiguous or low-confidence predictions still
require frequent offloading to the LLM, leading to significant communication
overhead in bandwidth-constrained settings. To address this, we propose FedHLM,
a communication-efficient HLM framework that integrates uncertainty-aware
inference with Federated Learning (FL). FedHLM's key innovation lies in
collaboratively learning token-level uncertainty thresholds that govern when
LLM assistance is needed. Rather than using static or manually tuned
thresholds, FedHLM employs FL to optimize these thresholds in a
privacy-preserving, distributed manner. Additionally, it leverages
embedding-based token representations for Peer-to-Peer (P2P) resolution,
enabling clients to reuse tokens inferred by semantically similar peers without
engaging the LLM. We further introduce hierarchical model aggregation: edge
servers refine local routing policies through client updates, while
cross-cluster coordination aligns global decision boundaries. This layered
design captures recurring uncertainty patterns, reducing redundant LLM queries.
Experiments on large-scale news classification tasks show that FedHLM reduces
LLM transmissions by over 95 percent with negligible accuracy loss, making it
well-suited for scalable and efficient edge-AI applications.

</details>


### [159] [Theoretical Modeling of LLM Self-Improvement Training Dynamics Through Solver-Verifier Gap](https://arxiv.org/abs/2507.00075)
*Yifan Sun,Yushan Liang,Zhen Zhang,Jiaye Teng*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Self-improvement is among the most prominent techniques within the realm of
large language models (LLM), aiming to enhance the LLM performance without
relying on external data. Despite its significance, generally how LLM
performances evolve during the self-improvement process remains underexplored.
In this paper, we theoretically model the training dynamics of self-improvement
via the concept of solver-verifier gap. This is inspired by the conjecture that
the performance enhancement of self-improvement stems from the gap between
LLM's solver capability and verifier capability. Based on the theoretical
framework, we further introduce how to predict the ultimate power of
self-improvement using only information from the first few training epochs. We
empirically validate the effectiveness of the theoretical model on various LLMs
and datasets. Beyond self-improvement, we extend our analysis to investigate
how external data influences these dynamics within the framework. Notably, we
find that under limited external data regimes, such external data can be
utilized at any stage without significantly affecting final performances, which
accords with the empirical observations.

</details>


### [160] [Interpretable AI for Time-Series: Multi-Model Heatmap Fusion with Global Attention and NLP-Generated Explanations](https://arxiv.org/abs/2507.00234)
*Jiztom Kavalakkatt Francis,Matthew J Darr*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this paper, we present a novel framework for enhancing model
interpretability by integrating heatmaps produced separately by ResNet and a
restructured 2D Transformer with globally weighted input saliency. We address
the critical problem of spatial-temporal misalignment in existing
interpretability methods, where convolutional networks fail to capture global
context and Transformers lack localized precision - a limitation that impedes
actionable insights in safety-critical domains like healthcare and industrial
monitoring. Our method merges gradient-weighted activation maps (ResNet) and
Transformer attention rollout into a unified visualization, achieving full
spatial-temporal alignment while preserving real-time performance. Empirical
evaluations on clinical (ECG arrhythmia detection) and industrial (energy
consumption prediction) datasets demonstrate significant improvements: the
hybrid framework achieves 94.1% accuracy (F1 0.93) on the PhysioNet dataset and
reduces regression error to RMSE = 0.28 kWh (R2 = 0.95) on the UCI Energy
Appliance dataset-outperforming standalone ResNet, Transformer, and
InceptionTime baselines by 3.8-12.4%. An NLP module translates fused heatmaps
into domain-specific narratives (e.g., "Elevated ST-segment between 2-4 seconds
suggests myocardial ischemia"), validated via BLEU-4 (0.586) and ROUGE-L
(0.650) scores. By formalizing interpretability as causal fidelity and
spatial-temporal alignment, our approach bridges the gap between technical
outputs and stakeholder understanding, offering a scalable solution for
transparent, time-aware decision-making.

</details>


### [161] [Open-ended Scientific Discovery via Bayesian Surprise](https://arxiv.org/abs/2507.00310)
*Dhruv Agarwal,Bodhisattwa Prasad Majumder,Reece Adamson,Megha Chakravorty,Satvika Reddy Gavireddy,Aditya Parashar,Harshit Surana,Bhavana Dalvi Mishra,Andrew McCallum,Ashish Sabharwal,Peter Clark*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The promise of autonomous scientific discovery (ASD) hinges not only on
answering questions, but also on knowing which questions to ask. Most recent
works in ASD explore the use of large language models (LLMs) in goal-driven
settings, relying on human-specified research questions to guide hypothesis
generation. However, scientific discovery may be accelerated further by
allowing the AI system to drive exploration by its own criteria. The few
existing approaches in open-ended ASD select hypotheses based on diversity
heuristics or subjective proxies for human interestingness, but the former
struggles to meaningfully navigate the typically vast hypothesis space, and the
latter suffers from imprecise definitions. This paper presents AutoDS -- a
method for open-ended ASD that instead drives scientific exploration using
Bayesian surprise. Here, we quantify the epistemic shift from the LLM's prior
beliefs about a hypothesis to its posterior beliefs after gathering
experimental results. To efficiently explore the space of nested hypotheses,
our method employs a Monte Carlo tree search (MCTS) strategy with progressive
widening using surprisal as the reward function. We evaluate AutoDS in the
setting of data-driven discovery across 21 real-world datasets spanning domains
such as biology, economics, finance, and behavioral science. Our results
demonstrate that under a fixed budget, AutoDS substantially outperforms
competitors by producing 5--29\% more discoveries deemed surprising by the LLM.
Our human evaluation further finds that two-thirds of AutoDS discoveries are
surprising to the domain experts, suggesting this is an important step forward
towards building open-ended ASD systems.

</details>


### [162] [$μ^2$Tokenizer: Differentiable Multi-Scale Multi-Modal Tokenizer for Radiology Report Generation](https://arxiv.org/abs/2507.00316)
*Siyou Li,Pengyao Qin,Huanan Wu,Dong Nie,Arun J. Thirunavukarasu,Juntao Yu,Le Zhang*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Automated radiology report generation (RRG) aims to produce detailed textual
reports from clinical imaging, such as computed tomography (CT) scans, to
improve the accuracy and efficiency of diagnosis and provision of management
advice. RRG is complicated by two key challenges: (1) inherent complexity in
extracting relevant information from imaging data under resource constraints,
and (2) difficulty in objectively evaluating discrepancies between
model-generated and expert-written reports. To address these challenges, we
propose $\mu^2$LLM, a $\underline{\textbf{mu}}$ltiscale
$\underline{\textbf{mu}}$ltimodal large language models for RRG tasks. The
novel ${\mu}^2$Tokenizer, as an intermediate layer, integrates multi-modal
features from the multiscale visual tokenizer and the text tokenizer, then
enhances report generation quality through direct preference optimization
(DPO), guided by GREEN-RedLlama. Experimental results on four large CT
image-report medical datasetdemonstrate that our method outperforms existing
approaches, highlighting the potential of our fine-tuned $\mu^2$LLMs on limited
data for RRG tasks.

</details>


### [163] [Strategic Counterfactual Modeling of Deep-Target Airstrike Systems via Intervention-Aware Spatio-Causal Graph Networks](https://arxiv.org/abs/2507.00083)
*Wei Meng*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This study addresses the lack of structured causal modeling between tactical
strike behavior and strategic delay in current strategic-level simulations,
particularly the structural bottlenecks in capturing intermediate variables
within the "resilience - nodal suppression - negotiation window" chain. We
propose the Intervention-Aware Spatio-Temporal Graph Neural Network (IA-STGNN),
a novel framework that closes the causal loop from tactical input to strategic
delay output. The model integrates graph attention mechanisms, counterfactual
simulation units, and spatial intervention node reconstruction to enable
dynamic simulations of strike configurations and synchronization strategies.
Training data are generated from a multi-physics simulation platform (GEANT4 +
COMSOL) under NIST SP 800-160 standards, ensuring structural traceability and
policy-level validation. Experimental results demonstrate that IA-STGNN
significantly outperforms baseline models (ST-GNN, GCN-LSTM, XGBoost),
achieving a 12.8 percent reduction in MAE and 18.4 percent increase in Top-5
percent accuracy, while improving causal path consistency and intervention
stability. IA-STGNN enables interpretable prediction of strategic delay and
supports applications such as nuclear deterrence simulation, diplomatic window
assessment, and multi-strategy optimization, providing a structured and
transparent AI decision-support mechanism for high-level policy modeling.

</details>


### [164] [A Joint Topology-Data Fusion Graph Network for Robust Traffic Speed Prediction with Data Anomalism](https://arxiv.org/abs/2507.00085)
*Ruiyuan Jiang,Dongyao Jia,Eng Gee Lim,Pengfei Fan,Yuli Zhang,Shangbo Wang*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Accurate traffic prediction is essential for Intelligent Transportation
Systems (ITS), yet current methods struggle with the inherent complexity and
non-linearity of traffic dynamics, making it difficult to integrate spatial and
temporal characteristics. Furthermore, existing approaches use static
techniques to address non-stationary and anomalous historical data, which
limits adaptability and undermines data smoothing. To overcome these
challenges, we propose the Graph Fusion Enhanced Network (GFEN), an innovative
framework for network-level traffic speed prediction. GFEN introduces a novel
topological spatiotemporal graph fusion technique that meticulously extracts
and merges spatial and temporal correlations from both data distribution and
network topology using trainable methods, enabling the modeling of multi-scale
spatiotemporal features. Additionally, GFEN employs a hybrid methodology
combining a k-th order difference-based mathematical framework with an
attention-based deep learning structure to adaptively smooth historical
observations and dynamically mitigate data anomalies and non-stationarity.
Extensive experiments demonstrate that GFEN surpasses state-of-the-art methods
by approximately 6.3% in prediction accuracy and exhibits convergence rates
nearly twice as fast as recent hybrid models, confirming its superior
performance and potential to significantly enhance traffic prediction system
efficiency.

</details>


### [165] [Flexible Language Modeling in Continuous Space with Transformer-based Autoregressive Flows](https://arxiv.org/abs/2507.00425)
*Ruixiang Zhang,Shuangfei Zhai,Jiatao Gu,Yizhe Zhang,Huangjie Zheng,Tianrong Chen,Miguel Angel Bautista,Josh Susskind,Navdeep Jaitly*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Autoregressive models have driven remarkable progress in language modeling.
Their foundational reliance on discrete tokens, unidirectional context, and
single-pass decoding, while central to their success, also inspires the
exploration of a design space that could offer new axes of modeling
flexibility. In this work, we explore an alternative paradigm, shifting
language modeling from a discrete token space to a continuous latent space. We
propose a novel framework TarFlowLM, that employs transformer-based
autoregressive normalizing flows to model these continuous representations.
This approach unlocks substantial flexibility, enabling the construction of
models that can capture global bi-directional context through stacked,
alternating-direction autoregressive transformations, support block-wise
generation with flexible token patch sizes, and facilitate a hierarchical
multi-pass generation process. We further propose new mixture-based coupling
transformations designed to capture complex dependencies within the latent
space shaped by discrete data, and demonstrate theoretical connections to
conventional discrete autoregressive models. Extensive experiments on language
modeling benchmarks demonstrate strong likelihood performance and highlight the
flexible modeling capabilities inherent in our framework.

</details>


### [166] [pUniFind: a unified large pre-trained deep learning model pushing the limit of mass spectra interpretation](https://arxiv.org/abs/2507.00087)
*Jiale Zhao,Pengzhi Mao,Kaifei Wang,Yiming Li,Yaping Peng,Ranfei Chen,Shuqi Lu,Xiaohong Ji,Jiaxiang Ding,Xin Zhang,Yucheng Liao,Weinan E,Weijie Zhang,Han Wen,Hao Chi*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Deep learning has advanced mass spectrometry data interpretation, yet most
models remain feature extractors rather than unified scoring frameworks. We
present pUniFind, the first large-scale multimodal pre-trained model in
proteomics that integrates end-to-end peptide-spectrum scoring with open,
zero-shot de novo sequencing. Trained on over 100 million open search-derived
spectra, pUniFind aligns spectral and peptide modalities via cross modality
prediction and outperforms traditional engines across diverse datasets,
particularly achieving a 42.6 percent increase in the number of identified
peptides in immunopeptidomics. Supporting over 1,300 modifications, pUniFind
identifies 60 percent more PSMs than existing de novo methods despite a
300-fold larger search space. A deep learning based quality control module
further recovers 38.5 percent additional peptides including 1,891 mapped to the
genome but absent from reference proteomes while preserving full fragment ion
coverage. These results establish a unified, scalable deep learning framework
for proteomic analysis, offering improved sensitivity, modification coverage,
and interpretability.

</details>


### [167] [Overcoming Long-Context Limitations of State-Space Models via Context-Dependent Sparse Attention](https://arxiv.org/abs/2507.00449)
*Zhihao Zhan,Jianan Zhao,Zhaocheng Zhu,Jian Tang*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Efficient long-context modeling remains a critical challenge for natural
language processing (NLP), as the time complexity of the predominant
Transformer architecture scales quadratically with the sequence length. While
state-space models (SSMs) offer alternative sub-quadratic solutions, they
struggle to capture long-range dependencies effectively. In this work, we focus
on analyzing and improving the long-context modeling capabilities of SSMs. We
show that the widely used synthetic task, associative recall, which requires a
model to recall a value associated with a single key without context,
insufficiently represents the complexities of real-world long-context modeling.
To address this limitation, we extend the associative recall to a novel
synthetic task, \emph{joint recall}, which requires a model to recall the value
associated with a key given in a specified context. Theoretically, we prove
that SSMs do not have the expressiveness to solve multi-query joint recall in
sub-quadratic time complexity. To resolve this issue, we propose a solution
based on integrating SSMs with Context-Dependent Sparse Attention (CDSA), which
has the expressiveness to solve multi-query joint recall with sub-quadratic
computation. To bridge the gap between theoretical analysis and real-world
applications, we propose locality-sensitive Hashing Attention with sparse Key
Selection (HAX), which instantiates the theoretical solution and is further
tailored to natural language domains. Extensive experiments on both synthetic
and real-world long-context benchmarks show that HAX consistently outperforms
SSM baselines and SSMs integrated with context-independent sparse attention
(CISA).

</details>


### [168] [Generating Heterogeneous Multi-dimensional Data : A Comparative Study](https://arxiv.org/abs/2507.00090)
*Corbeau Michael,Claeys Emmanuelle,Serrurier Mathieu,Zaraté Pascale*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Allocation of personnel and material resources is highly sensible in the case
of firefighter interventions. This allocation relies on simulations to
experiment with various scenarios. The main objective of this allocation is the
global optimization of the firefighters response. Data generation is then
mandatory to study various scenarios In this study, we propose to compare
different data generation methods. Methods such as Random Sampling, Tabular
Variational Autoencoders, standard Generative Adversarial Networks, Conditional
Tabular Generative Adversarial Networks and Diffusion Probabilistic Models are
examined to ascertain their efficacy in capturing the intricacies of
firefighter interventions. Traditional evaluation metrics often fall short in
capturing the nuanced requirements of synthetic datasets for real-world
scenarios. To address this gap, an evaluation of synthetic data quality is
conducted using a combination of domain-specific metrics tailored to the
firefighting domain and standard measures such as the Wasserstein distance.
Domain-specific metrics include response time distribution, spatial-temporal
distribution of interventions, and accidents representation. These metrics are
designed to assess data variability, the preservation of fine and complex
correlations and anomalies such as event with a very low occurrence, the
conformity with the initial statistical distribution and the operational
relevance of the synthetic data. The distribution has the particularity of
being highly unbalanced, none of the variables following a Gaussian
distribution, adding complexity to the data generation process.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [169] [Integrating Universal Generative AI Platforms in Educational Labs to Foster Critical Thinking and Digital Literacy](https://arxiv.org/abs/2507.00007)
*Vasiliy Znamenskiy,Rafael Niyazov,Joel Hernandez*

Main category: cs.CY

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper presents a new educational framework for integrating generative
artificial intelligence (GenAI) platforms such as ChatGPT, Claude, and Gemini
into laboratory activities aimed at developing critical thinking and digital
literacy among undergraduate students. Recognizing the limitations and risks of
uncritical reliance on large language models (LLMs), the proposed pedagogical
model reframes GenAI as a research subject and cognitive tool. Students
formulate discipline-specific prompts and evaluate GenAI-generated responses in
text, image, and video modalities. A pilot implementation in a general
astronomy course for non-science majors demonstrated high levels of engagement
and critical reflection, with many students continuing the activity after class
and presenting results at a research symposium. The results highlight the
importance of structured AI interactions in education and suggest that GenAI
can improve learning outcomes when combined with reflective assessment methods.
The study proposes a replicable model for interdisciplinary AI-integrated lab
work, adaptable to scientific disciplines. See the guide to learning activities
based on Generative-Ai platforms: https://doi.org/10.5281/zenodo.15555802

</details>


### [170] [Ken Utilization Layer: Hebbian Replay Within a Student's Ken for Adaptive Knowledge Tracing](https://arxiv.org/abs/2507.00032)
*Grey Kuling,Marinka Zitnik*

Main category: cs.CY

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We introduce KUL-KT, a biologically inspired architecture for knowledge
tracing (KT), combining Hebbian memory encoding with gradient-based
consolidation in a scalable, input-agnostic framework. KUL-KT adapts the
principle of memory consolidation in neural systems, to student modeling by
introducing two key innovations: (i) a time-decaying Hebbian memory update that
enables graceful forgetting, and (ii) a novel Loss-aligned Internal Target
(LIT) method to compute an ideal internal state, allowing continual learning
without backpropagation through time. The architecture consists of a fast
Hebbian memory that captures each learner interaction via a single associative
update, and a slower linear network that consolidates recalled samples through
gradient descent. This design enables few-shot personalization and natural
forgetting without storing raw data or relying on large cohort training.
Operating entirely in embedding space, KUL-KT supports both structured
(tabular) and unstructured (short-answer) inputs. Empirically, KUL-KT
outperforms strong baselines on ten public KT benchmarks in rank-sensitive
metrics such as nDCG and Recall@10. In a classroom deployment, KUL-KT
personalized quizzes from short-answer data, leading to improved
learner-perceived helpfulness and reduced difficulty (p < 0.05). Ablation
studies confirm that Hebbian decay and LIT are critical for continual
adaptation. Compared to a strong graph-based KT model, KUL-KT trains 1.75x
faster and uses 99.01\% less memory. These results position KUL-KT as a
biologically grounded, memory-efficient, and input-flexible framework for
personalized learning at scale.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [171] [Disentangled Feature Importance](https://arxiv.org/abs/2507.00260)
*Jin-Hong Du,Kathryn Roeder,Larry Wasserman*

Main category: stat.ML

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Feature importance quantification faces a fundamental challenge: when
predictors are correlated, standard methods systematically underestimate their
contributions. We prove that major existing approaches target identical
population functionals under squared-error loss, revealing why they share this
correlation-induced bias.
  To address this limitation, we introduce \emph{Disentangled Feature
Importance (DFI)}, a nonparametric generalization of the classical $R^2$
decomposition via optimal transport. DFI transforms correlated features into
independent latent variables using a transport map, eliminating correlation
distortion. Importance is computed in this disentangled space and attributed
back through the transport map's sensitivity. DFI provides a principled
decomposition of importance scores that sum to the total predictive variability
for latent additive models and to interaction-weighted functional ANOVA
variances more generally, under arbitrary feature dependencies.
  We develop a comprehensive semiparametric theory for DFI. For general
transport maps, we establish root-$n$ consistency and asymptotic normality of
importance estimators in the latent space, which extends to the original
feature space for the Bures-Wasserstein map. Notably, our estimators achieve
second-order estimation error, which vanishes if both regression function and
transport map estimation errors are $o_{\mathbb{P}}(n^{-1/4})$. By design, DFI
avoids the computational burden of repeated submodel refitting and the
challenges of conditional covariate distribution estimation, thereby achieving
computational efficiency.

</details>


### [172] [GRAND: Graph Release with Assured Node Differential Privacy](https://arxiv.org/abs/2507.00402)
*Suqing Liu,Xuan Bi,Tianxi Li*

Main category: stat.ML

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Differential privacy is a well-established framework for safeguarding
sensitive information in data. While extensively applied across various
domains, its application to network data -- particularly at the node level --
remains underexplored. Existing methods for node-level privacy either focus
exclusively on query-based approaches, which restrict output to pre-specified
network statistics, or fail to preserve key structural properties of the
network. In this work, we propose GRAND (Graph Release with Assured Node
Differential privacy), which is, to the best of our knowledge, the first
network release mechanism that releases entire networks while ensuring
node-level differential privacy and preserving structural properties. Under a
broad class of latent space models, we show that the released network
asymptotically follows the same distribution as the original network. The
effectiveness of the approach is evaluated through extensive experiments on
both synthetic and real-world datasets.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [173] [Generalization performance of narrow one-hidden layer networks in the teacher-student setting](https://arxiv.org/abs/2507.00629)
*Jean Barbier,Federica Gerace,Alessandro Ingrosso,Clarissa Lauditi,Enrico M. Malatesta,Gibbs Nwemadji,Rodrigo Pérez Ortiz*

Main category: cond-mat.dis-nn

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Understanding the generalization abilities of neural networks for simple
input-output distributions is crucial to account for their learning performance
on real datasets. The classical teacher-student setting, where a network is
trained from data obtained thanks to a label-generating teacher model, serves
as a perfect theoretical test bed. In this context, a complete theoretical
account of the performance of fully connected one-hidden layer networks in the
presence of generic activation functions is lacking. In this work, we develop
such a general theory for narrow networks, i.e. networks with a large number of
hidden units, yet much smaller than the input dimension. Using methods from
statistical physics, we provide closed-form expressions for the typical
performance of both finite temperature (Bayesian) and empirical risk
minimization estimators, in terms of a small number of weight statistics. In
doing so, we highlight the presence of a transition where hidden neurons
specialize when the number of samples is sufficiently large and proportional to
the number of parameters of the network. Our theory accurately predicts the
generalization error of neural networks trained on regression or classification
tasks with either noisy full-batch gradient descent (Langevin dynamics) or
full-batch gradient descent.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [174] [How large language models judge and influence human cooperation](https://arxiv.org/abs/2507.00088)
*Alexandre S. Pires,Laurens Samson,Sennay Ghebreab,Fernando P. Santos*

Main category: physics.soc-ph

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Humans increasingly rely on large language models (LLMs) to support decisions
in social settings. Previous work suggests that such tools shape people's moral
and political judgements. However, the long-term implications of LLM-based
social decision-making remain unknown. How will human cooperation be affected
when the assessment of social interactions relies on language models? This is a
pressing question, as human cooperation is often driven by indirect
reciprocity, reputations, and the capacity to judge interactions of others.
Here, we assess how state-of-the-art LLMs judge cooperative actions. We provide
21 different LLMs with an extensive set of examples where individuals cooperate
-- or refuse cooperating -- in a range of social contexts, and ask how these
interactions should be judged. Furthermore, through an evolutionary
game-theoretical model, we evaluate cooperation dynamics in populations where
the extracted LLM-driven judgements prevail, assessing the long-term impact of
LLMs on human prosociality. We observe a remarkable agreement in evaluating
cooperation against good opponents. On the other hand, we notice within- and
between-model variance when judging cooperation with ill-reputed individuals.
We show that the differences revealed between models can significantly impact
the prevalence of cooperation. Finally, we test prompts to steer LLM norms,
showing that such interventions can shape LLM judgements, particularly through
goal-oriented prompts. Our research connects LLM-based advices and long-term
social dynamics, and highlights the need to carefully align LLM norms in order
to preserve human cooperation.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [175] [Geometric Gaussian Approximations of Probability Distributions](https://arxiv.org/abs/2507.00616)
*Nathaël Da Costa,Bálint Mucsányi,Philipp Hennig*

Main category: math.DG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Approximating complex probability distributions, such as Bayesian posterior
distributions, is of central interest in many applications. We study the
expressivity of geometric Gaussian approximations. These consist of
approximations by Gaussian pushforwards through diffeomorphisms or Riemannian
exponential maps. We first review these two different kinds of geometric
Gaussian approximations. Then we explore their relationship to one another. We
further provide a constructive proof that such geometric Gaussian
approximations are universal, in that they can capture any probability
distribution. Finally, we discuss whether, given a family of probability
distributions, a common diffeomorphism can be found to obtain uniformly
high-quality geometric Gaussian approximations for that family.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [176] [$σ$-Maximal Ancestral Graphs](https://arxiv.org/abs/2507.00093)
*Binghua Yao,Joris M. Mooij*

Main category: cs.DM

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Maximal Ancestral Graphs (MAGs) provide an abstract representation of
Directed Acyclic Graphs (DAGs) with latent (selection) variables. These
graphical objects encode information about ancestral relations and
d-separations of the DAGs they represent. This abstract representation has been
used amongst others to prove the soundness and completeness of the FCI
algorithm for causal discovery, and to derive a do-calculus for its output. One
significant inherent limitation of MAGs is that they rule out the possibility
of cyclic causal relationships. In this work, we address that limitation. We
introduce and study a class of graphical objects that we coin
''$\sigma$-Maximal Ancestral Graphs'' (''$\sigma$-MAGs''). We show how these
graphs provide an abstract representation of (possibly cyclic) Directed Graphs
(DGs) with latent (selection) variables, analogously to how MAGs represent
DAGs. We study the properties of these objects and provide a characterization
of their Markov equivalence classes.

</details>
